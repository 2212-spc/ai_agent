"""
LangGraph Agent - å®Œæ•´çš„æ™ºèƒ½ä½“å®ç°
æ”¯æŒï¼šå¤šæ­¥éª¤è§„åˆ’ã€å¹¶è¡Œæ‰§è¡Œã€æ¡ä»¶è·¯ç”±ã€çŠ¶æ€æŒä¹…åŒ–ã€äººå·¥ä»‹å…¥
"""
from __future__ import annotations

import json
import logging
import operator
import re
import uuid
from datetime import datetime
from typing import Annotated, Any, Dict, List, Literal, Optional, Sequence, TypedDict

import httpx
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, StateGraph
from langgraph.prebuilt import ToolNode
from sqlalchemy.orm import Session

from .config import Settings
from .database import ToolRecord
from .rag_service import retrieve_context
from .tool_service import execute_tool, parse_tool_call

logger = logging.getLogger(__name__)


# ==================== LLM è°ƒç”¨å·¥å…· ====================

async def invoke_llm(
    messages: List[Dict[str, str]],
    settings: Settings,
    temperature: float = 0.7,
    max_tokens: Optional[int] = None,
) -> tuple[str, Dict[str, Any]]:
    """
    è°ƒç”¨ DeepSeek API è¿›è¡Œæ¨ç†
    
    Args:
        messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
        settings: é…ç½®å¯¹è±¡
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§ token æ•°
    
    Returns:
        (å›å¤å†…å®¹, å®Œæ•´å“åº”æ•°æ®)
    """
    payload: Dict[str, Any] = {
        "model": "deepseek-chat",
        "messages": messages,
        "temperature": temperature,
        "stream": False,
    }
    if max_tokens is not None:
        payload["max_tokens"] = max_tokens

    headers = {
        "Authorization": f"Bearer {settings.deepseek_api_key}",
        "Content-Type": "application/json",
    }
    endpoint = f"{settings.deepseek_base_url.rstrip('/')}/chat/completions"

    try:
        async with httpx.AsyncClient(timeout=httpx.Timeout(60.0)) as client:
            response = await client.post(endpoint, json=payload, headers=headers)

        if response.status_code != 200:
            logger.error(
                "DeepSeek API error %s: %s", response.status_code, response.text
            )
            return f"API è°ƒç”¨å¤±è´¥: {response.status_code}", {}

        data = response.json()
        reply = data["choices"][0]["message"]["content"]
        return reply, data
    
    except Exception as e:
        logger.error(f"LLM è°ƒç”¨å¼‚å¸¸: {e}")
        return f"LLM è°ƒç”¨å¤±è´¥: {str(e)}", {}


def parse_json_from_llm(text: str) -> Dict[str, Any]:
    """
    ä» LLM å“åº”ä¸­æå– JSON
    æ”¯æŒå¤„ç† markdown ä»£ç å—åŒ…è£¹çš„ JSON
    """
    # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
    text = text.strip()
    if text.startswith("```json"):
        text = text[7:]
    elif text.startswith("```"):
        text = text[3:]
    if text.endswith("```"):
        text = text[:-3]
    
    text = text.strip()
    
    try:
        return json.loads(text)
    except json.JSONDecodeError as e:
        logger.warning(f"JSON è§£æå¤±è´¥: {e}, åŸå§‹æ–‡æœ¬: {text[:200]}")
        # è¿”å›é»˜è®¤ç»“æ„
        return {
            "task_type": "ä¿¡æ¯æŸ¥è¯¢",
            "steps": ["åˆ†æé—®é¢˜", "ç”Ÿæˆå›ç­”"],
            "required_tools": [],
            "need_knowledge_base": False
        }


def format_tools_description(tool_records: List[ToolRecord]) -> str:
    """æ ¼å¼åŒ–å·¥å…·æè¿°ä¾› LLM ç†è§£"""
    if not tool_records:
        return "æ— å¯ç”¨å·¥å…·"
    
    descriptions = []
    for tool in tool_records:
        try:
            config = json.loads(tool.config or "{}")
            builtin_key = config.get("builtin_key", "")
            descriptions.append(
                f"- {tool.id}: {tool.name} ({builtin_key}) - {tool.description}"
            )
        except:
            descriptions.append(f"- {tool.id}: {tool.name} - {tool.description}")
    
    return "\n".join(descriptions)


# ==================== çŠ¶æ€å®šä¹‰ ====================
class AgentState(TypedDict):
    """Agent çš„çŠ¶æ€ï¼Œè´¯ç©¿æ•´ä¸ªå·¥ä½œæµ"""
    
    # åŸºç¡€ä¿¡æ¯
    user_query: str  # ç”¨æˆ·åŸå§‹é—®é¢˜
    conversation_history: Annotated[Sequence[Dict[str, str]], operator.add]  # å¯¹è¯å†å²
    
    # è§„åˆ’ä¿¡æ¯
    plan: Optional[str]  # Agent ç”Ÿæˆçš„è®¡åˆ’
    current_step: int  # å½“å‰æ‰§è¡Œåˆ°ç¬¬å‡ æ­¥
    max_iterations: int  # æœ€å¤§è¿­ä»£æ¬¡æ•°
    
    # å·¥å…·ç›¸å…³
    available_tools: List[str]  # å¯ç”¨çš„å·¥å…·IDåˆ—è¡¨
    tool_calls_made: Annotated[List[Dict[str, Any]], operator.add]  # å·²æ‰§è¡Œçš„å·¥å…·è°ƒç”¨
    tool_results: Annotated[List[Dict[str, Any]], operator.add]  # å·¥å…·æ‰§è¡Œç»“æœ
    skipped_tasks: Annotated[List[Dict[str, Any]], operator.add]  # è¢«è·³è¿‡çš„ä»»åŠ¡åŠåŸå› 
    
    # RAG ç›¸å…³
    use_knowledge_base: bool  # æ˜¯å¦ä½¿ç”¨çŸ¥è¯†åº“
    retrieved_contexts: List[Dict[str, Any]]  # æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
    
    # Agent æ€è€ƒè¿‡ç¨‹
    thoughts: Annotated[List[str], operator.add]  # Agent çš„æ€è€ƒè¿‡ç¨‹
    observations: Annotated[List[str], operator.add]  # è§‚å¯Ÿåˆ°çš„ç»“æœ
    
    # å†³ç­–ç›¸å…³
    next_action: Optional[str]  # ä¸‹ä¸€æ­¥åŠ¨ä½œï¼štool_call, search_kb, synthesize, complete
    needs_human_input: bool  # æ˜¯å¦éœ€è¦äººå·¥ä»‹å…¥
    human_feedback: Optional[str]  # äººå·¥åé¦ˆ
    
    # è´¨é‡æ§åˆ¶
    reflection: Optional[str]  # åæ€ç»“æœ
    quality_score: float  # è´¨é‡è¯„åˆ† 0-1
    
    # æœ€ç»ˆè¾“å‡º
    final_answer: Optional[str]  # æœ€ç»ˆç­”æ¡ˆ
    is_complete: bool  # æ˜¯å¦å®Œæˆ
    error: Optional[str]  # é”™è¯¯ä¿¡æ¯


# ==================== æ ¸å¿ƒèŠ‚ç‚¹å‡½æ•° ====================

async def planner_node(
    state: AgentState,
    settings: Settings,
    tool_records: List[ToolRecord],
) -> Dict[str, Any]:
    """
    è§„åˆ’å™¨èŠ‚ç‚¹ï¼šä½¿ç”¨ LLM åˆ†æç”¨æˆ·é—®é¢˜ï¼Œç”Ÿæˆæ™ºèƒ½æ‰§è¡Œè®¡åˆ’
    """
    logger.info("ğŸ§  [è§„åˆ’å™¨] å¼€å§‹æ™ºèƒ½åˆ†æä»»åŠ¡...")
    
    user_query = state["user_query"]
    use_knowledge_base = state.get("use_knowledge_base", False)
    
    # æ ¼å¼åŒ–å·¥å…·æè¿°
    tools_desc = format_tools_description(tool_records)
    
    # æ„å»ºæ™ºèƒ½è§„åˆ’æç¤ºè¯
    planning_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä»»åŠ¡è§„åˆ’åŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·é—®é¢˜ï¼Œåˆ¶å®šæ‰§è¡Œè®¡åˆ’ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_query}

å¯ç”¨å·¥å…·ï¼š
{tools_desc}

çŸ¥è¯†åº“ï¼š{"å·²å¯ç”¨" if use_knowledge_base else "æœªå¯ç”¨"}

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºè®¡åˆ’ï¼š
{{
  "task_type": "ä¿¡æ¯æŸ¥è¯¢|å·¥å…·è°ƒç”¨|çŸ¥è¯†æ£€ç´¢|å¤åˆä»»åŠ¡",
  "analysis": "ä»»åŠ¡åˆ†æç®€è¿°",
  "steps": ["æ­¥éª¤1", "æ­¥éª¤2", ...],
  "required_tools": ["tool_id_1", ...],
  "need_knowledge_base": true/false,
  "expected_result": "é¢„æœŸç»“æœæè¿°"
}}

æ³¨æ„ï¼š
1. task_type ä»ä»¥ä¸‹é€‰æ‹©ï¼šä¿¡æ¯æŸ¥è¯¢ã€å·¥å…·è°ƒç”¨ã€çŸ¥è¯†æ£€ç´¢ã€å¤åˆä»»åŠ¡
2. steps åº”è¯¥æ˜¯å…·ä½“çš„æ‰§è¡Œæ­¥éª¤
3. required_tools æ˜¯éœ€è¦è°ƒç”¨çš„å·¥å…·IDåˆ—è¡¨ï¼Œå¦‚æœä¸éœ€è¦å·¥å…·åˆ™ä¸ºç©ºæ•°ç»„
4. åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–è§£é‡Š
"""
    
    try:
        # è°ƒç”¨ LLM è¿›è¡Œè§„åˆ’
        llm_response, _ = await invoke_llm(
            messages=[{"role": "user", "content": planning_prompt}],
            settings=settings,
            temperature=0.3,  # ä½æ¸©åº¦ä¿è¯è§„åˆ’ç¨³å®š
            max_tokens=1000
        )
        
        # è§£æ LLM è¿”å›çš„ JSON
        plan_data = parse_json_from_llm(llm_response)
        
        task_type = plan_data.get("task_type", "ä¿¡æ¯æŸ¥è¯¢")
        analysis = plan_data.get("analysis", "åˆ†æä»»åŠ¡ä¸­...")
        steps = plan_data.get("steps", ["åˆ†æé—®é¢˜", "ç”Ÿæˆç­”æ¡ˆ"])
        
        logger.info(f"ğŸ“‹ è§„åˆ’å®Œæˆï¼š{task_type}, {len(steps)} ä¸ªæ­¥éª¤")
        
        # æ ¼å¼åŒ–ä¸ºå¯è¯»æ–‡æœ¬
        plan_text = f"""ä»»åŠ¡ç±»å‹ï¼š{task_type}
ä»»åŠ¡åˆ†æï¼š{analysis}

æ‰§è¡Œæ­¥éª¤ï¼š
{chr(10).join(f"{i+1}. {step}" for i, step in enumerate(steps))}

é¢„æœŸç»“æœï¼š{plan_data.get('expected_result', 'ä¸ºç”¨æˆ·æä¾›å‡†ç¡®ç­”æ¡ˆ')}
"""
        
        thought = f"æ™ºèƒ½è§„åˆ’å®Œæˆï¼šè¯†åˆ«ä¸ºã€{task_type}ã€‘ï¼Œå…± {len(steps)} ä¸ªæ­¥éª¤"
        
        return {
            "plan": plan_text,
            "current_step": 0,
            "thoughts": [thought],
            "next_action": "route"
        }
    
    except Exception as e:
        logger.error(f"è§„åˆ’å™¨å¤±è´¥: {e}")
        # é™çº§åˆ°ç®€å•è§„åˆ’
        fallback_plan = f"""ä»»åŠ¡åˆ†æï¼šç”¨æˆ·è¯¢é—®ã€Œ{user_query}ã€

æ‰§è¡Œæ­¥éª¤ï¼š
1. æ ¹æ®é—®é¢˜é€‰æ‹©åˆé€‚çš„å¤„ç†æ–¹å¼
2. æ”¶é›†å¿…è¦çš„ä¿¡æ¯
3. ç”Ÿæˆå®Œæ•´ç­”æ¡ˆ

é¢„æœŸç»“æœï¼šä¸ºç”¨æˆ·æä¾›æœ‰ç”¨çš„å›ç­”
"""
        return {
            "plan": fallback_plan,
            "current_step": 0,
            "thoughts": [f"ä½¿ç”¨ç®€åŒ–è§„åˆ’æ¨¡å¼ï¼ˆè§„åˆ’å™¨å¼‚å¸¸ï¼š{str(e)[:50]}ï¼‰"],
            "next_action": "route"
        }


async def router_node(
    state: AgentState,
    settings: Settings,
) -> Dict[str, Any]:
    """
    è·¯ç”±å™¨èŠ‚ç‚¹ï¼šä½¿ç”¨ LLM æ™ºèƒ½å†³å®šä¸‹ä¸€æ­¥åŠ¨ä½œ
    """
    logger.info("ğŸ”€ [è·¯ç”±å™¨] æ™ºèƒ½å†³ç­–ä¸‹ä¸€æ­¥åŠ¨ä½œ...")
    
    user_query = state["user_query"]
    current_step = state.get("current_step", 0)
    max_iterations = state.get("max_iterations", 10)
    tool_calls_made = state.get("tool_calls_made", [])
    use_knowledge_base = state.get("use_knowledge_base", False)
    observations = state.get("observations", [])
    retrieved_contexts = state.get("retrieved_contexts", [])
    tool_results = state.get("tool_results", [])
    
    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•°
    if current_step >= max_iterations:
        return {
            "next_action": "synthesize",
            "thoughts": [f"å·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°({max_iterations})ï¼Œå‡†å¤‡ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"],
            "current_step": current_step + 1
        }
    
    # å¦‚æœç¬¬ä¸€æ­¥ï¼Œå…ˆè¿›è¡Œç®€å•åˆ¤æ–­ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰
    if current_step == 0:
        kb_searched = any("çŸ¥è¯†åº“" in obs for obs in observations)
        
        # å¯ç”¨çŸ¥è¯†åº“ä½†æœªæ£€ç´¢
        if use_knowledge_base and not kb_searched:
            return {
                "next_action": "search_kb",
                "thoughts": ["é¦–æ¬¡æ‰§è¡Œï¼šä¼˜å…ˆæ£€ç´¢çŸ¥è¯†åº“"],
                "current_step": current_step + 1
            }
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å·¥å…·
        if should_call_tool(state):
            return {
                "next_action": "tool_executor",
                "thoughts": ["é¦–æ¬¡æ‰§è¡Œï¼šæ£€æµ‹åˆ°éœ€è¦å·¥å…·è°ƒç”¨"],
                "current_step": current_step + 1
            }
    
    # æ­¥éª¤ >= 1ï¼Œä½¿ç”¨ LLM æ™ºèƒ½å†³ç­–
    # å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»æ£€ç´¢è¿‡çŸ¥è¯†åº“ï¼Œé¿å…é‡å¤æœç´¢
    kb_already_searched = len(retrieved_contexts) > 0 or any("çŸ¥è¯†åº“" in obs or "æ£€ç´¢åˆ°" in obs for obs in observations)
    
    try:
        # æ„å»ºå†³ç­–ä¸Šä¸‹æ–‡
        kb_status = "å·²æ£€ç´¢" if kb_already_searched else "æœªæ£€ç´¢"
        kb_status_detail = f"å·²æ£€ç´¢ {len(retrieved_contexts)} æ¡" if retrieved_contexts else "æœªæ£€ç´¢"
        
        context_summary = f"""å½“å‰æ‰§è¡ŒçŠ¶æ€ï¼š
- ç”¨æˆ·é—®é¢˜ï¼š{user_query}
- æ‰§è¡Œæ­¥éª¤ï¼š{current_step}/{max_iterations}
- å·²è°ƒç”¨å·¥å…·æ•°ï¼š{len(tool_calls_made)}
- çŸ¥è¯†åº“æ£€ç´¢çŠ¶æ€ï¼š{kb_status_detail}
- å·¥å…·æ‰§è¡Œç»“æœæ•°ï¼š{len(tool_results)}

æœ€è¿‘è§‚å¯Ÿï¼š
{chr(10).join("- " + obs for obs in observations[-3:]) if observations else "æš‚æ— è§‚å¯Ÿ"}

è¯·åˆ¤æ–­ä¸‹ä¸€æ­¥åº”è¯¥åšä»€ä¹ˆï¼š
A. search_kb - éœ€è¦ä»çŸ¥è¯†åº“æ£€ç´¢ä¿¡æ¯
B. tool_executor - éœ€è¦è°ƒç”¨å¤–éƒ¨å·¥å…·è·å–æ•°æ®
C. synthesize - ä¿¡æ¯å·²è¶³å¤Ÿï¼Œå¯ä»¥ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ

è¦æ±‚ï¼š
1. å¦‚æœå¯ç”¨äº†çŸ¥è¯†åº“ä½†è¿˜æ²¡æ£€ç´¢ï¼ˆçŸ¥è¯†åº“æ£€ç´¢çŠ¶æ€æ˜¾ç¤º"æœªæ£€ç´¢"ï¼‰ï¼Œä¼˜å…ˆé€‰æ‹© A
2. å¦‚æœçŸ¥è¯†åº“å·²ç»æ£€ç´¢è¿‡ï¼ˆçŸ¥è¯†åº“æ£€ç´¢çŠ¶æ€æ˜¾ç¤º"å·²æ£€ç´¢"ï¼‰ï¼Œä¸è¦é‡å¤é€‰æ‹© Aï¼Œåº”è¯¥é€‰æ‹© B æˆ– C
3. å¦‚æœé—®é¢˜éœ€è¦å¤šä¸ªå·¥å…·ï¼ˆå¦‚ï¼šæœç´¢+ç»˜å›¾ï¼‰ï¼Œå¿…é¡»æ‰§è¡Œå®Œæ‰€æœ‰å·¥å…·åå†é€‰æ‹© C
4. å¦‚æœé—®é¢˜éœ€è¦å®æ—¶æ•°æ®ï¼ˆå¤©æ°”ã€æœç´¢ç­‰ï¼‰ï¼Œä½†è¿˜æ²¡è°ƒç”¨ç›¸åº”å·¥å…·ï¼Œé€‰æ‹© B
5. å¦‚æœå·²æœ‰è¶³å¤Ÿä¿¡æ¯ä¸”æ‰€æœ‰å¿…è¦å·¥å…·éƒ½å·²æ‰§è¡Œï¼Œé€‰æ‹© C
6. åªå›å¤ä¸€ä¸ªå­—æ¯ï¼ˆA/B/Cï¼‰ï¼Œä¸è¦è§£é‡Š
"""
        
        # è°ƒç”¨ LLM å†³ç­–
        llm_response, _ = await invoke_llm(
            messages=[{"role": "user", "content": context_summary}],
            settings=settings,
            temperature=0.1,  # æä½æ¸©åº¦ä¿è¯å†³ç­–ä¸€è‡´æ€§
            max_tokens=10
        )
        
        decision = llm_response.strip().upper()
        
        # æ˜ å°„å†³ç­–
        action_map = {
            "A": "search_kb",
            "B": "tool_executor",
            "C": "synthesize"
        }
        
        next_action = action_map.get(decision, "synthesize")
        
        # é˜²æ­¢é‡å¤æœç´¢çŸ¥è¯†åº“ï¼šå¦‚æœå·²ç»æ£€ç´¢è¿‡ï¼Œå¼ºåˆ¶æ”¹ä¸º synthesize æˆ– tool_executor
        if next_action == "search_kb" and kb_already_searched:
            logger.warning(f"âš ï¸ é˜»æ­¢é‡å¤çŸ¥è¯†åº“æœç´¢ï¼šå·²æ£€ç´¢è¿‡ {len(retrieved_contexts)} æ¡ï¼Œå¼ºåˆ¶æ”¹ä¸º synthesize")
            if should_call_tool(state):
                next_action = "tool_executor"
                thought = "LLMé€‰æ‹©Aä½†å·²æ£€ç´¢è¿‡çŸ¥è¯†åº“ï¼Œæ”¹ä¸ºè°ƒç”¨å·¥å…·"
            else:
                next_action = "synthesize"
                thought = "LLMé€‰æ‹©Aä½†å·²æ£€ç´¢è¿‡çŸ¥è¯†åº“ï¼Œæ”¹ä¸ºç”Ÿæˆç­”æ¡ˆ"
        else:
            thought = f"LLM æ™ºèƒ½è·¯ç”±ï¼š{decision} -> {next_action}"
        
        logger.info(f"ğŸ“ æ™ºèƒ½è·¯ç”±å†³ç­–ï¼šæ­¥éª¤{current_step}, å†³ç­–={decision}, ä¸‹ä¸€æ­¥={next_action}")
        
        return {
            "next_action": next_action,
            "thoughts": [thought],
            "current_step": current_step + 1
        }
    
    except Exception as e:
        logger.error(f"è·¯ç”±å™¨ LLM å†³ç­–å¤±è´¥: {e}")
        
        # é™çº§ç­–ç•¥ï¼šä½¿ç”¨ç®€å•è§„åˆ™
        kb_searched = len(retrieved_contexts) > 0 or any("çŸ¥è¯†åº“" in obs or "æ£€ç´¢åˆ°" in obs for obs in observations)
        
        # é˜²æ­¢é‡å¤æœç´¢ï¼šå¦‚æœå·²ç»æ£€ç´¢è¿‡ï¼Œä¸å†é€‰æ‹© search_kb
        if use_knowledge_base and not kb_searched and current_step < 2:
            next_action = "search_kb"
            thought = "é™çº§å†³ç­–ï¼šæ£€ç´¢çŸ¥è¯†åº“"
        elif kb_searched and current_step >= 2:
            # å·²ç»æ£€ç´¢è¿‡ï¼Œå¦‚æœè¿˜æœ‰å·¥å…·è¦è°ƒç”¨å°±è°ƒç”¨å·¥å…·ï¼Œå¦åˆ™ç”Ÿæˆç­”æ¡ˆ
            if should_call_tool(state):
                next_action = "tool_executor"
                thought = "é™çº§å†³ç­–ï¼šå·²æ£€ç´¢è¿‡çŸ¥è¯†åº“ï¼Œè°ƒç”¨å·¥å…·"
            else:
                next_action = "synthesize"
                thought = "é™çº§å†³ç­–ï¼šå·²æ£€ç´¢è¿‡çŸ¥è¯†åº“ï¼Œç”Ÿæˆç­”æ¡ˆ"
        elif should_call_tool(state):
            next_action = "tool_executor"
            thought = "é™çº§å†³ç­–ï¼šè°ƒç”¨å·¥å…·"
        else:
            next_action = "synthesize"
            thought = "é™çº§å†³ç­–ï¼šç”Ÿæˆç­”æ¡ˆ"
        
        return {
            "next_action": next_action,
            "thoughts": [thought],
            "current_step": current_step + 1
        }


def knowledge_search_node(
    state: AgentState,
    settings: Settings,
) -> Dict[str, Any]:
    """
    çŸ¥è¯†åº“æœç´¢èŠ‚ç‚¹ï¼šä»å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³å†…å®¹
    """
    logger.info("ğŸ“š [çŸ¥è¯†åº“] æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
    
    user_query = state["user_query"]
    
    try:
        # è°ƒç”¨ RAG æ£€ç´¢
        contexts = retrieve_context(
            query=user_query,
            settings=settings,
            top_k=4
        )
        
        retrieved = [
            {
                "document_id": ctx.document_id,
                "original_name": ctx.original_name,
                "content": ctx.content[:500]  # é™åˆ¶é•¿åº¦
            }
            for ctx in contexts
        ]
        
        observation = f"ä»çŸ¥è¯†åº“æ£€ç´¢åˆ° {len(retrieved)} ä¸ªç›¸å…³ç‰‡æ®µ"
        
        return {
            "retrieved_contexts": retrieved,
            "observations": [observation],
            "thoughts": ["çŸ¥è¯†åº“æ£€ç´¢å®Œæˆï¼Œè·å–åˆ°ç›¸å…³èƒŒæ™¯ä¿¡æ¯"]
        }
    
    except Exception as e:
        logger.error(f"çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {e}")
        return {
            "retrieved_contexts": [],
            "observations": [f"çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {str(e)}"],
            "error": str(e)
        }


async def tool_executor_node(
    state: AgentState,
    settings: Settings,
    session: Session,
    tool_records: List[ToolRecord],
) -> Dict[str, Any]:
    """å·¥å…·æ‰§è¡Œå™¨èŠ‚ç‚¹ï¼šæ™ºèƒ½é€‰æ‹©å¹¶æ‰§è¡Œå·¥å…·"""
    logger.info("ğŸ”§ [å·¥å…·æ‰§è¡Œå™¨] å‡†å¤‡è°ƒç”¨å·¥å…·...")

    user_query = state.get("user_query", "")
    tool_calls_made = state.get("tool_calls_made", [])
    tool_results = state.get("tool_results", [])
    skipped_tasks = state.get("skipped_tasks", [])

    tasks = infer_tool_tasks(user_query)
    if not tasks:
        observation = f"åˆ†ææŸ¥è¯¢æœªå‘ç°éœ€è¦è°ƒç”¨å·¥å…·çš„æŒ‡ä»¤ï¼š{user_query}" if user_query else "æ— éœ€è°ƒç”¨å·¥å…·"
        return {
            "thoughts": ["æœªæ‰¾åˆ°éœ€è¦æ‰§è¡Œçš„å·¥å…·ä»»åŠ¡"],
            "observations": [observation],
            "next_action": "synthesize",
        }

    completed_tasks = {call.get("task") for call in tool_calls_made if call.get("task")}
    skipped_task_keys = {
        item.get("task")
        for item in skipped_tasks
        if isinstance(item, dict) and item.get("task")
    }

    tool_index: Dict[str, ToolRecord] = {}
    for record in tool_records:
        if getattr(record, "is_active", True):
            task_key = map_tool_to_task(record)
            if task_key and task_key not in tool_index:
                tool_index[task_key] = record

    pending_task: Optional[str] = None
    for task in tasks:
        if task in completed_tasks or task in skipped_task_keys:
            continue
        pending_task = task
        break

    if not pending_task:
        return {
            "thoughts": ["å¤©æ°”ç»“æœæœªæ‰¾åˆ°"],
            "observations": ["æ‰€æœ‰å·²è¯†åˆ«ä»»åŠ¡å‡å·²å®Œæˆæˆ–è·³è¿‡"],
            "next_action": "synthesize",
        }

    selected_tool = tool_index.get(pending_task)
    if not selected_tool:
        reason = f"æ‰¾ä¸åˆ°ä»»åŠ¡ {pending_task} å¯¹åº”çš„å·¥å…·"
        logger.warning(reason)
        return {
            "skipped_tasks": [{"task": pending_task, "reason": reason}],
            "observations": [reason],
            "thoughts": ["æ‰¾ä¸åˆ°å¯ç”¨å·¥å…·"],
        }

    logger.info("âœ… é€‰æ‹©å·¥å…·ï¼šä»»åŠ¡ %sï¼Œå·¥å…·å %s", pending_task, selected_tool.name)

    tool_args: Dict[str, Any] = {}
    action_description = ""

    if pending_task == "weather":
        city = extract_city_from_query(user_query)
        tool_args = {"city": city}
        action_description = f"æŸ¥è¯¢{city}å¤©æ°”"
    elif pending_task == "search":
        search_query = extract_search_query(user_query)
        tool_args = {"query": search_query, "num_results": 6}
        action_description = f"æœç´¢'{search_query}'è·å–ä¿¡æ¯"
    elif pending_task == "diagram":
        # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢ç»“æœï¼Œå¦‚æœæœ‰ï¼Œä½¿ç”¨ LLM ç”Ÿæˆé«˜è´¨é‡çš„æ€ç»´å¯¼å›¾
        search_context = None
        for result in reversed(tool_results):
            task_id = result.get("task")
            if task_id == "search":
                search_context = result.get("output", "")[:2000]  # å¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦
                break
        
        # å¦‚æœæœ‰æœç´¢ç»“æœï¼Œä½¿ç”¨ LLM ç”Ÿæˆæ€ç»´å¯¼å›¾
        if search_context:
            try:
                payload = await generate_diagram_payload_with_llm(user_query, search_context, settings)
                tool_args = payload
                action_description = "åŸºäºæœç´¢ç»“æœä½¿ç”¨LLMç”Ÿæˆæ€ç»´å¯¼å›¾"
            except Exception as e:
                logger.warning(f"LLMç”Ÿæˆæ€ç»´å¯¼å›¾å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ–¹æ³•: {e}")
                payload = generate_diagram_payload(user_query, search_context)
                tool_args = payload
                action_description = "åŸºäºæœç´¢ç»“æœç”Ÿæˆæ€ç»´å¯¼å›¾"
        else:
            payload = generate_diagram_payload(user_query, None)
            tool_args = payload
            action_description = "ç”Ÿæˆæ€ç»´å¯¼å›¾"
    elif pending_task == "note":
        weather_result = None
        for result in reversed(tool_results):
            task_id = result.get("task")
            tool_name = result.get("tool_name", "")
            if task_id == "weather" or "å¤©æ°”" in tool_name:
                weather_result = result
                break

        if not weather_result:
            reason = "ç¬”è®°ä»»åŠ¡ä¾èµ–å¤©æ°”ç»“æœï¼Œä½†æœªæ‰¾åˆ°"
            logger.warning(reason)
            return {
                "skipped_tasks": [{"task": "note", "reason": reason}],
                "observations": [reason],
                "thoughts": ["å¤©æ°”ç»“æœæœªæ‰¾åˆ°"],
            }

        weather_text = weather_result.get("output", "")
        if not detect_rain_in_text(weather_text):
            reason = "å¤©æ°”é¢„æŠ¥æ— é™é›¨ï¼Œæ— éœ€æé†’å¸¦ä¼"
            logger.info(reason)
            return {
                "skipped_tasks": [{"task": "note", "reason": reason}],
                "observations": [reason],
                "thoughts": ["ä¸æ»¡è¶³æ¡ä»¶ï¼Œè·³è¿‡"],
            }

        city_from_weather = weather_result.get("arguments", {}).get("city")
        if not city_from_weather:
            city_from_weather = extract_city_from_query(user_query)
        filename = build_note_filename(city_from_weather)
        note_content = build_note_content(city_from_weather, weather_text, user_query)
        tool_args = {"filename": filename, "content": note_content}
        action_description = f"ä¸º{city_from_weather}åˆ›å»ºå¸¦ä¼æé†’"
    else:
        reason = f"æ— æ³•å¤„ç†ä»»åŠ¡ç±»å‹ï¼š{pending_task}"
        logger.warning(reason)
        return {
            "skipped_tasks": [{"task": pending_task, "reason": reason}],
            "observations": [reason],
            "thoughts": ["å·²ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"],
        }

    try:
        result = execute_tool(
            tool=selected_tool,
            arguments=tool_args,
            settings=settings,
            session=session,
        )
    except Exception as exc:
        logger.error("å·¥å…·æ‰§è¡Œå¤±è´¥: %s", exc)
        return {
            "observations": [f"å·¥å…·è°ƒç”¨å¤±è´¥ï¼š{exc}"],
            "error": str(exc),
            "thoughts": ["å·¥å…·æ‰§è¡Œå¤±è´¥"],
        }

    timestamp = datetime.now().isoformat()
    tool_call_record = {
        "tool_id": selected_tool.id,
        "tool_name": selected_tool.name,
        "task": pending_task,
        "arguments": tool_args,
        "result": result,
        "timestamp": timestamp,
    }

    tool_result_record = {
        "tool_name": selected_tool.name,
        "task": pending_task,
        "output": result,
        "arguments": tool_args,
    }

    observation = f"å·¥å…·[{selected_tool.name}] æ‰§è¡Œå®Œæˆï¼š{action_description}"
    if result:
        observation += f"ï¼Œç»“æœï¼š{result[:200]}"

    thought = f"å®Œæˆä»»åŠ¡ {pending_task}ï¼Œè°ƒç”¨äº† {selected_tool.name}"

    return {
        "tool_calls_made": [tool_call_record],
        "tool_results": [tool_result_record],
        "observations": [observation],
        "thoughts": [thought],
    }
def reflector_node(state: AgentState) -> Dict[str, Any]:
    """
    åæ€å™¨èŠ‚ç‚¹ï¼šè¯„ä¼°å½“å‰è¿›å±•ï¼Œå†³å®šæ˜¯å¦éœ€è¦è°ƒæ•´ç­–ç•¥
    """
    logger.info("ğŸ¤” [åæ€å™¨] è¯„ä¼°å½“å‰è¿›å±•...")
    
    user_query = state["user_query"]
    tool_results = state.get("tool_results", [])
    retrieved_contexts = state.get("retrieved_contexts", [])
    current_step = state.get("current_step", 0)
    
    # è¯„ä¼°ä¿¡æ¯å®Œæ•´æ€§
    has_tool_results = len(tool_results) > 0
    has_kb_context = len(retrieved_contexts) > 0
    
    quality_score = 0.0
    reflection = ""
    
    if has_tool_results or has_kb_context:
        quality_score = 0.7
        reflection = "å·²æ”¶é›†åˆ°ç›¸å…³ä¿¡æ¯ï¼Œå¯ä»¥å°è¯•ç”Ÿæˆç­”æ¡ˆ"
    else:
        quality_score = 0.3
        reflection = "ä¿¡æ¯æ”¶é›†ä¸è¶³ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ£€ç´¢æˆ–å·¥å…·è°ƒç”¨"
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦äººå·¥ä»‹å…¥
    needs_human = quality_score < 0.5 and current_step > 3
    
    thought = f"åæ€ç»“æœï¼šè´¨é‡è¯„åˆ† {quality_score:.2f}"
    
    return {
        "reflection": reflection,
        "quality_score": quality_score,
        "needs_human_input": needs_human,
        "thoughts": [thought]
    }


async def synthesizer_node(
    state: AgentState,
    settings: Settings,
) -> Dict[str, Any]:
    """åˆæˆå™¨èŠ‚ç‚¹ï¼šä½¿ç”¨ LLM ç»¼åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
    logger.info("âœ¨ [åˆæˆå™¨] ä½¿ç”¨ LLM ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...")

    user_query = state.get("user_query", "")
    retrieved_contexts = state.get("retrieved_contexts", [])
    tool_results = state.get("tool_results", [])
    skipped_tasks = state.get("skipped_tasks", [])

    # æ„å»ºä¿¡æ¯ä¸Šä¸‹æ–‡
    context_parts: List[str] = []
    
    # 1. æ·»åŠ çŸ¥è¯†åº“æ£€ç´¢å†…å®¹
    if retrieved_contexts:
        kb_content = "\n\n".join([
            f"ã€æ–‡æ¡£ç‰‡æ®µ {i+1}ã€‘\næ¥æºï¼š{ctx.get('original_name', 'æœªçŸ¥')}\nå†…å®¹ï¼š{ctx.get('content', '')[:500]}"
            for i, ctx in enumerate(retrieved_contexts[:3])  # æœ€å¤š3ä¸ªç‰‡æ®µ
        ])
        context_parts.append(f"## çŸ¥è¯†åº“æ£€ç´¢ç»“æœ\n{kb_content}")
    
    # 2. æ·»åŠ å·¥å…·æ‰§è¡Œç»“æœ
    if tool_results:
        tool_outputs = []
        for tr in tool_results:
            tool_name = tr.get("tool_name", "å·¥å…·")
            output = tr.get("output", "")
            tool_outputs.append(f"ã€{tool_name}ã€‘\n{output[:600]}")
        context_parts.append(f"## å·¥å…·æ‰§è¡Œç»“æœ\n" + "\n\n".join(tool_outputs))
    
    # 3. æ·»åŠ è·³è¿‡çš„ä»»åŠ¡è¯´æ˜
    if skipped_tasks:
        skip_info = "\n".join([
            f"- {item.get('task', 'æœªçŸ¥ä»»åŠ¡')}: {item.get('reason', 'æœªè¯´æ˜')}"
            for item in skipped_tasks
        ])
        context_parts.append(f"## è·³è¿‡çš„ä»»åŠ¡\n{skip_info}")
    
    # åˆ¤æ–­æ˜¯å¦æœ‰è¶³å¤Ÿä¿¡æ¯
    has_info = bool(retrieved_contexts or tool_results)
    
    try:
        if not has_info:
            # æ²¡æœ‰ä»»ä½•é¢å¤–ä¿¡æ¯ï¼Œç›´æ¥è®© LLM åŸºäºè‡ªèº«çŸ¥è¯†å›ç­”
            synthesis_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{user_query}

å½“å‰ç³»ç»Ÿæ²¡æœ‰æ£€ç´¢åˆ°çŸ¥è¯†åº“å†…å®¹ï¼Œä¹Ÿæ²¡æœ‰è°ƒç”¨ä»»ä½•å·¥å…·ã€‚
è¯·åŸºäºä½ è‡ªèº«çš„çŸ¥è¯†ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. å¦‚æœä½ çŸ¥é“ç­”æ¡ˆï¼Œè¯·è¯¦ç»†ã€å‡†ç¡®åœ°å›ç­”
2. å¦‚æœä¸ç¡®å®šï¼Œè¯·è¯šå®è¯´æ˜ï¼Œå¹¶ç»™å‡ºå»ºè®®
3. å›ç­”è¦æœ‰æ¡ç†ï¼Œä½¿ç”¨ Markdown æ ¼å¼
4. ä¸è¦ç¼–é€ ä¿¡æ¯
"""
        else:
            # æœ‰ä¿¡æ¯ï¼šè¦æ±‚ LLM ç»¼åˆå›ç­”
            all_context = "\n\n".join(context_parts)
            synthesis_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{user_query}

æˆ‘å·²ç»ä¸ºä½ æ”¶é›†äº†ä»¥ä¸‹ä¿¡æ¯ï¼š

{all_context}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œç»¼åˆå›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. ä¼˜å…ˆå¼•ç”¨å…·ä½“çš„ä¿¡æ¯æ¥æºï¼ˆçŸ¥è¯†åº“æˆ–å·¥å…·ç»“æœï¼‰
2. å¦‚æœä¿¡æ¯ä¸å®Œæ•´ï¼Œè¯·è¯´æ˜ç¼ºå°‘ä»€ä¹ˆ
3. ä¿æŒå®¢è§‚å‡†ç¡®ï¼Œä¸è¦ç¼–é€ å†…å®¹
4. å›ç­”è¦æœ‰æ¡ç†ï¼Œä½¿ç”¨ Markdown æ ¼å¼
5. å¦‚æœæœ‰å·¥å…·æ‰§è¡Œç»“æœï¼Œè¯·é‡ç‚¹çªå‡º
"""
        
        # è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        final_answer, _ = await invoke_llm(
            messages=[{"role": "user", "content": synthesis_prompt}],
            settings=settings,
            temperature=0.7,  # é€‚ä¸­æ¸©åº¦ï¼Œä¿è¯æµç•…æ€§
            max_tokens=2000
        )
        
        logger.info("âœ… LLM æˆåŠŸç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
        
        return {
            "final_answer": final_answer,
            "is_complete": True,
            "thoughts": ["LLM å·²ç”Ÿæˆç»¼åˆç­”æ¡ˆ"],
        }
    
    except Exception as e:
        logger.error(f"åˆæˆå™¨ LLM å¤±è´¥: {e}")
        
        # é™çº§ç­–ç•¥ï¼šä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²æ‹¼æ¥
        results_by_task: Dict[str, List[Dict[str, Any]]] = {}
        for result in tool_results:
            task_key = result.get("task") or ""
            results_by_task.setdefault(task_key, []).append(result)

        sections: List[str] = []

        def truncate(text: str, limit: int = 400) -> str:
            if not text:
                return ""
            cleaned = text.strip()
            return cleaned if len(cleaned) <= limit else cleaned[:limit] + "..."

        weather_results = results_by_task.get("weather")
        if weather_results:
            latest_weather = weather_results[-1]
            city = latest_weather.get("arguments", {}).get("city")
            heading = "### å¤©æ°”ä¿¡æ¯" + (f"ï¼ˆ{city}ï¼‰" if city else "")
            sections.append(f"{heading}\n{truncate(latest_weather.get('output', ''))}")

        search_results = results_by_task.get("search")
        if search_results:
            sections.append("### æœç´¢ç»“æœ\n" + truncate(search_results[-1].get("output", "")))

        diagram_results = results_by_task.get("diagram")
        if diagram_results:
            sections.append("### æ€ç»´å¯¼å›¾\n" + truncate(diagram_results[-1].get("output", ""), limit=200))

        note_results = results_by_task.get("note")
        if note_results:
            sections.append("### æé†’ç¬”è®°\n" + truncate(note_results[-1].get("output", "")))

        if not sections and retrieved_contexts:
            first_ctx = retrieved_contexts[0]
            origin = first_ctx.get("original_name", "æœªçŸ¥")
            sections.append(
                f"### çŸ¥è¯†åº“å†…å®¹ï¼ˆæ¥è‡ª{origin}ï¼‰\n" + truncate(first_ctx.get("content", ""))
            )

        if not sections:
            final_answer = (
                f"å…³äºæ‚¨çš„é—®é¢˜ã€Œ{user_query}ã€ï¼Œæˆ‘ç›®å‰æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„ä¿¡æ¯ã€‚\n\n"
                "å»ºè®®ï¼š\n"
                "1. æ‚¨å¯ä»¥å°è¯•ä¸Šä¼ ç›¸å…³æ–‡æ¡£åˆ°çŸ¥è¯†åº“\n"
                "2. æˆ–è€…æ¢ä¸€ä¸ªæ›´å…·ä½“çš„é—®é¢˜\n\n"
                f"ï¼ˆæ³¨ï¼šç³»ç»Ÿå½“å‰ä½¿ç”¨é™çº§æ¨¡å¼ï¼ŒåŸå› ï¼š{str(e)[:100]}ï¼‰"
            )
        else:
            summary_intro = f"æ ¹æ®æ‚¨çš„é—®é¢˜ã€Œ{user_query}ã€ï¼Œä¸ºæ‚¨æ•´ç†å¦‚ä¸‹ï¼š" if user_query else "ä»¥ä¸‹æ˜¯ä¸ºæ‚¨æ‰¾åˆ°çš„ä¿¡æ¯ï¼š"
            final_answer = summary_intro + "\n\n" + "\n\n".join(sections)

        return {
            "final_answer": final_answer,
            "is_complete": True,
            "thoughts": [f"ä½¿ç”¨é™çº§æ¨¡å¼ç”Ÿæˆç­”æ¡ˆï¼ˆLLM å¼‚å¸¸ï¼š{str(e)[:50]}ï¼‰"],
        }

def human_input_node(state: AgentState) -> Dict[str, Any]:
    """
    äººå·¥ä»‹å…¥èŠ‚ç‚¹ï¼šæš‚åœæ‰§è¡Œï¼Œç­‰å¾…äººå·¥åé¦ˆ
    """
    logger.info("ğŸ‘¤ [äººå·¥ä»‹å…¥] ç­‰å¾…äººå·¥åé¦ˆ...")
    
    # è¿™ä¸ªèŠ‚ç‚¹ä¼šæš‚åœæ‰§è¡Œï¼Œç­‰å¾…å¤–éƒ¨è¾“å…¥
    # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œéœ€è¦é€šè¿‡ API æ¥æ¢å¤æ‰§è¡Œ
    
    return {
        "thoughts": ["ç­‰å¾…äººå·¥åé¦ˆä¸­..."],
        "needs_human_input": True
    }


# ==================== è¾…åŠ©å‡½æ•° ====================

TASK_ORDER: List[str] = ["weather", "search", "diagram", "note"]  # æ‰§è¡Œé¡ºåºï¼šç¡®ä¿æœç´¢åœ¨ç»˜å›¾å‰

TASK_KEYWORDS: Dict[str, List[str]] = {
    "weather": ["å¤©æ°”", "æ°”æ¸©", "ä¸‹é›¨", "é™é›¨", "é›¨ä¼", "rain", "weather", "forecast", "æ˜å¤©", "ä»Šå¤©", "åå¤©"],
    "search": ["æœç´¢", "æŸ¥æ‰¾", "æœä¸€ä¸‹", "è°ƒæŸ¥", "look up", "research", "æ‰©æ•£æ¨¡å‹", "æœ€æ–°è¿›å±•"],
    "diagram": ["æ€ç»´å¯¼å›¾", "æµç¨‹å›¾", "ç”»å›¾", "ç»˜åˆ¶", "diagram", "flowchart", "ç»“æ„å›¾", "å›¾è¡¨", "å¯¼å›¾"],
    "note": ["ç¬”è®°", "æé†’", "è®°å½•", "å¤‡å¿˜", "è®°ä¸‹æ¥", "note", "å¸¦ä¼", "æé†’æˆ‘"],
}

RAIN_KEYWORDS: List[str] = [
    "é›¨", "é˜µé›¨", "é›·é˜µé›¨", "å°é›¨", "ä¸­é›¨", "å¤§é›¨", "æš´é›¨", "é›¨å¤¹é›ª", "é™é›¨", "rain", "shower", "storm", "drizzle"
]

COMMON_CHINESE_CITIES: List[str] = [
    "åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·", "æ·±åœ³", "å¤©æ´¥", "æ­å·", "å—äº¬", "æ­¦æ±‰",
    "æˆéƒ½", "é‡åº†", "è¥¿å®‰", "è‹å·", "é•¿æ²™", "é’å²›", "å¦é—¨", "å¤§è¿"
]

ENGLISH_CITY_ALIASES: Dict[str, str] = {
    "beijing": "åŒ—äº¬",
    "shanghai": "ä¸Šæµ·",
    "guangzhou": "å¹¿å·",
    "shenzhen": "æ·±åœ³",
    "tianjin": "å¤©æ´¥",
    "hangzhou": "æ­å·",
    "nanjing": "å—äº¬",
    "wuhan": "æ­¦æ±‰",
    "chengdu": "æˆéƒ½",
    "chongqing": "é‡åº†",
    "xian": "è¥¿å®‰",
    "suzhou": "è‹å·",
    "changsha": "é•¿æ²™",
    "qingdao": "é’å²›",
    "xiamen": "å¦é—¨",
    "dalian": "å¤§è¿"
}

CITY_SLUG_OVERRIDES: Dict[str, str] = {
    "åŒ—äº¬": "beijing",
    "ä¸Šæµ·": "shanghai",
    "å¹¿å·": "guangzhou",
    "æ·±åœ³": "shenzhen",
    "å¤©æ´¥": "tianjin",
    "æ­å·": "hangzhou",
    "å—äº¬": "nanjing",
    "æ­¦æ±‰": "wuhan",
    "æˆéƒ½": "chengdu",
    "é‡åº†": "chongqing",
    "è¥¿å®‰": "xian",
    "è‹å·": "suzhou",
    "é•¿æ²™": "changsha",
    "é’å²›": "qingdao",
    "å¦é—¨": "xiamen",
    "å¤§è¿": "dalian"
}

SEARCH_PREFIXES: List[str] = [
    "å¸®æˆ‘æœç´¢", "è¯·æœç´¢", "æœç´¢ä¸€ä¸‹", "æŸ¥ä¸€ä¸‹", "æŸ¥è¯¢ä¸€ä¸‹", "å¸®æˆ‘æŸ¥", "è¯·å¸®æˆ‘æŸ¥", "å¸®æˆ‘æ‰¾", "æ‰¾ä¸€ä¸‹", "è¯·å¸®æˆ‘æœç´¢"
]

SEARCH_SUFFIXES: List[str] = [
    "å¹¶æ€»ç»“", "å¹¶ç”»", "å¹¶å¸®æˆ‘", "å¹¶å†™", "ç„¶å", "é¡ºä¾¿", "åŒæ—¶", "æ€»ç»“", "æé†’", "å†™ä¸ªç¬”è®°", "ç”»ä¸ª", "å¸¦ä¼"
]

MAX_TOOL_CALLS = 5

def infer_tool_tasks(query: str) -> List[str]:
    """ä»æŸ¥è¯¢æ¨æ–­éœ€è¦çš„å·¥å…·ä»»åŠ¡ï¼ˆæ”¹è¿›ç‰ˆï¼šæ”¯æŒä¸Šä¸‹æ–‡ç†è§£ï¼‰"""
    if not query:
        return []
    
    normalized = query.lower()
    query_original = query
    
    # ä»»åŠ¡åŒ¹é…åˆ†æ•°
    task_scores: Dict[str, int] = {task: 0 for task in TASK_ORDER}
    
    # 1. å¤©æ°”ä»»åŠ¡æ£€æµ‹ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
    weather_indicators = ["å¤©æ°”", "æ°”æ¸©", "ä¸‹é›¨", "é™é›¨", "æ˜å¤©", "ä»Šå¤©", "åå¤©", "weather", "forecast"]
    for indicator in weather_indicators:
        if indicator in query_original or indicator in normalized:
            task_scores["weather"] += 10  # é«˜æƒé‡
    
    # å¦‚æœæåˆ°åŸå¸‚å+æ—¶é—´è¯ï¼Œå¤§æ¦‚ç‡æ˜¯å¤©æ°”æŸ¥è¯¢
    has_city = any(city in query_original for city in COMMON_CHINESE_CITIES)
    has_time = any(t in query_original for t in ["æ˜å¤©", "ä»Šå¤©", "åå¤©", "tomorrow", "today"])
    if has_city and has_time:
        task_scores["weather"] += 15
    
    # 2. æœç´¢ä»»åŠ¡æ£€æµ‹
    search_strong_keywords = ["æœç´¢", "æŸ¥æ‰¾", "æœä¸€ä¸‹", "research", "æœ€æ–°è¿›å±•", "æ‰©æ•£æ¨¡å‹"]
    for keyword in search_strong_keywords:
        if keyword in query_original or keyword in normalized:
            task_scores["search"] += 8
    
    # 3. å›¾è¡¨ä»»åŠ¡æ£€æµ‹
    diagram_keywords = ["æ€ç»´å¯¼å›¾", "æµç¨‹å›¾", "ç”»å›¾", "ç»˜åˆ¶", "diagram", "å¯¼å›¾", "ç”»ä¸ª"]
    for keyword in diagram_keywords:
        if keyword in query_original or keyword in normalized:
            task_scores["diagram"] += 10
    
    # 4. ç¬”è®°ä»»åŠ¡æ£€æµ‹
    note_keywords = ["ç¬”è®°", "æé†’", "è®°å½•", "å¤‡å¿˜", "å¸¦ä¼", "æé†’æˆ‘", "å†™ä¸ªç¬”è®°"]
    for keyword in note_keywords:
        if keyword in query_original or keyword in normalized:
            task_scores["note"] += 10
    
    # æŒ‰TASK_ORDERé¡ºåºè¿‡æ»¤å‡ºå¾—åˆ†>0çš„ä»»åŠ¡ï¼ˆä¿æŒä¼˜å…ˆçº§ï¼Œä¸æŒ‰åˆ†æ•°æ’åºï¼‰
    result = []
    for task in TASK_ORDER:
        if task_scores[task] > 0:
            result.append(task)
    
    logger.info(f"ä»»åŠ¡æ¨æ–­ç»“æœï¼šæŸ¥è¯¢='{query[:50]}...' -> ä»»åŠ¡={result}, å¾—åˆ†={dict(task_scores)}")
    
    return result

def map_tool_to_task(tool: ToolRecord) -> Optional[str]:
    """æ˜ å°„å·¥å…·è®°å½•åˆ°ä»»åŠ¡ç±»å‹"""
    try:
        config = json.loads(tool.config or "{}")
    except json.JSONDecodeError:
        return None
    if tool.tool_type != "builtin":
        return None
    builtin_key = config.get("builtin_key")
    mapping = {
        "get_weather": "weather",
        "web_search": "search",
        "draw_diagram": "diagram",
        "write_note": "note",
    }
    return mapping.get(builtin_key)

def should_call_tool(state: AgentState) -> bool:
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»§ç»­è°ƒç”¨å·¥å…·"""
    previous_calls = state.get("tool_calls_made", [])
    if len(previous_calls) >= MAX_TOOL_CALLS:
        return False

    user_query = state.get("user_query", "")
    tasks = infer_tool_tasks(user_query)
    if not tasks:
        return False

    completed_tasks = {call.get("task") for call in previous_calls if call.get("task")}
    skipped_task_keys = {
        item.get("task")
        for item in state.get("skipped_tasks", [])
        if isinstance(item, dict) and item.get("task")
    }

    for task in tasks:
        if task in completed_tasks or task in skipped_task_keys:
            continue
        if task == "note" and "weather" in tasks and "weather" not in completed_tasks and "weather" not in skipped_task_keys:
            continue
        return True

    return False

def extract_city_from_query(query: str) -> str:
    """ä»æŸ¥è¯¢ä¸­æå–åŸå¸‚åï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰"""
    if not query:
        return "åŒ—äº¬"

    for city in COMMON_CHINESE_CITIES:
        if city in query:
            return city

    lower_query = query.lower()
    for alias, city in ENGLISH_CITY_ALIASES.items():
        if alias in lower_query:
            return city

    match_cn = re.search(r"([ä¸€-é¾¥]{2,5})(?:å¤©æ°”|æ˜å¤©|ä»Šæ—¥|ç°åœ¨|æœªæ¥)", query)
    if match_cn:
        return match_cn.group(1)

    match_en = re.search(r"in\s+([A-Za-z\s]+)", query, flags=re.IGNORECASE)
    if match_en:
        candidate = match_en.group(1).strip()
        alias = candidate.lower()
        if alias in ENGLISH_CITY_ALIASES:
            return ENGLISH_CITY_ALIASES[alias]
        return candidate.title()

    return "åŒ—äº¬"

def extract_search_query(query: str) -> str:
    """ä»æŸ¥è¯¢ä¸­æå–æœç´¢å…³é”®è¯"""
    if not query:
        return ""

    cleaned = query.strip()
    for prefix in SEARCH_PREFIXES:
        if cleaned.startswith(prefix):
            cleaned = cleaned[len(prefix):].strip()
            break

    for suffix in SEARCH_SUFFIXES:
        idx = cleaned.find(suffix)
        if idx > 0:
            cleaned = cleaned[:idx].strip()
            break

    cleaned = cleaned.strip("ï¼Œã€‚,.!?ï¼›; ")
    return cleaned or query.strip()

async def generate_diagram_payload_with_llm(
    user_query: str, 
    search_context: Optional[str], 
    settings: Settings
) -> Dict[str, str]:
    """ä½¿ç”¨ LLM ç”Ÿæˆé«˜è´¨é‡çš„æ€ç»´å¯¼å›¾å†…å®¹"""
    topic_source = user_query or "ä¸»é¢˜"
    diagram_type = "mindmap" if any(keyword in topic_source for keyword in ["æ€ç»´å¯¼å›¾", "å¯¼å›¾", "mindmap"]) else "flowchart"
    
    # æå–ä¸»é¢˜ï¼ˆæ¸…ç†ç”¨æˆ·æŸ¥è¯¢ï¼‰
    topic = topic_source
    for prefix in ["å¸®æˆ‘æœç´¢", "æœç´¢", "ç”»ä¸ª", "ç»˜åˆ¶", "ç”Ÿæˆ"]:
        topic = topic.replace(prefix, "")
    for suffix in ["æ€»ç»“å…³é”®ç‚¹", "å¹¶ç”»ä¸ªæ€ç»´å¯¼å›¾", "ç”»ä¸ªæ€ç»´å¯¼å›¾", "æ€ç»´å¯¼å›¾"]:
        topic = topic.replace(suffix, "")
    topic = topic.strip("ï¼Œã€‚ã€ ")
    if len(topic) > 30:
        topic = topic[:30]

    if diagram_type == "mindmap":
        # ä½¿ç”¨ LLM åˆ†æå’Œæ€»ç»“æœç´¢ç»“æœï¼Œç”Ÿæˆç»“æ„åŒ–æ€ç»´å¯¼å›¾
        prompt = f"""åŸºäºä»¥ä¸‹æœç´¢ç»“æœï¼Œç”Ÿæˆä¸€ä¸ªå…³äºã€Œ{topic}ã€çš„æ€ç»´å¯¼å›¾ï¼ˆMermaid mindmap æ ¼å¼ï¼‰ã€‚

æœç´¢ç»“æœï¼š
{search_context[:2000]}

è¦æ±‚ï¼š
1. æå–æœç´¢ç»“æœçš„**æ ¸å¿ƒå…³é”®ç‚¹**ï¼Œå½¢æˆ3-5ä¸ªä¸»è¦åˆ†æ”¯
2. æ¯ä¸ªåˆ†æ”¯è¦æœ‰æ¸…æ™°çš„å­åˆ†æ”¯ï¼ˆ2-3ä¸ªï¼‰
3. ä½¿ç”¨ç®€æ´ã€ä¸“ä¸šçš„ä¸­æ–‡æè¿°ï¼Œé¿å…ç›´æ¥å¤åˆ¶æœç´¢ç»“æœæ–‡æœ¬
4. ç¡®ä¿æ€ç»´å¯¼å›¾ç»“æ„æ¸…æ™°ã€é€»è¾‘åˆç†
5. åªè¾“å‡º Mermaid mindmap ä»£ç ï¼Œä¸è¦å…¶ä»–è§£é‡Š

æ ¼å¼ç¤ºä¾‹ï¼š
```mermaid
mindmap
  root((ä¸»é¢˜))
    ä¸»è¦åˆ†æ”¯1
      å­åˆ†æ”¯1.1
      å­åˆ†æ”¯1.2
    ä¸»è¦åˆ†æ”¯2
      å­åˆ†æ”¯2.1
      å­åˆ†æ”¯2.2
```

è¯·ç”Ÿæˆæ€ç»´å¯¼å›¾ï¼š"""
        
        try:
            llm_response, _ = await invoke_llm(
                messages=[{"role": "user", "content": prompt}],
                settings=settings,
                temperature=0.7,
                max_tokens=800
            )
            
            # æå– Mermaid ä»£ç å—
            diagram_code = llm_response.strip()
            
            # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
            if "```mermaid" in diagram_code:
                diagram_code = diagram_code.split("```mermaid")[1].split("```")[0].strip()
            elif "```" in diagram_code:
                diagram_code = diagram_code.split("```")[1].split("```")[0].strip()
            
            # ç¡®ä¿æ˜¯ mindmap æ ¼å¼
            if not diagram_code.startswith("mindmap"):
                # å¦‚æœ LLM æ²¡æœ‰ç”Ÿæˆæ­£ç¡®çš„æ ¼å¼ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿
                logger.warning("LLM ç”Ÿæˆçš„æ€ç»´å¯¼å›¾æ ¼å¼ä¸æ­£ç¡®ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿")
                diagram_code = f"""mindmap
  root(({topic}))
    æ ¸å¿ƒæ¦‚å¿µ
      å®šä¹‰ä¸ç‰¹ç‚¹
      åº”ç”¨é¢†åŸŸ
    æœ€æ–°è¿›å±•
      æŠ€æœ¯çªç ´
      è¡Œä¸šåŠ¨æ€
    å‘å±•è¶‹åŠ¿
      æœªæ¥æ–¹å‘
      æ½œåœ¨å½±å“"""
        except Exception as e:
            logger.error(f"LLM ç”Ÿæˆæ€ç»´å¯¼å›¾å¤±è´¥: {e}")
            raise  # è®©è°ƒç”¨è€…å¤„ç†é”™è¯¯
        
        filename = f"{topic[:20].replace(' ', '_').replace('/', '_')}_mindmap.md"
    else:
        # æµç¨‹å›¾ç±»å‹ï¼ˆæš‚æ—¶ä¸éœ€è¦LLMï¼Œä½¿ç”¨ç®€å•æ¨¡æ¿ï¼‰
        diagram_code = f"""flowchart TD
    A[éœ€æ±‚ï¼š{topic[:20]}] --> B{{ä¿¡æ¯æ”¶é›†}}
    B --> C[åˆ†æå¤„ç†]
    C --> D{{å†³ç­–}}
    D --> E[æ‰§è¡Œ]
    E --> F[å®Œæˆ]"""
        filename = f"{topic[:20].replace(' ', '_').replace('/', '_')}_flowchart.md"

    return {
        "filename": filename,
        "diagram_code": diagram_code,
        "diagram_type": diagram_type
    }


def generate_diagram_payload(user_query: str, search_context: Optional[str] = None) -> Dict[str, str]:
    """ç”Ÿæˆæ€ç»´å¯¼å›¾çš„å‚æ•°ï¼ˆæ™ºèƒ½ç‰ˆï¼šåŸºäºæœç´¢ç»“æœï¼‰"""
    topic_source = user_query or "ä¸»é¢˜"
    diagram_type = "mindmap" if any(keyword in topic_source for keyword in ["æ€ç»´å¯¼å›¾", "å¯¼å›¾", "mindmap"]) else "flowchart"
    
    # æå–ä¸»é¢˜ï¼ˆæ¸…ç†ç”¨æˆ·æŸ¥è¯¢ï¼‰
    topic = topic_source
    for prefix in ["å¸®æˆ‘æœç´¢", "æœç´¢", "ç”»ä¸ª", "ç»˜åˆ¶", "ç”Ÿæˆ"]:
        topic = topic.replace(prefix, "")
    for suffix in ["æ€»ç»“å…³é”®ç‚¹", "å¹¶ç”»ä¸ªæ€ç»´å¯¼å›¾", "ç”»ä¸ªæ€ç»´å¯¼å›¾", "æ€ç»´å¯¼å›¾"]:
        topic = topic.replace(suffix, "")
    topic = topic.strip("ï¼Œã€‚ã€ ")
    if len(topic) > 30:
        topic = topic[:30]

    if diagram_type == "mindmap":
        # å¦‚æœæœ‰æœç´¢ç»“æœï¼Œå°è¯•æå–å…³é”®ç‚¹
        if search_context:
            # ç®€å•çš„å…³é”®ç‚¹æå–ï¼ˆå®é™…åº”è¯¥ç”¨ LLMï¼‰
            lines = search_context.split('\n')
            key_points = []
            for line in lines[:6]:  # æœ€å¤š6ä¸ªå…³é”®ç‚¹
                line = line.strip()
                if line and len(line) > 10 and len(line) < 100:
                    # æ¸…ç†æ— ç”¨å­—ç¬¦
                    line = re.sub(r'^\d+[\.ã€]', '', line)  # ç§»é™¤åºå·
                    line = re.sub(r'^[â€¢\-\*]', '', line).strip()  # ç§»é™¤åˆ—è¡¨ç¬¦å·
                    if line:
                        key_points.append(line[:50])  # é™åˆ¶é•¿åº¦
            
            # ç”ŸæˆåŸºäºå†…å®¹çš„æ€ç»´å¯¼å›¾
            if key_points:
                points_section = []
                for i, point in enumerate(key_points[:4], 1):  # æœ€å¤š4ä¸ªä¸»è¦åˆ†æ”¯
                    points_section.append(f"    åˆ†æ”¯{i}ï¼š{point[:25]}")
                    if i < len(key_points):
                        points_section.append(f"      è¯¦ç»†{chr(65+i)}")
                
                diagram_code = f"""mindmap
  root(({topic}))
{chr(10).join(points_section)}"""
            else:
                # å›é€€åˆ°é€šç”¨æ¨¡æ¿
                diagram_code = f"""mindmap
  root(({topic}))
    æ ¸å¿ƒæ¦‚å¿µ
      å®šä¹‰
      ç‰¹ç‚¹
    åº”ç”¨åœºæ™¯
      é¢†åŸŸ1
      é¢†åŸŸ2
    å‘å±•è¶‹åŠ¿
      æœ€æ–°è¿›å±•
      æœªæ¥æ–¹å‘"""
        else:
            # æ²¡æœ‰æœç´¢ç»“æœï¼Œä½¿ç”¨é€šç”¨æ¨¡æ¿
            diagram_code = f"""mindmap
  root(({topic}))
    ä¿¡æ¯æ”¶é›†
      å…³é”®ç‚¹1
      å…³é”®ç‚¹2
    åˆ†æåˆ¤æ–­
      é£é™©
      æœºä¼š
    è¡ŒåŠ¨æ–¹æ¡ˆ
      ä¸‹ä¸€æ­¥å»ºè®®"""
        
        filename = f"{topic[:20].replace(' ', '_')}_mindmap.md"
    else:
        # æµç¨‹å›¾ç±»å‹
        diagram_code = f"""flowchart TD
    A[éœ€æ±‚ï¼š{topic[:20]}] --> B{{ä¿¡æ¯æ”¶é›†}}
    B --> C[åˆ†æå¤„ç†]
    C --> D{{å†³ç­–}}
    D --> E[æ‰§è¡Œ]
    E --> F[å®Œæˆ]"""
        filename = f"{topic[:20].replace(' ', '_')}_flowchart.md"

    return {
        "filename": filename,
        "diagram_code": diagram_code,
        "diagram_type": diagram_type,
    }

def detect_rain_in_text(text: str) -> bool:
    """æ£€æµ‹æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«é™é›¨ä¿¡æ¯"""
    if not text:
        return False
    lowered = text.lower()
    return any(keyword in text or keyword in lowered for keyword in RAIN_KEYWORDS)

def build_note_filename(city: str) -> str:
    """æ„å»ºç¬”è®°æ–‡ä»¶å"""
    slug = CITY_SLUG_OVERRIDES.get(city, city)
    slug = re.sub(r"[^A-Za-z0-9]+", "-", slug).strip("-").lower() or "reminder"
    timestamp = datetime.now().strftime("%Y%m%d")
    return f"{slug}_umbrella_{timestamp}.txt"

def summarize_for_note(text: str, limit: int = 200) -> str:
    """æ€»ç»“æ–‡æœ¬ç”¨äºç¬”è®°"""
    if not text:
        return "å¤©æ°”ä¿¡æ¯ç¼ºå¤±"
    clean_text = text.replace("\r", " ").replace("\n\n", "\n")
    clean_text = clean_text.replace("\n", " ")
    return clean_text.strip()[:limit]

def build_note_content(city: str, weather_text: str, user_query: str) -> str:
    """æ„å»ºç¬”è®°å†…å®¹"""
    summary = summarize_for_note(weather_text)
    now_str = datetime.now().strftime("%Y-%m-%d %H:%M")
    note_lines = [
        f"# {city}å¸¦ä¼æé†’",
        "",
        f"åˆ›å»ºæ—¶é—´ï¼š{now_str}",
        f"è§¦å‘æŸ¥è¯¢ï¼š{user_query}",
        "",
        "## å¤©æ°”æƒ…å†µ",
        summary,
        "",
        "## æ¸©é¦¨æç¤º",
        "- ä»Šæ—¥å¯èƒ½æœ‰é™é›¨ï¼Œå»ºè®®æºå¸¦é›¨å…·",
        "- å‡ºé—¨å‰è¯·å†æ¬¡æŸ¥çœ‹æœ€æ–°å¤©æ°”",
    ]
    return "\n".join(note_lines) + "\n"

def route_after_planning(state: AgentState) -> str:
    """è§„åˆ’åçš„è·¯ç”±"""
    return "router"


def route_after_routing(state: AgentState) -> str:
    """è·¯ç”±å™¨ä¹‹åçš„è·¯ç”±"""
    next_action = state.get("next_action", "synthesize")
    
    if next_action == "search_kb":
        return "knowledge_search"
    elif next_action == "tool_executor":
        return "tool_executor"
    elif next_action == "synthesize":
        return "reflector"
    else:
        return "synthesizer"


def route_after_knowledge_search(state: AgentState) -> str:
    """çŸ¥è¯†åº“æœç´¢åçš„è·¯ç”±"""
    return "router"


def route_after_tool_execution(state: AgentState) -> str:
    """å·¥å…·æ‰§è¡Œåçš„è·¯ç”±"""
    return "router"


def route_after_reflection(state: AgentState) -> str:
    """åæ€åçš„è·¯ç”±"""
    needs_human = state.get("needs_human_input", False)
    quality_score = state.get("quality_score", 0.0)
    
    if needs_human:
        return "human_input"
    else:
        return "synthesizer"


def route_after_human_input(state: AgentState) -> str:
    """äººå·¥ä»‹å…¥åçš„è·¯ç”±"""
    human_feedback = state.get("human_feedback", "")
    
    if human_feedback:
        return "router"  # æ ¹æ®äººå·¥åé¦ˆé‡æ–°è·¯ç”±
    else:
        return "synthesizer"  # å¦‚æœæ²¡æœ‰åé¦ˆï¼Œç›´æ¥åˆæˆç­”æ¡ˆ


def should_end(state: AgentState) -> str:
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»“æŸ"""
    is_complete = state.get("is_complete", False)
    
    if is_complete:
        return END
    else:
        return "continue"


# ==================== å·¥ä½œæµæ„å»º ====================

def create_agent_graph(
    settings: Settings,
    session: Session,
    tool_records: List[ToolRecord],
    checkpoint_dir: str = "backend/data/checkpoints"
) -> StateGraph:
    """
    åˆ›å»ºå®Œæ•´çš„ LangGraph Agent å·¥ä½œæµï¼ˆæ”¯æŒå¼‚æ­¥èŠ‚ç‚¹ï¼‰
    """
    logger.info("ğŸ—ï¸ æ„å»º LangGraph Agent å·¥ä½œæµ...")
    
    # åˆ›å»ºå›¾
    workflow = StateGraph(AgentState)
    
    # åˆ›å»ºå¼‚æ­¥èŠ‚ç‚¹åŒ…è£…å™¨
    async def planner_wrapper(state: AgentState) -> Dict[str, Any]:
        return await planner_node(state, settings, tool_records)
    
    async def router_wrapper(state: AgentState) -> Dict[str, Any]:
        return await router_node(state, settings)
    
    async def synthesizer_wrapper(state: AgentState) -> Dict[str, Any]:
        return await synthesizer_node(state, settings)
    
    async def tool_executor_wrapper(state: AgentState) -> Dict[str, Any]:
        return await tool_executor_node(state, settings, session, tool_records)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("planner", planner_wrapper)
    workflow.add_node("router", router_wrapper)
    workflow.add_node(
        "knowledge_search",
        lambda state: knowledge_search_node(state, settings)
    )
    workflow.add_node("tool_executor", tool_executor_wrapper)
    workflow.add_node("reflector", reflector_node)
    workflow.add_node("synthesizer", synthesizer_wrapper)
    # æš‚æ—¶ç¦ç”¨äººå·¥ä»‹å…¥èŠ‚ç‚¹ï¼ˆæœªå®Œå…¨å®ç°ï¼‰
    # workflow.add_node("human_input", human_input_node)
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("planner")
    
    # æ·»åŠ è¾¹ï¼ˆå®šä¹‰æµç¨‹ï¼‰
    workflow.add_edge("planner", "router")
    
    # è·¯ç”±å™¨çš„æ¡ä»¶è¾¹
    workflow.add_conditional_edges(
        "router",
        route_after_routing,
        {
            "knowledge_search": "knowledge_search",
            "tool_executor": "tool_executor",
            "reflector": "reflector",
            "synthesizer": "synthesizer"
        }
    )
    
    workflow.add_edge("knowledge_search", "router")
    workflow.add_edge("tool_executor", "router")
    workflow.add_edge("reflector", "synthesizer")
    
    # åˆæˆå™¨åç»“æŸ
    workflow.add_edge("synthesizer", END)
    
    # äººå·¥ä»‹å…¥æµç¨‹ï¼ˆå¯é€‰ï¼‰
    # workflow.add_conditional_edges(
    #     "reflector",
    #     route_after_reflection,
    #     {
    #         "human_input": "human_input",
    #         "synthesizer": "synthesizer"
    #     }
    # )
    # workflow.add_edge("human_input", "router")
    
    logger.info("âœ… LangGraph Agent å·¥ä½œæµæ„å»ºå®Œæˆ")
    
    return workflow


async def run_agent(
    user_query: str,
    settings: Settings,
    session: Session,
    tool_records: List[ToolRecord],
    use_knowledge_base: bool = False,
    conversation_history: List[Dict[str, str]] = None,
) -> Dict[str, Any]:
    """
    è¿è¡Œ LangGraph Agent
    
    Args:
        user_query: ç”¨æˆ·é—®é¢˜
        settings: é…ç½®
        session: æ•°æ®åº“ä¼šè¯
        tool_records: å¯ç”¨å·¥å…·åˆ—è¡¨
        use_knowledge_base: æ˜¯å¦ä½¿ç”¨çŸ¥è¯†åº“
        conversation_history: å¯¹è¯å†å²
    
    Returns:
        åŒ…å« Agent å®Œæ•´æ‰§è¡Œè¿‡ç¨‹çš„å­—å…¸
    """
    logger.info(f"ğŸš€ å¯åŠ¨ LangGraph Agent å¤„ç†é—®é¢˜: {user_query}")
    
    # æ„å»ºå·¥ä½œæµ
    workflow = create_agent_graph(settings, session, tool_records)
    
    # ç¼–è¯‘å›¾ï¼ˆä½¿ç”¨å†…å­˜æ£€æŸ¥ç‚¹ï¼‰
    # æ³¨æ„ï¼šMemorySaver åœ¨æœåŠ¡å™¨é‡å¯åä¼šä¸¢å¤±çŠ¶æ€ï¼Œä½†åŠŸèƒ½å®Œå…¨æ­£å¸¸
    checkpointer = MemorySaver()
    app = workflow.compile(checkpointer=checkpointer)
    
    # åˆå§‹åŒ–çŠ¶æ€
    initial_state: AgentState = {
        "user_query": user_query,
        "conversation_history": conversation_history or [],
        "plan": None,
        "current_step": 0,
        "max_iterations": 10,
        "available_tools": [tool.id for tool in tool_records],
        "tool_calls_made": [],
        "tool_results": [],
        "skipped_tasks": [],
        "use_knowledge_base": use_knowledge_base,
        "retrieved_contexts": [],
        "thoughts": [],
        "observations": [],
        "next_action": None,
        "needs_human_input": False,
        "human_feedback": None,
        "reflection": None,
        "quality_score": 0.0,
        "final_answer": None,
        "is_complete": False,
        "error": None
    }
    
    # ç”Ÿæˆå”¯ä¸€çš„çº¿ç¨‹IDï¼ˆç”¨äºæ£€æŸ¥ç‚¹ï¼‰
    thread_id = str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}
    
    # æ‰§è¡Œå·¥ä½œæµ
    try:
        final_state = await app.ainvoke(initial_state, config=config)
        
        logger.info("âœ… LangGraph Agent æ‰§è¡Œå®Œæˆ")
        
        return {
            "success": True,
            "final_answer": final_state.get("final_answer", "æœªèƒ½ç”Ÿæˆç­”æ¡ˆ"),
            "thoughts": final_state.get("thoughts", []),
            "observations": final_state.get("observations", []),
            "tool_results": final_state.get("tool_results", []),
            "retrieved_contexts": final_state.get("retrieved_contexts", []),
            "plan": final_state.get("plan", ""),
            "quality_score": final_state.get("quality_score", 0.0),
            "reflection": final_state.get("reflection", ""),
            "thread_id": thread_id,
            "error": final_state.get("error")
        }
    
    except Exception as e:
        logger.error(f"âŒ LangGraph Agent æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        return {
            "success": False,
            "final_answer": f"æŠ±æ­‰ï¼Œå¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}",
            "error": str(e),
            "thoughts": [],
            "observations": [],
            "tool_results": [],
            "skipped_tasks": [],
            "retrieved_contexts": []
        }


async def stream_agent(
    user_query: str,
    settings: Settings,
    session: Session,
    tool_records: List[ToolRecord],
    use_knowledge_base: bool = False,
    conversation_history: List[Dict[str, str]] = None,
):
    """
    æµå¼è¿è¡Œ LangGraph Agentï¼Œå®æ—¶è¿”å›æ¯ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œç»“æœ
    
    ç”¨äºå‰ç«¯å®æ—¶å±•ç¤º Agent çš„æ€è€ƒè¿‡ç¨‹
    """
    logger.info(f"ğŸŒŠ å¯åŠ¨æµå¼ LangGraph Agent: {user_query}")
    
    workflow = create_agent_graph(settings, session, tool_records)
    checkpointer = MemorySaver()
    app = workflow.compile(checkpointer=checkpointer)
    
    initial_state: AgentState = {
        "user_query": user_query,
        "conversation_history": conversation_history or [],
        "plan": None,
        "current_step": 0,
        "max_iterations": 10,
        "available_tools": [tool.id for tool in tool_records],
        "tool_calls_made": [],
        "tool_results": [],
        "skipped_tasks": [],
        "use_knowledge_base": use_knowledge_base,
        "retrieved_contexts": [],
        "thoughts": [],
        "observations": [],
        "next_action": None,
        "needs_human_input": False,
        "human_feedback": None,
        "reflection": None,
        "quality_score": 0.0,
        "final_answer": None,
        "is_complete": False,
        "error": None
    }
    
    thread_id = str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}
    
    # æµå¼æ‰§è¡Œ
    async for event in app.astream(initial_state, config=config):
        # event æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯èŠ‚ç‚¹åï¼Œå€¼æ˜¯è¯¥èŠ‚ç‚¹çš„è¾“å‡º
        for node_name, node_output in event.items():
            if node_name != "__end__":
                yield {
                    "event": "node_output",
                    "node": node_name,
                    "data": node_output,
                    "timestamp": datetime.now().isoformat()
                }
    
    # æµå¼ç»“æŸ
    yield {
        "event": "completed",
        "thread_id": thread_id,
        "timestamp": datetime.now().isoformat()
    }

