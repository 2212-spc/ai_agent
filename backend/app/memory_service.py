"""
é•¿æœŸè®°å¿†ç³»ç»Ÿ - æ™ºèƒ½è®°å¿†æå–ã€å­˜å‚¨å’Œæ£€ç´¢
æ”¯æŒè®°å¿†å»é‡å’Œåˆå¹¶
"""
from __future__ import annotations

import json
import logging
import re
from difflib import SequenceMatcher
from typing import List, Optional, Dict, Any, Tuple

import httpx
from sqlalchemy.orm import Session

from .config import Settings
from .database import (
    ConversationHistory,
    LongTermMemory,
    get_conversation_history,
    save_conversation_message,
    save_long_term_memory,
    search_long_term_memory,
    get_recent_memories,
    update_memory_access,
    get_similar_memories,
    update_memory_content,
    merge_memories,
)
from .rag_service import retrieve_context, get_embeddings
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma

logger = logging.getLogger(__name__)

# è®°å¿†å‘é‡æ•°æ®åº“ç¼“å­˜ï¼ˆç‹¬ç«‹çš„ collectionï¼‰
_MEMORY_VECTORSTORE_CACHE: Dict[str, Chroma] = {}


# ==================== è®°å¿†ç›¸ä¼¼åº¦æ£€æµ‹ ====================

def calculate_text_similarity(text1: str, text2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰
    ä½¿ç”¨ SequenceMatcher è®¡ç®—åºåˆ—ç›¸ä¼¼åº¦
    """
    if not text1 or not text2:
        return 0.0
    
    # è½¬æ¢ä¸ºå°å†™å¹¶å»é™¤å¤šä½™ç©ºæ ¼
    text1 = re.sub(r'\s+', ' ', text1.lower().strip())
    text2 = re.sub(r'\s+', ' ', text2.lower().strip())
    
    # å¦‚æœå®Œå…¨ç›¸åŒï¼Œè¿”å›1.0
    if text1 == text2:
        return 1.0
    
    # ä½¿ç”¨ SequenceMatcher è®¡ç®—ç›¸ä¼¼åº¦
    similarity = SequenceMatcher(None, text1, text2).ratio()
    return similarity


def calculate_jaccard_similarity(text1: str, text2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ Jaccard ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰
    åŸºäºè¯æ±‡é›†åˆçš„äº¤é›†å’Œå¹¶é›†
    """
    if not text1 or not text2:
        return 0.0
    
    # åˆ†è¯ï¼ˆç®€å•åˆ†è¯ï¼ŒæŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
    words1 = set(re.findall(r'\w+', text1.lower()))
    words2 = set(re.findall(r'\w+', text2.lower()))
    
    if not words1 or not words2:
        return 0.0
    
    # è®¡ç®—äº¤é›†å’Œå¹¶é›†
    intersection = len(words1 & words2)
    union = len(words1 | words2)
    
    if union == 0:
        return 0.0
    
    return intersection / union


def calculate_semantic_similarity(
    text1: str,
    text2: str,
    embeddings_model=None,
) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰
    ä½¿ç”¨ embedding å‘é‡è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    
    æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•å¯èƒ½è¾ƒæ…¢ï¼Œç”¨äºé«˜ç²¾åº¦ç›¸ä¼¼åº¦æ£€æµ‹
    """
    try:
        if embeddings_model is None:
            embeddings_model = get_embeddings()
        
        # ç”Ÿæˆ embedding
        emb1 = embeddings_model.embed_query(text1)
        emb2 = embeddings_model.embed_query(text2)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        import numpy as np
        
        dot_product = np.dot(emb1, emb2)
        norm1 = np.linalg.norm(emb1)
        norm2 = np.linalg.norm(emb2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        similarity = dot_product / (norm1 * norm2)
        # å½’ä¸€åŒ–åˆ° 0-1ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦èŒƒå›´æ˜¯ -1 åˆ° 1ï¼‰
        return (similarity + 1) / 2
        
    except Exception as e:
        logger.warning(f"è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}ï¼Œä½¿ç”¨æ–‡æœ¬ç›¸ä¼¼åº¦")
        return calculate_text_similarity(text1, text2)


def find_similar_memory(
    session: Session,
    new_content: str,
    memory_type: str,
    user_id: Optional[str] = None,
    similarity_threshold: float = 0.75,
    use_semantic: bool = False,
) -> Optional[Tuple[LongTermMemory, float]]:
    """
    æŸ¥æ‰¾ä¸æ–°è®°å¿†ç›¸ä¼¼çš„å·²æœ‰è®°å¿†
    
    Args:
        session: æ•°æ®åº“ä¼šè¯
        new_content: æ–°è®°å¿†çš„å†…å®¹
        memory_type: è®°å¿†ç±»å‹
        user_id: ç”¨æˆ·ID
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰ï¼Œè¶…è¿‡æ­¤å€¼è®¤ä¸ºç›¸ä¼¼
        use_semantic: æ˜¯å¦ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆæ›´å‡†ç¡®ä½†è¾ƒæ…¢ï¼‰
    
    Returns:
        (ç›¸ä¼¼è®°å¿†, ç›¸ä¼¼åº¦) æˆ– None
    """
    # è·å–åŒç±»å‹ã€åŒç”¨æˆ·çš„è®°å¿†
    similar_memories = get_similar_memories(
        session=session,
        memory_type=memory_type,
        content=new_content,
        user_id=user_id,
        limit=20,  # æ£€æŸ¥æœ€å¤š20æ¡è®°å¿†
    )
    
    if not similar_memories:
        return None
    
    best_match = None
    best_similarity = 0.0
    embeddings_model = None
    
    if use_semantic:
        embeddings_model = get_embeddings()
    
    # è®¡ç®—ä¸æ¯æ¡è®°å¿†çš„ç›¸ä¼¼åº¦
    for memory in similar_memories:
        # ä¼˜å…ˆä½¿ç”¨æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆå¿«é€Ÿï¼‰
        text_sim = calculate_text_similarity(new_content, memory.content)
        jaccard_sim = calculate_jaccard_similarity(new_content, memory.content)
        
        # ç»¼åˆç›¸ä¼¼åº¦ï¼ˆæ–‡æœ¬ç›¸ä¼¼åº¦æƒé‡0.6ï¼ŒJaccardç›¸ä¼¼åº¦æƒé‡0.4ï¼‰
        combined_sim = text_sim * 0.6 + jaccard_sim * 0.4
        
        # å¦‚æœéœ€è¦é«˜ç²¾åº¦ï¼Œä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦
        if use_semantic and combined_sim > 0.5:
            semantic_sim = calculate_semantic_similarity(
                new_content,
                memory.content,
                embeddings_model,
            )
            # è¯­ä¹‰ç›¸ä¼¼åº¦æƒé‡æ›´é«˜
            combined_sim = combined_sim * 0.4 + semantic_sim * 0.6
        
        if combined_sim > best_similarity:
            best_similarity = combined_sim
            best_match = memory
    
    # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œè¿”å›æœ€ä½³åŒ¹é…
    if best_match and best_similarity >= similarity_threshold:
        return (best_match, best_similarity)
    
    return None


def merge_memory_with_existing(
    session: Session,
    existing_memory: LongTermMemory,
    new_content: str,
    new_importance: int,
    new_source_id: Optional[str] = None,
    new_metadata: Optional[Dict[str, Any]] = None,
    settings: Optional[Settings] = None,
) -> LongTermMemory:
    """
    å°†æ–°è®°å¿†åˆå¹¶åˆ°å·²æœ‰è®°å¿†ä¸­
    
    ç­–ç•¥ï¼š
    1. å¦‚æœæ–°å†…å®¹æ›´å®Œæ•´æˆ–æ›´è¯¦ç»†ï¼Œæ›´æ–°å†…å®¹
    2. å–æ›´é«˜çš„é‡è¦æ€§è¯„åˆ†
    3. åˆå¹¶å…ƒæ•°æ®
    4. æ›´æ–°è®¿é—®ç»Ÿè®¡
    """
    # åˆ¤æ–­å“ªä¸ªå†…å®¹æ›´å¥½ï¼ˆæ›´é•¿æˆ–åŒ…å«æ›´å¤šä¿¡æ¯ï¼‰
    existing_content = existing_memory.content
    should_update_content = False
    
    # å¦‚æœæ–°å†…å®¹æ›´é•¿æˆ–åŒ…å«æ›´å¤šå…³é”®è¯ï¼Œè®¤ä¸ºæ˜¯æ›´å¥½çš„ç‰ˆæœ¬
    if len(new_content) > len(existing_content) * 1.2:
        should_update_content = True
    elif len(new_content) > len(existing_content):
        # æ–°å†…å®¹ç¨é•¿ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«æ›´å¤šä¿¡æ¯
        new_words = set(re.findall(r'\w+', new_content.lower()))
        existing_words = set(re.findall(r'\w+', existing_content.lower()))
        if len(new_words - existing_words) > len(existing_words - new_words):
            should_update_content = True
    
    # æ›´æ–°å†…å®¹ï¼ˆå¦‚æœéœ€è¦ï¼‰
    final_content = new_content if should_update_content else existing_content
    
    # å–æ›´é«˜çš„é‡è¦æ€§è¯„åˆ†
    final_importance = max(existing_memory.importance_score, new_importance)
    
    # å‡†å¤‡å…ƒæ•°æ®
    merged_metadata = new_metadata or {}
    merged_metadata["merged_count"] = merged_metadata.get("merged_count", 0) + 1
    if new_source_id:
        merged_metadata["latest_source_id"] = new_source_id
    
    # æ›´æ–°è®°å¿†
    updated_memory = update_memory_content(
        session=session,
        memory_id=existing_memory.id,
        new_content=final_content,
        new_importance_score=final_importance,
        new_metadata=merged_metadata,
    )
    
    logger.info(
        f"åˆå¹¶è®°å¿†: {existing_memory.id} <- æ–°è®°å¿† "
        f"(ç›¸ä¼¼åº¦: é«˜, å†…å®¹{'å·²æ›´æ–°' if should_update_content else 'ä¿ç•™åŸç‰ˆ'}, "
        f"é‡è¦æ€§: {final_importance})"
    )
    
    return updated_memory


def save_memory_with_dedup(
    session: Session,
    memory_type: str,
    content: str,
    importance_score: int = 50,
    user_id: Optional[str] = None,
    source_conversation_id: Optional[str] = None,
    metadata: Optional[dict] = None,
    similarity_threshold: float = 0.75,
    use_semantic_similarity: bool = False,
    settings: Optional[Settings] = None,
) -> LongTermMemory:
    """
    ä¿å­˜è®°å¿†ï¼Œå¹¶åœ¨ä¿å­˜å‰æ£€æŸ¥é‡å¤å¹¶åˆå¹¶
    
    Args:
        session: æ•°æ®åº“ä¼šè¯
        memory_type: è®°å¿†ç±»å‹
        content: è®°å¿†å†…å®¹
        importance_score: é‡è¦æ€§è¯„åˆ†
        user_id: ç”¨æˆ·ID
        source_conversation_id: æ¥æºå¯¹è¯ID
        metadata: å…ƒæ•°æ®
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
        use_semantic_similarity: æ˜¯å¦ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æµ‹
        settings: é…ç½®å¯¹è±¡ï¼ˆç”¨äºå‘é‡åŒ–ï¼‰
    
    Returns:
        ä¿å­˜æˆ–åˆå¹¶åçš„è®°å¿†å¯¹è±¡
    """
    if settings is None:
        from .config import get_settings
        settings = get_settings()
    
    # æŸ¥æ‰¾ç›¸ä¼¼è®°å¿†
    similar_result = find_similar_memory(
        session=session,
        new_content=content,
        memory_type=memory_type,
        user_id=user_id,
        similarity_threshold=similarity_threshold,
        use_semantic=use_semantic_similarity,
    )
    
    if similar_result:
        existing_memory, similarity = similar_result
        # åˆå¹¶åˆ°å·²æœ‰è®°å¿†
        merged_metadata = metadata or {}
        merged_metadata["similarity_score"] = similarity
        
        merged_memory = merge_memory_with_existing(
            session=session,
            existing_memory=existing_memory,
            new_content=content,
            new_importance=importance_score,
            new_source_id=source_conversation_id,
            new_metadata=merged_metadata,
            settings=settings,
        )
        
        # å¦‚æœå†…å®¹å·²æ›´æ–°ï¼ŒåŒæ­¥æ›´æ–°å‘é‡æ•°æ®åº“
        if merged_memory.content != existing_memory.content:
            update_memory_in_vectorstore(
                memory_id=merged_memory.id,
                content=merged_memory.content,
                memory_type=merged_memory.memory_type,
                user_id=merged_memory.user_id,
                settings=settings,
            )
        
        return merged_memory
    else:
        # æ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼è®°å¿†ï¼Œä¿å­˜æ–°è®°å¿†
        new_memory = save_long_term_memory(
            session=session,
            memory_type=memory_type,
            content=content,
            importance_score=importance_score,
            user_id=user_id,
            source_conversation_id=source_conversation_id,
            metadata=metadata,
        )
        
        # å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        add_memory_to_vectorstore(
            memory_id=new_memory.id,
            content=new_memory.content,
            memory_type=new_memory.memory_type,
            user_id=new_memory.user_id,
            settings=settings,
        )
        
        return new_memory


async def extract_memories_from_conversation(
    conversation_text: str,
    settings: Settings,
    session_id: str,
    user_id: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨ LLM ä»å¯¹è¯ä¸­æå–é‡è¦ä¿¡æ¯ä½œä¸ºé•¿æœŸè®°å¿†
    
    Returns:
        æå–çš„è®°å¿†åˆ—è¡¨ï¼Œæ¯ä¸ªè®°å¿†åŒ…å« type, content, importance_score
    """
    try:
        extraction_prompt = f"""è¯·åˆ†æä»¥ä¸‹å¯¹è¯ï¼Œæå–å‡ºåº”è¯¥è¢«é•¿æœŸè®°ä½çš„é‡è¦ä¿¡æ¯ã€‚

å¯¹è¯å†…å®¹ï¼š
{conversation_text}

è¯·æå–ä»¥ä¸‹ç±»å‹çš„ä¿¡æ¯ï¼š
1. **fact** - æ˜ç¡®çš„äº‹å®ä¿¡æ¯ï¼ˆå¦‚ï¼šç”¨æˆ·çš„åå­—ã€èŒä¸šã€å·¥ä½œåœ°ç‚¹ã€å±…ä½åœ°ç­‰ï¼‰
   - **ç‰¹åˆ«æ³¨æ„**ï¼šå¦‚æœå¯¹è¯ä¸­æåˆ°ç”¨æˆ·çš„åå­—ï¼Œå¿…é¡»æå–ä¸º fact ç±»å‹ï¼Œé‡è¦æ€§è®¾ä¸º 90-100
   - ä¾‹å¦‚ï¼š"æˆ‘å«å¼ ä¸‰" â†’ {{"type": "fact", "content": "ç”¨æˆ·çš„åå­—æ˜¯å¼ ä¸‰", "importance": 95}}
2. **preference** - ç”¨æˆ·çš„åå¥½å’Œä¹ æƒ¯ï¼ˆå¦‚ï¼šå–œæ¬¢çš„é£Ÿç‰©ã€ç¼–ç¨‹è¯­è¨€ã€å·¥ä½œä¹ æƒ¯ç­‰ï¼‰
3. **event** - é‡è¦çš„äº‹ä»¶æˆ–ç»å†ï¼ˆå¦‚ï¼šç”Ÿæ—¥ã€æ—…è¡Œè®¡åˆ’ã€ä¼šè®®å®‰æ’ç­‰ï¼‰
4. **relationship** - äººç‰©å…³ç³»æˆ–ç¤¾äº¤ä¿¡æ¯

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºæå–çš„è®°å¿†ï¼š
{{
  "memories": [
    {{
      "type": "fact|preference|event|relationship",
      "content": "è®°å¿†å†…å®¹çš„ç®€æ´æè¿°",
      "importance": 50-100  // é‡è¦æ€§è¯„åˆ†ï¼Œ50ä¸ºä¸€èˆ¬é‡è¦ï¼Œ100ä¸ºéå¸¸é‡è¦ã€‚å§“åç­‰é‡è¦ä¿¡æ¯åº”è®¾ä¸º90-100
    }}
  ]
}}

è¦æ±‚ï¼š
1. **å¿…é¡»æå–ç”¨æˆ·å§“å**ï¼šå¦‚æœå¯¹è¯ä¸­æåˆ°ç”¨æˆ·çš„åå­—ï¼ˆå¦‚"æˆ‘å«XXX"ã€"æˆ‘æ˜¯XXX"ï¼‰ï¼Œå¿…é¡»æå–ä¸º fact ç±»å‹ï¼Œimportance è®¾ä¸º 90-100
2. åªæå–çœŸæ­£é‡è¦ã€å€¼å¾—é•¿æœŸè®°ä½çš„ä¿¡æ¯
3. å†…å®¹è¦ç®€æ´ã€æ˜ç¡®ï¼Œä½¿ç”¨ç¬¬ä¸‰äººç§°æè¿°ï¼ˆå¦‚"ç”¨æˆ·çš„åå­—æ˜¯XXX"è€Œä¸æ˜¯"æˆ‘çš„åå­—æ˜¯XXX"ï¼‰
4. importance è¯„åˆ†è¦åˆç†ï¼š
   - å§“åã€èŒä¸šç­‰å…³é”®ä¿¡æ¯ï¼š90-100
   - é‡è¦åå¥½ã€äº‹ä»¶ï¼š70-89
   - ä¸€èˆ¬ä¿¡æ¯ï¼š50-69
5. å¦‚æœå¯¹è¯ä¸­æ²¡æœ‰é‡è¦ä¿¡æ¯ï¼Œè¿”å›ç©ºçš„ memories æ•°ç»„
6. åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–è§£é‡Š
"""

        headers = {
            "Authorization": f"Bearer {settings.deepseek_api_key}",
            "Content-Type": "application/json",
        }
        endpoint = f"{settings.deepseek_base_url.rstrip('/')}/chat/completions"

        payload = {
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": extraction_prompt}],
            "temperature": 0.3,  # ä½æ¸©åº¦ä¿è¯æå–ç¨³å®š
            "max_tokens": 1000,
            "stream": False,
        }

        async with httpx.AsyncClient(timeout=httpx.Timeout(60.0)) as client:
            response = await client.post(endpoint, json=payload, headers=headers)

        if response.status_code != 200:
            logger.error(f"è®°å¿†æå– API é”™è¯¯ {response.status_code}: {response.text}")
            return []

        data = response.json()
        reply_text = data["choices"][0]["message"]["content"]

        # è§£æ JSON å“åº”
        memories = _parse_memory_extraction(reply_text)

        logger.info(f"ä»å¯¹è¯ä¸­æå–äº† {len(memories)} æ¡è®°å¿†")
        return memories

    except Exception as e:
        logger.error(f"è®°å¿†æå–å¤±è´¥: {e}", exc_info=True)
        return []


def _parse_memory_extraction(text: str) -> List[Dict[str, Any]]:
    """è§£æ LLM è¿”å›çš„è®°å¿†æå–ç»“æœ"""
    try:
        # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
        cleaned = text.strip()
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        elif cleaned.startswith("```"):
            cleaned = cleaned[3:]
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
        cleaned = cleaned.strip()

        data = json.loads(cleaned)
        memories = data.get("memories", [])

        # éªŒè¯å’Œæ¸…ç†è®°å¿†æ•°æ®
        validated = []
        for mem in memories:
            mem_type = mem.get("type", "").lower()
            if mem_type not in ["fact", "preference", "event", "relationship"]:
                continue
            
            content = mem.get("content", "").strip()
            if not content:
                continue
            
            importance = int(mem.get("importance", 50))
            importance = max(50, min(100, importance))  # é™åˆ¶åœ¨ 50-100

            validated.append({
                "type": mem_type,
                "content": content,
                "importance": importance,
            })

        return validated

    except json.JSONDecodeError as e:
        logger.warning(f"JSON è§£æå¤±è´¥: {e}, åŸå§‹æ–‡æœ¬: {text[:200]}")
        return []
    except Exception as e:
        logger.error(f"è®°å¿†è§£æå¤±è´¥: {e}", exc_info=True)
        return []


async def save_conversation_and_extract_memories(
    session: Session,
    session_id: str,
    user_query: str,
    assistant_reply: str,
    settings: Settings,
    user_id: Optional[str] = None,
    metadata: Optional[Dict[str, Any]] = None,
) -> List[LongTermMemory]:
    """
    ä¿å­˜å¯¹è¯å¹¶è‡ªåŠ¨æå–è®°å¿†
    è¿”å›æ–°ä¿å­˜çš„è®°å¿†åˆ—è¡¨
    """
    # ğŸ” ä¸¥æ ¼æ£€æŸ¥ï¼šæ²¡æœ‰ user_id ä¸ä¿å­˜è®°å¿†ï¼ˆé˜²æ­¢è·¨è´¦å·æ··æ·†ï¼‰
    if not user_id:
        logger.warning("âš ï¸ ç¼ºå°‘ user_idï¼Œè·³è¿‡è®°å¿†ä¿å­˜ï¼ˆå®‰å…¨è€ƒè™‘ï¼‰")
        # ä»ç„¶ä¿å­˜å¯¹è¯è®°å½•ï¼ˆä¸éœ€è¦ user_idï¼‰
        save_conversation_message(
            session=session,
            session_id=session_id,
            role="user",
            content=user_query,
            user_id=None,
            metadata=metadata,
        )
        save_conversation_message(
            session=session,
            session_id=session_id,
            role="assistant",
            content=assistant_reply,
            user_id=None,
            metadata=metadata,
        )
        return []
    
    # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
    save_conversation_message(
        session=session,
        session_id=session_id,
        role="user",
        content=user_query,
        user_id=user_id,
        metadata=metadata,
    )

    # ä¿å­˜åŠ©æ‰‹å›å¤
    save_conversation_message(
        session=session,
        session_id=session_id,
        role="assistant",
        content=assistant_reply,
        user_id=user_id,
        metadata=metadata,
    )

    # æ„å»ºå¯¹è¯æ–‡æœ¬ç”¨äºæå–è®°å¿†
    conversation_text = f"ç”¨æˆ·: {user_query}\nåŠ©æ‰‹: {assistant_reply}"

    # æå–è®°å¿†
    extracted_memories = await extract_memories_from_conversation(
        conversation_text=conversation_text,
        settings=settings,
        session_id=session_id,
        user_id=user_id,
    )

    # ä¿å­˜æå–çš„è®°å¿†ï¼ˆä½¿ç”¨å»é‡å’Œåˆå¹¶é€»è¾‘ï¼Œå¹¶è‡ªåŠ¨å‘é‡åŒ–ï¼‰
    saved_memories = []
    for mem in extracted_memories:
        try:
            # ä½¿ç”¨å¸¦å»é‡çš„ä¿å­˜å‡½æ•°ï¼ˆä¼šè‡ªåŠ¨å‘é‡åŒ–ï¼‰
            memory_record = save_memory_with_dedup(
                session=session,
                memory_type=mem["type"],
                content=mem["content"],
                importance_score=mem["importance"],
                user_id=user_id,
                source_conversation_id=session_id,
                metadata={"extracted_at": "auto"},
                similarity_threshold=0.75,  # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œå¯é…ç½®
                use_semantic_similarity=False,  # é»˜è®¤ä½¿ç”¨å¿«é€Ÿæ–‡æœ¬ç›¸ä¼¼åº¦ï¼Œéœ€è¦é«˜ç²¾åº¦æ—¶å¯è®¾ä¸ºTrue
                settings=settings,  # ä¼ å…¥ settings ç”¨äºå‘é‡åŒ–
            )
            saved_memories.append(memory_record)
        except Exception as e:
            logger.error(f"ä¿å­˜è®°å¿†å¤±è´¥: {e}", exc_info=True)

    return saved_memories


async def retrieve_relevant_memories(
    session: Session,
    query: str,
    settings: Settings,
    user_id: Optional[str] = None,
    max_memories: int = 5,
    session_id: Optional[str] = None,
    share_memory: Optional[bool] = None,
) -> List[LongTermMemory]:
    """
    æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„é•¿æœŸè®°å¿†
    
    ä½¿ç”¨å‘é‡æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢ç»“åˆçš„æ–¹å¼
    åŒæ—¶ç¡®ä¿è¿”å›é‡è¦çš„ç”¨æˆ·ä¿¡æ¯ï¼ˆå¦‚å§“åç­‰ï¼‰
    
    Args:
        session: æ•°æ®åº“ä¼šè¯
        query: æŸ¥è¯¢æ–‡æœ¬
        settings: é…ç½®å¯¹è±¡
        user_id: ç”¨æˆ·IDï¼ˆå¿…éœ€ï¼Œç”¨äºéš”ç¦»ä¸åŒè´¦å·çš„è®°å¿†ï¼‰
        max_memories: æœ€å¤§è¿”å›è®°å¿†æ•°
        session_id: å½“å‰ä¼šè¯IDï¼ˆç”¨äºè®°å¿†éš”ç¦»ï¼‰
        share_memory: æ˜¯å¦å…±äº«è®°å¿†ï¼ˆNone è¡¨ç¤ºä½¿ç”¨ä¼šè¯é…ç½®ï¼‰
    
    Returns:
        ç›¸å…³è®°å¿†åˆ—è¡¨
    """
    # ğŸ” ä¸¥æ ¼æ£€æŸ¥ï¼šå¿…é¡»æœ‰ user_idï¼Œå¦åˆ™è¿”å›ç©ºåˆ—è¡¨ï¼ˆé˜²æ­¢è·¨è´¦å·è®°å¿†æ³„éœ²ï¼‰
    if not user_id:
        logger.warning("âš ï¸ è®°å¿†æ£€ç´¢ç¼ºå°‘ user_idï¼Œä¸ºå®‰å…¨èµ·è§è¿”å›ç©ºåˆ—è¡¨")
        return []
    
    # æ£€æŸ¥ä¼šè¯é…ç½®ï¼Œå†³å®šæ˜¯å¦å…±äº«è®°å¿†
    # ğŸ”’ é»˜è®¤å€¼æ”¹ä¸º Falseï¼ˆç‹¬ç«‹è®°å¿†ï¼‰ï¼Œåªæœ‰æ˜¾å¼å¼€å¯å…¨å±€è®°å¿†æ—¶æ‰å…±äº«
    should_share = False  # é»˜è®¤ï¼šç‹¬ç«‹è®°å¿†ï¼ˆä¼šè¯éš”ç¦»ï¼‰
    
    # å¦‚æœæ˜ç¡®ä¼ é€’äº† share_memory å‚æ•°ï¼Œä¼˜å…ˆä½¿ç”¨
    if share_memory is not None:
        should_share = share_memory
    # å¦åˆ™ï¼Œæ£€æŸ¥ä¼šè¯é…ç½®
    elif session_id:
        from .database import get_session_config
        config = get_session_config(session, session_id)
        if config:
            should_share = config.share_memory_across_sessions
    
    # å¦‚æœä¸åº”å…±äº«è®°å¿†ï¼Œåªæ£€ç´¢å½“å‰ä¼šè¯çš„è®°å¿†
    if not should_share and session_id:
        # åªæ£€ç´¢æ¥è‡ªå½“å‰ä¼šè¯çš„è®°å¿†
        keyword_memories = search_long_term_memory(
            session=session,
            query=query,
            user_id=user_id,
            limit=max_memories * 2,
            min_importance=40,
        )
        
        # è¿‡æ»¤å‡ºå½“å‰ä¼šè¯çš„è®°å¿†
        session_memories = [
            m for m in keyword_memories
            if m.source_conversation_id == session_id
        ]
        
        # æ›´æ–°è®¿é—®ä¿¡æ¯
        for mem in session_memories:
            update_memory_access(session, mem.id)
        
        # æŒ‰é‡è¦æ€§æ’åº
        sorted_memories = sorted(
            session_memories,
            key=lambda m: (
                m.memory_type == "fact",  # fact ç±»å‹ä¼˜å…ˆ
                m.importance_score,
                m.access_count
            ),
            reverse=True,
        )[:max_memories]
        
        logger.debug(f"ä¼šè¯éš”ç¦»æ¨¡å¼ï¼šåªæ£€ç´¢å½“å‰ä¼šè¯ {session_id} çš„è®°å¿†ï¼Œæ‰¾åˆ° {len(sorted_memories)} æ¡")
        return sorted_memories
    
    # åŸæœ‰é€»è¾‘ï¼ˆå…±äº«è®°å¿†æ¨¡å¼ï¼‰
    all_memories = {}
    
    # 1. å…ˆå°è¯•å‘é‡æ£€ç´¢ï¼ˆå¦‚æœå‘é‡æ•°æ®åº“ä¸­æœ‰è®°å¿†ï¼‰
    vector_memories = _retrieve_memories_by_embedding(
        query=query,
        session=session,
        settings=settings,
        user_id=user_id,
        limit=max_memories,
    )
    for mem in vector_memories:
        all_memories[mem.id] = mem

    # 2. å…³é”®è¯æ£€ç´¢ä½œä¸ºè¡¥å……
    keyword_memories = search_long_term_memory(
        session=session,
        query=query,
        user_id=user_id,
        limit=max_memories,
        min_importance=50,  # åªæ£€ç´¢é‡è¦è®°å¿†
    )
    for mem in keyword_memories:
        all_memories[mem.id] = mem
    
    # 3. å¦‚æœæŸ¥è¯¢åŒ¹é…çš„è®°å¿†è¾ƒå°‘ï¼Œè¡¥å……ä¸€äº›é‡è¦çš„ç”¨æˆ·è®°å¿†ï¼ˆç‰¹åˆ«æ˜¯ fact ç±»å‹ï¼Œå¦‚å§“åï¼‰
    # è¿™æ ·å¯ä»¥ç¡®ä¿ç”¨æˆ·ä¿¡æ¯ï¼ˆå¦‚å§“åï¼‰æ€»æ˜¯å¯ç”¨çš„
    if len(all_memories) < max_memories:
        try:
            # è·å–æœ€è¿‘çš„é‡è¦è®°å¿†ï¼Œç‰¹åˆ«æ˜¯ fact ç±»å‹ï¼ˆåŒ…å«å§“åç­‰ä¿¡æ¯ï¼‰
            recent_facts = get_recent_memories(
                session=session,
                user_id=user_id,
                limit=max_memories * 2,  # è·å–æ›´å¤šå€™é€‰
            )
            
            # ä¼˜å…ˆé€‰æ‹© fact ç±»å‹çš„é«˜é‡è¦æ€§è®°å¿†
            fact_memories = [m for m in recent_facts if m.memory_type == "fact" and m.importance_score >= 60]
            preference_memories = [m for m in recent_facts if m.memory_type == "preference" and m.importance_score >= 60]
            
            # è¡¥å…… fact ç±»å‹è®°å¿†ï¼ˆé€šå¸¸æ˜¯å§“åç­‰å…³é”®ä¿¡æ¯ï¼‰
            for mem in fact_memories:
                if mem.id not in all_memories and len(all_memories) < max_memories:
                    all_memories[mem.id] = mem
            
            # å¦‚æœè¿˜æœ‰ç©ºé—´ï¼Œè¡¥å…… preference ç±»å‹è®°å¿†
            if len(all_memories) < max_memories:
                for mem in preference_memories:
                    if mem.id not in all_memories and len(all_memories) < max_memories:
                        all_memories[mem.id] = mem
            
            # æœ€åè¡¥å……å…¶ä»–é‡è¦è®°å¿†
            if len(all_memories) < max_memories:
                for mem in recent_facts:
                    if mem.id not in all_memories and mem.importance_score >= 60:
                        if len(all_memories) < max_memories:
                            all_memories[mem.id] = mem
                        else:
                            break
        except Exception as e:
            logger.warning(f"è·å–è¡¥å……è®°å¿†å¤±è´¥: {e}")

    # æ›´æ–°è®¿é—®ä¿¡æ¯
    for mem_id in all_memories:
        update_memory_access(session, mem_id)

    # æŒ‰é‡è¦æ€§æ’åº
    sorted_memories = sorted(
        all_memories.values(),
        key=lambda m: (
            m.memory_type == "fact",  # fact ç±»å‹ä¼˜å…ˆ
            m.importance_score,
            m.access_count
        ),
        reverse=True,
    )[:max_memories]

    return sorted_memories


def get_memory_vectorstore(settings: Settings) -> Chroma:
    """
    è·å–è®°å¿†å‘é‡æ•°æ®åº“ï¼ˆç‹¬ç«‹çš„ collectionï¼‰
    ä½¿ç”¨ç‹¬ç«‹çš„ collection å­˜å‚¨è®°å¿†ï¼Œä¸æ–‡æ¡£åˆ†å¼€
    """
    key = str(settings.chroma_dir)
    store = _MEMORY_VECTORSTORE_CACHE.get(key)
    if store is None:
        settings.chroma_dir.mkdir(parents=True, exist_ok=True)
        store = Chroma(
            collection_name="memories",  # ç‹¬ç«‹çš„ collection åç§°
            embedding_function=get_embeddings(),
            persist_directory=str(settings.chroma_dir),
        )
        _MEMORY_VECTORSTORE_CACHE[key] = store
        logger.info("è®°å¿†å‘é‡æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    return store


def add_memory_to_vectorstore(
    memory_id: str,
    content: str,
    memory_type: str,
    user_id: Optional[str] = None,
    settings: Settings = None,
) -> None:
    """
    å°†è®°å¿†æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
    
    Args:
        memory_id: è®°å¿†ID
        content: è®°å¿†å†…å®¹
        memory_type: è®°å¿†ç±»å‹
        user_id: ç”¨æˆ·ID
        settings: é…ç½®å¯¹è±¡
    """
    try:
        if settings is None:
            from .config import get_settings
            settings = get_settings()
        
        vectorstore = get_memory_vectorstore(settings)
        
        # åˆ›å»º Document å¯¹è±¡ï¼Œmetadata åŒ…å«è®°å¿†çš„æ‰€æœ‰ä¿¡æ¯
        metadata = {
            "memory_id": memory_id,
            "memory_type": memory_type,
        }
        if user_id:
            metadata["user_id"] = user_id
        
        doc = Document(page_content=content, metadata=metadata)
        
        # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“ï¼Œä½¿ç”¨ memory_id ä½œä¸ºå”¯ä¸€ID
        vectorstore.add_documents([doc], ids=[memory_id])
        
        logger.debug(f"è®°å¿†å·²å‘é‡åŒ–å¹¶å­˜å‚¨: {memory_id}")
        
    except Exception as e:
        logger.error(f"å‘é‡åŒ–è®°å¿†å¤±è´¥ {memory_id}: {e}", exc_info=True)
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸è®°å¿†ä¿å­˜ç»§ç»­ï¼Œåªæ˜¯æ²¡æœ‰å‘é‡åŒ–


def update_memory_in_vectorstore(
    memory_id: str,
    content: str,
    memory_type: str,
    user_id: Optional[str] = None,
    settings: Settings = None,
) -> None:
    """
    æ›´æ–°å‘é‡æ•°æ®åº“ä¸­çš„è®°å¿†
    
    å…ˆåˆ é™¤æ—§å‘é‡ï¼Œå†æ·»åŠ æ–°å‘é‡
    """
    try:
        if settings is None:
            from .config import get_settings
            settings = get_settings()
        
        vectorstore = get_memory_vectorstore(settings)
        
        # åˆ é™¤æ—§å‘é‡
        try:
            vectorstore.delete(ids=[memory_id])
        except Exception as e:
            logger.warning(f"åˆ é™¤æ—§å‘é‡å¤±è´¥ï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰: {e}")
        
        # æ·»åŠ æ–°å‘é‡
        add_memory_to_vectorstore(
            memory_id=memory_id,
            content=content,
            memory_type=memory_type,
            user_id=user_id,
            settings=settings,
        )
        
        logger.debug(f"è®°å¿†å‘é‡å·²æ›´æ–°: {memory_id}")
        
    except Exception as e:
        logger.error(f"æ›´æ–°è®°å¿†å‘é‡å¤±è´¥ {memory_id}: {e}", exc_info=True)


def delete_memory_from_vectorstore(
    memory_id: str,
    settings: Settings = None,
) -> None:
    """
    ä»å‘é‡æ•°æ®åº“ä¸­åˆ é™¤è®°å¿†
    """
    try:
        if settings is None:
            from .config import get_settings
            settings = get_settings()
        
        vectorstore = get_memory_vectorstore(settings)
        
        # åˆ é™¤å‘é‡
        vectorstore.delete(ids=[memory_id])
        
        logger.debug(f"è®°å¿†å‘é‡å·²åˆ é™¤: {memory_id}")
        
    except Exception as e:
        logger.warning(f"åˆ é™¤è®°å¿†å‘é‡å¤±è´¥ï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰: {e}")


def vectorize_existing_memories(
    session: Session,
    settings: Settings,
    user_id: Optional[str] = None,
    batch_size: int = 100,
) -> int:
    """
    æ‰¹é‡å‘é‡åŒ–å·²æœ‰è®°å¿†ï¼ˆç”¨äºè¿ç§»æˆ–åˆå§‹åŒ–ï¼‰
    
    Args:
        session: æ•°æ®åº“ä¼šè¯
        settings: é…ç½®å¯¹è±¡
        user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼Œå¦‚æœæŒ‡å®šåˆ™åªå‘é‡åŒ–è¯¥ç”¨æˆ·çš„è®°å¿†ï¼‰
        batch_size: æ‰¹é‡å¤„ç†å¤§å°
    
    Returns:
        æˆåŠŸå‘é‡åŒ–çš„è®°å¿†æ•°é‡
    """
    try:
        vectorstore = get_memory_vectorstore(settings)
        
        # è·å–å·²æœ‰å‘é‡IDåˆ—è¡¨ï¼ˆé¿å…é‡å¤å‘é‡åŒ–ï¼‰
        existing_ids = set()
        try:
            existing_vectors = vectorstore.get()
            if existing_vectors and existing_vectors.get("ids"):
                existing_ids = set(existing_vectors["ids"])
        except Exception as e:
            logger.warning(f"è·å–å·²æœ‰å‘é‡åˆ—è¡¨å¤±è´¥: {e}")
        
        # æŸ¥è¯¢éœ€è¦å‘é‡åŒ–çš„è®°å¿†
        if user_id:
            # æŸ¥è¯¢ç‰¹å®šç”¨æˆ·çš„è®°å¿†
            from .database import LongTermMemory
            from sqlalchemy import select
            statement = select(LongTermMemory).where(LongTermMemory.user_id == user_id)
            memories = list(session.execute(statement).scalars())
        else:
            # æŸ¥è¯¢æ‰€æœ‰è®°å¿†
            from .database import LongTermMemory
            from sqlalchemy import select
            statement = select(LongTermMemory)
            memories = list(session.execute(statement).scalars())
        
        # è¿‡æ»¤æ‰å·²å‘é‡åŒ–çš„è®°å¿†
        memories_to_vectorize = [
            mem for mem in memories
            if mem.id not in existing_ids
        ]
        
        if not memories_to_vectorize:
            logger.info("æ²¡æœ‰éœ€è¦å‘é‡åŒ–çš„è®°å¿†")
            return 0
        
        logger.info(f"å¼€å§‹å‘é‡åŒ– {len(memories_to_vectorize)} æ¡è®°å¿†...")
        
        # æ‰¹é‡å¤„ç†
        success_count = 0
        for i in range(0, len(memories_to_vectorize), batch_size):
            batch = memories_to_vectorize[i:i + batch_size]
            
            documents = []
            ids = []
            for mem in batch:
                metadata = {
                    "memory_id": mem.id,
                    "memory_type": mem.memory_type,
                }
                if mem.user_id:
                    metadata["user_id"] = mem.user_id
                
                doc = Document(page_content=mem.content, metadata=metadata)
                documents.append(doc)
                ids.append(mem.id)
            
            try:
                vectorstore.add_documents(documents, ids=ids)
                success_count += len(batch)
                logger.info(f"å·²å‘é‡åŒ– {success_count}/{len(memories_to_vectorize)} æ¡è®°å¿†")
            except Exception as e:
                logger.error(f"æ‰¹é‡å‘é‡åŒ–å¤±è´¥: {e}", exc_info=True)
        
        logger.info(f"å‘é‡åŒ–å®Œæˆï¼ŒæˆåŠŸ {success_count}/{len(memories_to_vectorize)} æ¡")
        return success_count
        
    except Exception as e:
        logger.error(f"æ‰¹é‡å‘é‡åŒ–è®°å¿†å¤±è´¥: {e}", exc_info=True)
        return 0


def _retrieve_memories_by_embedding(
    query: str,
    session: Session,
    settings: Settings,
    user_id: Optional[str] = None,
    limit: int = 5,
) -> List[LongTermMemory]:
    """
    ä½¿ç”¨å‘é‡æ£€ç´¢è®°å¿†ï¼ˆçœŸæ­£çš„å‘é‡ç›¸ä¼¼åº¦æœç´¢ï¼‰
    
    ä½¿ç”¨ Chroma å‘é‡æ•°æ®åº“è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
    """
    # ğŸ” ä¸¥æ ¼æ£€æŸ¥ï¼šå¿…é¡»æœ‰ user_idï¼Œå¦åˆ™è¿”å›ç©ºåˆ—è¡¨
    if not user_id:
        logger.warning("âš ï¸ å‘é‡æ£€ç´¢ç¼ºå°‘ user_idï¼Œè¿”å›ç©ºåˆ—è¡¨")
        return []
    
    try:
        vectorstore = get_memory_vectorstore(settings)
        
        # æ‰§è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢
        # æœç´¢æ›´å¤šå€™é€‰ï¼ˆlimit * 3ï¼‰ï¼Œç„¶åæ ¹æ®ç”¨æˆ·è¿‡æ»¤
        search_k = min(limit * 5, 100)  # æœç´¢æ›´å¤šå€™é€‰ä»¥åº”å¯¹è¿‡æ»¤åçš„æŸå¤±
        
        try:
            # å°è¯•ä½¿ç”¨ filter å‚æ•°ï¼ˆæ–°ç‰ˆæœ¬ Chroma æ”¯æŒï¼‰
            if user_id:
                results = vectorstore.similarity_search_with_score(
                    query,
                    k=search_k,
                    filter={"user_id": user_id},
                )
            else:
                results = vectorstore.similarity_search_with_score(
                    query,
                    k=search_k,
                )
        except TypeError:
            # å¦‚æœä¸æ”¯æŒ filter å‚æ•°ï¼Œä½¿ç”¨æ—§æ–¹æ³•ï¼šå…ˆæœç´¢å†è¿‡æ»¤
            results = vectorstore.similarity_search_with_score(query, k=search_k)
        
        if not results:
            logger.debug("å‘é‡æœç´¢æœªæ‰¾åˆ°ç›¸å…³è®°å¿†")
            return []
        
        # ä»å‘é‡æ•°æ®åº“ä¸­è·å–çš„ (Document, score) åˆ—è¡¨
        # score æ˜¯è·ç¦»ï¼ˆè¶Šå°è¶Šç›¸ä¼¼ï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸ºç›¸ä¼¼åº¦
        memory_ids = []
        scored_memories = {}
        
        for doc, distance in results:
            memory_id = doc.metadata.get("memory_id")
            if not memory_id:
                continue
            
            # è·³è¿‡ä¸åŒç”¨æˆ·ï¼ˆå¦‚æœæŒ‡å®šäº† user_idï¼‰
            if user_id and doc.metadata.get("user_id") != user_id:
                continue
            
            # å°†è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆChroma ä½¿ç”¨ä½™å¼¦è·ç¦»ï¼ŒèŒƒå›´ 0-2ï¼‰
            # ç›¸ä¼¼åº¦ = 1 - (distance / 2)
            similarity_score = max(0.0, 1.0 - (distance / 2.0))
            
            memory_ids.append(memory_id)
            scored_memories[memory_id] = similarity_score
        
        if not memory_ids:
            return []
        
        # ä»æ•°æ®åº“åŠ è½½è®°å¿†å¯¹è±¡
        memories = []
        for memory_id in memory_ids[:limit * 2]:  # åŠ è½½æ›´å¤šç”¨äºåç»­ç­›é€‰
            try:
                # ä» session ä¸­è·å–è®°å¿†ï¼ˆéœ€è¦ä» LongTermMemory è¡¨ä¸­æŸ¥è¯¢ï¼‰
                memory = session.get(LongTermMemory, memory_id)
                if memory:
                    # å°†ç›¸ä¼¼åº¦åˆ†æ•°å­˜å‚¨åœ¨ä¸´æ—¶å±æ€§ä¸­
                    memory._vector_similarity = scored_memories.get(memory_id, 0.0)
                    memories.append(memory)
            except Exception as e:
                logger.warning(f"åŠ è½½è®°å¿†å¤±è´¥ {memory_id}: {e}")
                continue
        
        # æŒ‰ç›¸ä¼¼åº¦åˆ†æ•°å’Œé‡è¦æ€§ç»¼åˆæ’åº
        memories.sort(
            key=lambda m: (
                getattr(m, '_vector_similarity', 0.0),  # å‘é‡ç›¸ä¼¼åº¦
                m.importance_score / 100.0,  # é‡è¦æ€§è¯„åˆ†å½’ä¸€åŒ–
            ),
            reverse=True,
        )
        
        # åªè¿”å›å‰ limit æ¡
        result = memories[:limit]
        
        logger.debug(f"å‘é‡æ£€ç´¢æ‰¾åˆ° {len(result)} æ¡è®°å¿†ï¼ˆä» {len(memories)} æ¡ä¸­ç­›é€‰ï¼‰")
        return result

    except Exception as e:
        logger.error(f"å‘é‡æ£€ç´¢è®°å¿†å¤±è´¥: {e}", exc_info=True)
        # å¤±è´¥æ—¶å›é€€åˆ°å…³é”®è¯æ£€ç´¢
        logger.info("å‘é‡æ£€ç´¢å¤±è´¥ï¼Œå›é€€åˆ°å…³é”®è¯æ£€ç´¢")
        return search_long_term_memory(
            session=session,
            query=query,
            user_id=user_id,
            limit=limit,
            min_importance=40,
        )


def format_memories_for_context(memories: List[LongTermMemory]) -> str:
    """
    å°†è®°å¿†æ ¼å¼åŒ–ä¸ºä¸Šä¸‹æ–‡æç¤ºè¯ï¼ˆéšå¼æ ¼å¼ï¼Œç”¨äºå†…éƒ¨å¤„ç†ï¼‰
    ä¸æ˜¾ç¤º"è®°å¿†"ã€"è®°å½•"ç­‰æ ‡ç­¾ï¼Œè®©ä¿¡æ¯çœ‹èµ·æ¥åƒæ˜¯å·²çŸ¥çš„èƒŒæ™¯çŸ¥è¯†
    """
    if not memories:
        return ""

    parts = []
    for mem in memories:
        # ç›´æ¥æ˜¾ç¤ºå†…å®¹ï¼Œä¸æ·»åŠ åºå·å’Œæ ‡ç­¾ï¼Œè®©ä¿¡æ¯æ›´è‡ªç„¶
        parts.append(mem.content)

    return "\n".join(parts)


def format_memories_for_prompt(memories: List[LongTermMemory]) -> str:
    """
    å°†è®°å¿†æ ¼å¼åŒ–ä¸ºç”¨äº LLM prompt çš„æ ¼å¼ï¼ˆå®Œå…¨éšå¼ï¼‰
    ä¸æ˜¾ç¤ºä»»ä½•"è®°å¿†"ã€"ä¿¡æ¯"ç­‰æ ‡ç­¾ï¼Œåªæä¾›çº¯å†…å®¹
    """
    if not memories:
        return ""
    
    # åªè¿”å›è®°å¿†å†…å®¹ï¼Œä¸€è¡Œä¸€ä¸ªï¼Œæ²¡æœ‰ä»»ä½•æ ‡ç­¾
    return "\n".join(mem.content for mem in memories)


def get_conversation_context(
    session: Session,
    session_id: str,
    limit: int = 10,
    user_id: Optional[str] = None,
) -> List[Dict[str, str]]:
    """
    è·å–å¯¹è¯å†å²ä½œä¸ºä¸Šä¸‹æ–‡
    
    Returns:
        æ ¼å¼åŒ–çš„å¯¹è¯æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼: [{"role": "user|assistant", "content": "..."}, ...]
    """
    history = get_conversation_history(
        session=session,
        session_id=session_id,
        limit=limit,
        user_id=user_id,
    )

    messages = []
    for record in history:
        messages.append({
            "role": record.role,
            "content": record.content,
        })

    return messages

