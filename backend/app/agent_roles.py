"""
æ™ºèƒ½ä½“è§’è‰²å®šä¹‰
å®šä¹‰å„ä¸ªä¸“å®¶æ™ºèƒ½ä½“çš„èŠ‚ç‚¹å‡½æ•°å’Œèƒ½åŠ›
"""
from __future__ import annotations

import json
import logging
from datetime import datetime
from typing import Any, Dict, List, Optional

from sqlalchemy.orm import Session

from .config import Settings
from .database import ToolRecord, get_active_prompt_for_agent
from .graph_agent import invoke_llm, parse_json_from_llm
from .rag_service import retrieve_context
from .shared_workspace import MultiAgentState, SharedWorkspace
from .tool_service import execute_tool

logger = logging.getLogger(__name__)


# ==================== Promptç®¡ç†è¾…åŠ©å‡½æ•° ====================

def get_agent_prompt(
    agent_id: str,
    session: Session,
    default_prompt: str,
    **kwargs
) -> str:
    """
    è·å–æ™ºèƒ½ä½“çš„æ¿€æ´»promptæ¨¡æ¿
    
    Args:
        agent_id: æ™ºèƒ½ä½“ID
        session: æ•°æ®åº“ä¼šè¯
        default_prompt: é»˜è®¤promptï¼ˆå¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æ¿€æ´»çš„æ¨¡æ¿ï¼Œä½¿ç”¨è¿™ä¸ªï¼‰
        **kwargs: ç”¨äºæ›¿æ¢promptä¸­çš„å ä½ç¬¦ï¼Œå¦‚ user_query, task_description ç­‰
    
    Returns:
        å¤„ç†åçš„promptå­—ç¬¦ä¸²
    """
    try:
        # å°è¯•ä»æ•°æ®åº“è·å–æ¿€æ´»çš„prompt
        template = get_active_prompt_for_agent(session, agent_id)
        
        if template and template.is_active:
            prompt = template.content
            logger.info(f"âœ… [Promptç®¡ç†] ä½¿ç”¨æ•°æ®åº“ä¸­çš„æ¿€æ´»æ¨¡æ¿: {template.name} (ID: {template.id})")
        else:
            prompt = default_prompt
            logger.info(f"â„¹ï¸ [Promptç®¡ç†] ä½¿ç”¨é»˜è®¤ç¡¬ç¼–ç prompt (æ™ºèƒ½ä½“: {agent_id})")
    except Exception as e:
        logger.warning(f"âš ï¸ [Promptç®¡ç†] è·å–promptå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤prompt: {e}")
        prompt = default_prompt
    
    # æ›¿æ¢å ä½ç¬¦ï¼ˆå¢å¼ºç‰ˆï¼‰
    try:
        import re
        
        # 1. å…ˆæ›¿æ¢åŒèŠ±æ‹¬å·ä¸ºå•èŠ±æ‹¬å·ï¼ˆå…¼å®¹æ€§å¤„ç†ï¼‰
        # å¤„ç† {{variable}} æ ¼å¼ï¼ˆPython f-string è½¬ä¹‰ï¼‰
        prompt = prompt.replace("{{", "{").replace("}}", "}")
        
        # 2. æ›¿æ¢å¸¸è§çš„å ä½ç¬¦
        prompt = prompt.replace("{user_query}", kwargs.get("user_query", ""))
        prompt = prompt.replace("{task_description}", kwargs.get("task_description", ""))
        prompt = prompt.replace("{analysis_context}", kwargs.get("analysis_context", ""))
        prompt = prompt.replace("{full_context}", kwargs.get("full_context", ""))
        prompt = prompt.replace("{final_answer}", kwargs.get("final_answer", ""))
        
        # 3. æ›¿æ¢å…¶ä»–è‡ªå®šä¹‰å ä½ç¬¦
        for key, value in kwargs.items():
            placeholder = f"{{{key}}}"
            if placeholder in prompt:
                prompt = prompt.replace(placeholder, str(value))
        
        # 4. æ£€æŸ¥æœªæ›¿æ¢çš„å ä½ç¬¦ï¼ˆè­¦å‘Šï¼‰
        unmatched = re.findall(r'\{(\w+)\}', prompt)
        if unmatched:
            # è¿‡æ»¤æ‰å·²ç»æ›¿æ¢çš„å ä½ç¬¦
            replaced_placeholders = ["user_query", "task_description", "analysis_context", 
                                   "full_context", "final_answer"] + list(kwargs.keys())
            truly_unmatched = [p for p in unmatched if p not in replaced_placeholders]
            
            if truly_unmatched:
                logger.warning(f"âš ï¸ [Promptç®¡ç†] æœªæ›¿æ¢çš„å ä½ç¬¦: {truly_unmatched}")
                # å¯ä»¥é€‰æ‹©ç§»é™¤æœªæ›¿æ¢çš„å ä½ç¬¦ï¼Œæˆ–ä¿ç•™ï¼ˆè¿™é‡Œé€‰æ‹©ä¿ç•™å¹¶è®°å½•è­¦å‘Šï¼‰
                for var in truly_unmatched:
                    # ä¿ç•™å ä½ç¬¦ï¼Œä½†æ·»åŠ è­¦å‘Šæ ‡è®°ï¼ˆå¯é€‰ï¼‰
                    # prompt = prompt.replace(f"{{{var}}}", f"[å ä½ç¬¦{var}æœªå®šä¹‰]")
                    pass
        
    except Exception as e:
        logger.warning(f"âš ï¸ [Promptç®¡ç†] æ›¿æ¢å ä½ç¬¦æ—¶å‡ºé”™: {e}")
    
    return prompt


# ==================== æ£€ç´¢ä¸“å®¶ï¼ˆRetrieval Specialistï¼‰ ====================

async def retrieval_specialist_node(
    state: MultiAgentState,
    settings: Settings,
    session: Session,
    tool_records: List[ToolRecord],
) -> Dict[str, Any]:
    """
    æ£€ç´¢ä¸“å®¶æ™ºèƒ½ä½“
    
    èŒè´£ï¼š
    - çŸ¥è¯†åº“æ£€ç´¢ï¼ˆRAGï¼‰
    - ç½‘ç»œæœç´¢
    - æ–‡æ¡£æŸ¥æ‰¾
    
    èƒ½åŠ›ï¼š
    - å‘é‡æ£€ç´¢
    - å…³é”®è¯æœç´¢
    - ç½‘é¡µæœç´¢å·¥å…·
    """
    logger.info("ğŸ” [æ£€ç´¢ä¸“å®¶] å¼€å§‹æ‰§è¡Œæ£€ç´¢ä»»åŠ¡...")
    
    workspace = SharedWorkspace(state)
    agent_id = "retrieval_specialist"
    
    # æ³¨å†Œæ™ºèƒ½ä½“
    workspace.register_agent(agent_id)
    workspace.update_agent_status(agent_id, "running")
    
    user_query = state.get("user_query", "")
    use_knowledge_base = state.get("use_knowledge_base", False)
    
    retrieval_results = {}
    thoughts = []
    observations = []
    
    try:
        # 1. çŸ¥è¯†åº“æ£€ç´¢ï¼ˆæ™ºèƒ½ç‰ˆï¼šå¸¦ç½®ä¿¡åº¦è¯„ä¼°ï¼‰
        if use_knowledge_base:
            try:
                logger.info("ğŸ“š æ‰§è¡ŒçŸ¥è¯†åº“æ£€ç´¢ï¼ˆå¸¦ç½®ä¿¡åº¦è¯„ä¼°ï¼‰...")
                
                # ä½¿ç”¨å¸¦ç½®ä¿¡åº¦çš„æ£€ç´¢å‡½æ•°
                from .rag_service import retrieve_context_with_confidence
                
                contexts, confidence = retrieve_context_with_confidence(
                    query=user_query,
                    settings=settings,
                    top_k=5,
                    confidence_threshold=0.3,  # ç½®ä¿¡åº¦é˜ˆå€¼
                )
                
                # æ ¹æ®ç½®ä¿¡åº¦åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æ£€ç´¢ç»“æœ
                if contexts and confidence == "high":
                    # é«˜ç½®ä¿¡åº¦ï¼šä½¿ç”¨æ£€ç´¢ç»“æœ
                    retrieval_results["knowledge_base"] = [
                        {
                            "document_id": ctx.document_id,
                            "original_name": ctx.original_name,
                            "content": ctx.content[:500],
                        }
                        for ctx in contexts
                    ]
                    thoughts.append(f"âœ… ä»çŸ¥è¯†åº“æ£€ç´¢åˆ° {len(contexts)} ä¸ªé«˜ç›¸å…³æ€§ç‰‡æ®µ")
                    observations.append(
                        f"çŸ¥è¯†åº“æ£€ç´¢å®Œæˆï¼šæ‰¾åˆ° {len(contexts)} ä¸ªç›¸å…³æ–‡æ¡£ï¼ˆé«˜ç½®ä¿¡åº¦ï¼‰"
                    )
                    logger.info(f"âœ… çŸ¥è¯†åº“æ£€ç´¢æˆåŠŸï¼Œç½®ä¿¡åº¦ï¼š{confidence}")
                    
                elif contexts and confidence == "low":
                    # ä½ç½®ä¿¡åº¦ï¼šä¸ä½¿ç”¨æ£€ç´¢ç»“æœï¼Œè®°å½•æ—¥å¿—
                    thoughts.append(f"âš ï¸ çŸ¥è¯†åº“æ£€ç´¢ç½®ä¿¡åº¦è¾ƒä½ï¼Œå†…å®¹å¯èƒ½ä¸ç›¸å…³")
                    observations.append(
                        f"çŸ¥è¯†åº“æ£€ç´¢å®Œæˆï¼Œä½†ç›¸å…³æ€§è¾ƒä½ï¼ˆå°†ä¼˜å…ˆä½¿ç”¨å…¶ä»–ä¿¡æ¯æºï¼‰"
                    )
                    logger.warning(f"âš ï¸ çŸ¥è¯†åº“æ£€ç´¢ç½®ä¿¡åº¦ä½ï¼Œè·³è¿‡ä½¿ç”¨æ£€ç´¢ç»“æœ")
                    # ä¸æ·»åŠ åˆ° retrieval_resultsï¼Œè®©åç»­æµç¨‹ä½¿ç”¨å·¥å…·è°ƒç”¨
                    
                else:
                    # æœªæ‰¾åˆ°å†…å®¹
                    thoughts.append("çŸ¥è¯†åº“æ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
                    observations.append("çŸ¥è¯†åº“ä¸ºç©ºæˆ–æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
                    logger.info("çŸ¥è¯†åº“æ£€ç´¢ä¸ºç©º")
            
            except Exception as e:
                logger.error(f"çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {e}")
                thoughts.append(f"çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {str(e)}")
        
        # 2. æ™ºèƒ½å·¥å…·è°ƒç”¨ï¼ˆé€šç”¨æ–¹æ¡ˆï¼‰- è®©LLMè‡ªå·±åˆ¤æ–­éœ€è¦ä»€ä¹ˆå·¥å…·
        has_kb_results = "knowledge_base" in retrieval_results and len(retrieval_results["knowledge_base"]) > 0
        
        # æ„å»ºå·¥å…·æè¿°
        from .tool_service import BUILTIN_TOOLS
        
        tool_descriptions = []
        available_tools_map = {}  # tool_key -> tool_record
        
        for tool in tool_records:
            try:
                import json
                config = json.loads(tool.config or "{}")
                builtin_key = config.get("builtin_key")
                
                if builtin_key and builtin_key in BUILTIN_TOOLS:
                    tool_def = BUILTIN_TOOLS[builtin_key]
                    tool_descriptions.append(
                        f"- **{tool_def.name}** ({builtin_key}): {tool_def.description}"
                    )
                    available_tools_map[builtin_key] = tool
            except:
                continue
        
        if available_tools_map:
            # æ„å»ºè¯¦ç»†çš„å·¥å…·schemaä¿¡æ¯
            tool_schemas = []
            for tool_key, tool in available_tools_map.items():
                config = json.loads(tool.config or "{}")
                builtin_key = config.get("builtin_key")
                if builtin_key and builtin_key in BUILTIN_TOOLS:
                    tool_def = BUILTIN_TOOLS[builtin_key]
                    tool_schemas.append({
                        "key": builtin_key,
                        "name": tool_def.name,
                        "description": tool_def.description,
                        "schema": tool_def.input_schema
                    })
            
            # ä½¿ç”¨LLMæ™ºèƒ½åˆ¤æ–­éœ€è¦è°ƒç”¨å“ªäº›å·¥å…·
            tool_selection_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå·¥å…·è°ƒç”¨ä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·é—®é¢˜ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š{user_query}

ã€çŸ¥è¯†åº“æ£€ç´¢çŠ¶æ€ã€‘ï¼š{"âœ… å·²æ‰¾åˆ° " + str(len(retrieval_results.get("knowledge_base", []))) + " ä¸ªç›¸å…³å†…å®¹" if has_kb_results else "âŒ çŸ¥è¯†åº“æ— ç›¸å…³å†…å®¹æˆ–æœªå¯ç”¨"}

ã€å¯ç”¨å·¥å…·åŠå…¶å‚æ•°ã€‘ï¼š
{json.dumps(tool_schemas, ensure_ascii=False, indent=2)}

ã€åˆ¤æ–­è§„åˆ™ã€‘ï¼š
1. æ£€ç´¢ä¸“å®¶çš„æ ¸å¿ƒèŒè´£æ˜¯**æ”¶é›†ä¿¡æ¯**ï¼Œä¸è´Ÿè´£æœ€ç»ˆå†…å®¹è¾“å‡º
2. âœ… åº”è¯¥è°ƒç”¨ï¼šweb_searchï¼ˆç½‘é¡µæœç´¢ï¼‰ã€search_knowledgeï¼ˆçŸ¥è¯†åº“æ£€ç´¢ï¼‰ç­‰ä¿¡æ¯è·å–å·¥å…·
3. âŒ ä¸è¦è°ƒç”¨ï¼šwrite_noteï¼ˆå†™å…¥ç¬”è®°ï¼‰ã€draw_diagramï¼ˆç»˜åˆ¶å›¾è¡¨ï¼‰ç­‰å†…å®¹è¾“å‡ºå·¥å…·
4. å†…å®¹è¾“å‡ºå·¥å…·åº”è¯¥åœ¨åˆ†æå’Œæ€»ç»“å®Œæˆåç”±åç»­ä¸“å®¶è°ƒç”¨
5. å¦‚æœç”¨æˆ·é—®é¢˜éœ€è¦å®æ—¶æ•°æ®ï¼ˆå¤©æ°”ã€æ–°é—»ã€æœ€æ–°ä¿¡æ¯ç­‰ï¼‰ï¼Œåº”è¯¥è°ƒç”¨web_search

ã€å‚æ•°è¯´æ˜ã€‘ï¼š
- å¯¹äºweb_searchå·¥å…·ï¼šæä¾›æ¸…æ™°çš„æœç´¢æŸ¥è¯¢ï¼Œnum_resultsé€šå¸¸è®¾ä¸º5-10
- å¯¹äºsearch_knowledgeå·¥å…·ï¼šæä¾›å‡†ç¡®çš„æŸ¥è¯¢å…³é”®è¯ï¼Œtop_kè®¾ä¸º3-5
- å¯¹äºå…¶ä»–ä¿¡æ¯è·å–å·¥å…·ï¼šæä¾›å®Œæ•´çš„å¿…éœ€å‚æ•°

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºéœ€è¦è°ƒç”¨çš„å·¥å…·ï¼ˆåªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šï¼‰ï¼š
{{
  "need_tools": true/false,
  "tools_to_call": [
    {{
      "tool_key": "å·¥å…·çš„key",
      "reason": "è°ƒç”¨åŸå› ",
      "arguments": {{å®Œæ•´çš„å‚æ•°å¯¹è±¡}}
    }}
  ],
  "reasoning": "åˆ¤æ–­ç†ç”±"
}}
"""
            
            try:
                logger.info("ğŸ¤– ä½¿ç”¨LLMæ™ºèƒ½åˆ¤æ–­éœ€è¦è°ƒç”¨çš„å·¥å…·...")
                
                tool_decision, _ = await invoke_llm(
                    messages=[{"role": "user", "content": tool_selection_prompt}],
                    settings=settings,
                    temperature=0.2,
                    max_tokens=4000,  # å¢åŠ tokené™åˆ¶ä»¥æ”¯æŒç”Ÿæˆå®Œæ•´çš„å·¥å…·å‚æ•°
                )
                
                decision_data = parse_json_from_llm(tool_decision)
                need_tools = decision_data.get("need_tools", False)
                tools_to_call = decision_data.get("tools_to_call", [])
                reasoning = decision_data.get("reasoning", "")
                
                logger.info(f"ğŸ§  LLMåˆ¤æ–­ï¼šneed_tools={need_tools}, ç†ç”±={reasoning}")
                
                if need_tools and tools_to_call:
                    thoughts.append(f"LLMå†³ç­–ï¼šéœ€è¦è°ƒç”¨ {len(tools_to_call)} ä¸ªå·¥å…·")
                    
                    # æ‰§è¡ŒLLMå»ºè®®çš„å·¥å…·è°ƒç”¨
                    for tool_call in tools_to_call:
                        tool_key = tool_call.get("tool_key")
                        tool_reason = tool_call.get("reason", "")
                        tool_args = tool_call.get("arguments", {})
                        
                        if tool_key in available_tools_map:
                            try:
                                tool_record = available_tools_map[tool_key]
                                logger.info(f"ğŸ”§ æ‰§è¡Œå·¥å…·ï¼š{tool_key}ï¼ŒåŸå› ï¼š{tool_reason}")
                                
                                # å¯¹äºéœ€è¦ç”Ÿæˆå†…å®¹çš„å·¥å…·ï¼Œå…ˆç”¨LLMç”Ÿæˆå†…å®¹
                                if tool_key == "write_note":
                                    # ç¡®ä¿æœ‰filename
                                    if not tool_args.get("filename"):
                                        tool_args["filename"] = f"note_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
                                    
                                    # å¦‚æœå†…å®¹ä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œåˆ™ç”Ÿæˆå†…å®¹
                                    if not tool_args.get("content") or len(str(tool_args.get("content", ""))) < 100:
                                        logger.info(f"ğŸ“ ä¸ºwrite_noteå·¥å…·ç”Ÿæˆå†…å®¹...")
                                        kb_context = ""
                                        if retrieval_results.get("knowledge_base"):
                                            kb_context = "\n\n".join([f"ã€æ–‡æ¡£ç‰‡æ®µ {i+1}ã€‘\n{ctx.content}" 
                                                                      for i, ctx in enumerate(retrieval_results.get("knowledge_base", [])[:5])])
                                        
                                        # æ”¶é›†æ‰€æœ‰å¯ç”¨çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆåŒ…æ‹¬å…¶ä»–å·¥å…·çš„ç»“æœï¼‰
                                        tool_results_context = ""
                                        if retrieval_results:
                                            for tool_name, tool_data in retrieval_results.items():
                                                if tool_name != "knowledge_base" and isinstance(tool_data, dict):
                                                    result_str = tool_data.get("result", "")
                                                    if result_str:
                                                        tool_results_context += f"\n\nã€{tool_name}å·¥å…·ç»“æœã€‘ï¼š\n{result_str[:2000]}"  # é™åˆ¶é•¿åº¦
                                        
                                        content_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆå®Œæ•´çš„ç¬”è®°å†…å®¹ï¼š

ã€ç”¨æˆ·éœ€æ±‚ã€‘ï¼š{user_query}

ã€çŸ¥è¯†åº“å†…å®¹ã€‘ï¼š
{kb_context if kb_context else "ï¼ˆæ— ç›¸å…³çŸ¥è¯†åº“å†…å®¹ï¼‰"}
{tool_results_context}

ã€ä»»åŠ¡ã€‘ï¼šæ ¹æ®ç”¨æˆ·çš„å…·ä½“éœ€æ±‚ï¼Œç”Ÿæˆä¸€ä»½è¯¦ç»†çš„æŠ€æœ¯æ€»ç»“æ–‡æ¡£ï¼ˆMarkdownæ ¼å¼ï¼‰ã€‚
- å¦‚æœç”¨æˆ·è¦æ±‚åˆ†ææŸä¸ªä¸»é¢˜ï¼Œè¯·æä¾›å…¨é¢çš„åˆ†æ
- å¦‚æœç”¨æˆ·è¦æ±‚æ€»ç»“å‰æ²¿å†…å®¹ï¼Œè¯·é‡ç‚¹å…³æ³¨æœ€æ–°è¿›å±•å’Œæœªæ¥æ–¹å‘
- å¦‚æœç”¨æˆ·è¦æ±‚è¯†åˆ«åˆ›æ–°ç‚¹ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºçªç ´æ€§çš„æŠ€æœ¯ç‚¹
- æ–‡æ¡£åº”ç»“æ„æ¸…æ™°ã€å†…å®¹å……å®ã€é€»è¾‘è¿è´¯

è¯·ç›´æ¥è¾“å‡ºå®Œæ•´çš„Markdownæ–‡æ¡£å†…å®¹ï¼Œä¸è¦æœ‰ä»»ä½•å‰ç¼€è¯´æ˜ã€‚"""
                                        
                                        content, _ = await invoke_llm(
                                            messages=[{"role": "user", "content": content_prompt}],
                                            settings=settings,
                                            temperature=0.7,
                                            max_tokens=4000,
                                        )
                                        tool_args["content"] = content.strip()
                                        logger.info(f"âœ… å·²ç”Ÿæˆç¬”è®°å†…å®¹ï¼Œé•¿åº¦ï¼š{len(content)} å­—ç¬¦")
                                
                                elif tool_key == "draw_diagram":
                                    # ç¡®ä¿æœ‰filename
                                    if not tool_args.get("filename"):
                                        tool_args["filename"] = f"diagram_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
                                    
                                    # å¦‚æœdiagram_codeä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œåˆ™ç”Ÿæˆä»£ç 
                                    if not tool_args.get("diagram_code") or len(str(tool_args.get("diagram_code", ""))) < 50:
                                        logger.info(f"ğŸ¨ ä¸ºdraw_diagramå·¥å…·ç”Ÿæˆå›¾è¡¨ä»£ç ...")
                                        kb_context = ""
                                        if retrieval_results.get("knowledge_base"):
                                            kb_context = "\n\n".join([f"ã€æ–‡æ¡£ç‰‡æ®µ {i+1}ã€‘\n{ctx.content}" 
                                                                      for i, ctx in enumerate(retrieval_results.get("knowledge_base", [])[:5])])
                                        
                                        # æ”¶é›†æ‰€æœ‰å¯ç”¨çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆåŒ…æ‹¬å…¶ä»–å·¥å…·çš„ç»“æœï¼‰
                                        tool_results_context = ""
                                        if retrieval_results:
                                            for tool_name, tool_data in retrieval_results.items():
                                                if tool_name != "knowledge_base" and isinstance(tool_data, dict):
                                                    result_str = tool_data.get("result", "")
                                                    if result_str:
                                                        tool_results_context += f"\n\nã€{tool_name}å·¥å…·ç»“æœã€‘ï¼š\n{result_str[:1500]}"  # é™åˆ¶é•¿åº¦
                                        
                                        diagram_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”ŸæˆMermaidæ€ç»´å¯¼å›¾ä»£ç ï¼š

ã€ç”¨æˆ·éœ€æ±‚ã€‘ï¼š{user_query}

ã€çŸ¥è¯†åº“å†…å®¹ã€‘ï¼š
{kb_context if kb_context else "ï¼ˆæ— ç›¸å…³çŸ¥è¯†åº“å†…å®¹ï¼‰"}
{tool_results_context}

ã€ä»»åŠ¡ã€‘ï¼šæ ¹æ®ç”¨æˆ·éœ€æ±‚å’Œæä¾›çš„ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªç»“æ„æ¸…æ™°çš„Mermaidæ€ç»´å¯¼å›¾ï¼ˆgraph TDæ ¼å¼ï¼‰ã€‚
- æå–æ ¸å¿ƒä¸»é¢˜å’Œå…³é”®æ¦‚å¿µ
- æ„å»ºåˆç†çš„å±‚çº§ç»“æ„
- çªå‡ºé‡è¦çš„å…³è”å…³ç³»
- ç¡®ä¿å›¾è¡¨æ˜“äºç†è§£

ã€è¯­æ³•è¦æ±‚ã€‘ï¼š
- ä½¿ç”¨æ ‡å‡†çš„ graph TD æ ¼å¼
- èŠ‚ç‚¹æ ¼å¼ï¼šA[æ–‡æœ¬å†…å®¹]
- è¿æ¥æ ¼å¼ï¼šA --> B
- ä¸è¦ä½¿ç”¨ ::iconã€:::classã€classDef ç­‰é«˜çº§è¯­æ³•
- ä¸è¦ä½¿ç”¨ subgraphï¼ˆå­å›¾ï¼‰
- ä¿æŒè¯­æ³•ç®€æ´æ ‡å‡†

è¯·ç›´æ¥è¾“å‡ºå®Œæ•´çš„Mermaidä»£ç ï¼Œä¸è¦æœ‰markdownä»£ç å—æ ‡è®°ï¼ˆä¸è¦```mermaidï¼‰ï¼Œä¸è¦æœ‰ä»»ä½•å‰ç¼€è¯´æ˜ã€‚
ç¤ºä¾‹æ ¼å¼ï¼š
graph TD
    A[ä¸»é¢˜] --> B[å­æ¦‚å¿µ1]
    A --> C[å­æ¦‚å¿µ2]
    B --> B1[ç»†èŠ‚]
"""
                                        
                                        diagram_code, _ = await invoke_llm(
                                            messages=[{"role": "user", "content": diagram_prompt}],
                                            settings=settings,
                                            temperature=0.7,
                                            max_tokens=2000,
                                        )
                                        # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                                        diagram_code = diagram_code.strip()
                                        if diagram_code.startswith("```"):
                                            diagram_code = diagram_code.split("```", 2)[1]
                                            if diagram_code.startswith("mermaid"):
                                                diagram_code = diagram_code[7:]
                                            diagram_code = diagram_code.strip()
                                        if diagram_code.endswith("```"):
                                            diagram_code = diagram_code[:-3].strip()
                                        
                                        # æ¸…ç†ä¸æ”¯æŒçš„Mermaidè¯­æ³•
                                        import re
                                        # ç§»é™¤ ::icon(...) è¯­æ³•
                                        diagram_code = re.sub(r'\s*::icon\([^)]*\)', '', diagram_code)
                                        # ç§»é™¤ :::className è¯­æ³•
                                        diagram_code = re.sub(r'\s*:::[^\s\n]+', '', diagram_code)
                                        # ç§»é™¤ classDef å®šä¹‰
                                        diagram_code = re.sub(r'classDef\s+\w+\s+[^\n]+\n?', '', diagram_code)
                                        # ç§»é™¤ class èµ‹å€¼
                                        diagram_code = re.sub(r'class\s+[\w,]+\s+\w+\s*\n?', '', diagram_code)
                                        
                                        tool_args["diagram_code"] = diagram_code
                                        logger.info(f"âœ… å·²ç”Ÿæˆå›¾è¡¨ä»£ç ï¼Œé•¿åº¦ï¼š{len(diagram_code)} å­—ç¬¦")
                                
                                # æ‰§è¡Œå·¥å…·
                                result = execute_tool(
                                    tool=tool_record,
                                    arguments=tool_args,
                                    settings=settings,
                                    session=session,
                                )
                                
                                # ä¿å­˜ç»“æœ
                                retrieval_results[tool_key] = {
                                    "arguments": tool_args,
                                    "result": result,
                                }
                                
                                thoughts.append(f"âœ… å·¥å…·è°ƒç”¨æˆåŠŸï¼š{tool_key}")
                                observations.append(f"å·¥å…· {tool_key} æ‰§è¡Œå®Œæˆï¼š{tool_reason}")
                                logger.info(f"âœ… å·¥å…· {tool_key} è°ƒç”¨æˆåŠŸ")
                                
                            except Exception as e:
                                logger.error(f"å·¥å…· {tool_key} è°ƒç”¨å¤±è´¥: {e}")
                                thoughts.append(f"âŒ å·¥å…· {tool_key} è°ƒç”¨å¤±è´¥: {str(e)}")
                        else:
                            logger.warning(f"âš ï¸ å·¥å…· {tool_key} ä¸å¯ç”¨")
                else:
                    thoughts.append(f"LLMåˆ¤æ–­ï¼šæ— éœ€è°ƒç”¨å·¥å…·ï¼ˆ{reasoning}ï¼‰")
                    logger.info(f"ğŸ’¡ LLMåˆ¤æ–­æ— éœ€å·¥å…·è°ƒç”¨ï¼š{reasoning}")
                    
            except Exception as e:
                logger.error(f"æ™ºèƒ½å·¥å…·åˆ¤æ–­å¤±è´¥: {e}")
                thoughts.append(f"æ™ºèƒ½å·¥å…·åˆ¤æ–­å¤±è´¥ï¼Œè·³è¿‡å·¥å…·è°ƒç”¨")
        
        # 4. å­˜å‚¨ç»“æœåˆ°å…±äº«å·¥ä½œç©ºé—´
        workspace.store_agent_result(agent_id, retrieval_results)
        workspace.set_shared_data("retrieval_results", retrieval_results)
        
        # 5. å‘é€ç»“æœæ¶ˆæ¯ç»™åè°ƒå™¨
        workspace.send_message(
            from_agent=agent_id,
            to_agent="orchestrator",
            message_type="result",
            content={
                "status": "completed",
                "retrieval_results": retrieval_results,
                "summary": f"æ£€ç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(retrieval_results)} ç±»ç»“æœ",
            },
        )
        
        workspace.update_agent_status(agent_id, "completed")
        
        logger.info(f"âœ… [æ£€ç´¢ä¸“å®¶] æ‰§è¡Œå®Œæˆï¼Œæ‰¾åˆ° {len(retrieval_results)} ç±»ç»“æœ")
        
        return {
            "agent_thoughts": {agent_id: thoughts},
            "agent_observations": {agent_id: observations},
            "retrieved_contexts": retrieval_results.get("knowledge_base", []),
        }
    
    except Exception as e:
        logger.error(f"âŒ [æ£€ç´¢ä¸“å®¶] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        workspace.update_agent_status(agent_id, "failed")
        workspace.send_message(
            from_agent=agent_id,
            to_agent="orchestrator",
            message_type="error",
            content={"error": str(e)},
        )
        
        return {
            "agent_thoughts": {agent_id: [f"æ‰§è¡Œå¤±è´¥: {str(e)}"]},
            "error": str(e),
        }


# ==================== åˆ†æä¸“å®¶ï¼ˆAnalysis Specialistï¼‰ ====================

async def analysis_specialist_node(
    state: MultiAgentState,
    settings: Settings,
    session: Session,
) -> Dict[str, Any]:
    """
    åˆ†æä¸“å®¶æ™ºèƒ½ä½“
    
    èŒè´£ï¼š
    - æ•°æ®åˆ†æ
    - å†…å®¹ç†è§£
    - å…³é”®ä¿¡æ¯æå–
    
    èƒ½åŠ›ï¼š
    - æ–‡æœ¬åˆ†æï¼ˆä½¿ç”¨ LLMï¼‰
    - æ•°æ®æå–
    - æ¨¡å¼è¯†åˆ«
    """
    logger.info("ğŸ“Š [åˆ†æä¸“å®¶] å¼€å§‹æ‰§è¡Œåˆ†æä»»åŠ¡...")
    
    workspace = SharedWorkspace(state)
    agent_id = "analysis_specialist"
    
    workspace.register_agent(agent_id)
    workspace.update_agent_status(agent_id, "running")
    
    user_query = state.get("user_query", "")
    thoughts = []
    observations = []
    
    try:
        # 1. è·å–æ£€ç´¢ä¸“å®¶çš„ç»“æœ
        retrieval_results = workspace.get_shared_data("retrieval_results", {})
        
        if not retrieval_results:
            thoughts.append("æœªæ‰¾åˆ°æ£€ç´¢ç»“æœï¼Œä½¿ç”¨ç”¨æˆ·æŸ¥è¯¢è¿›è¡Œåˆ†æ")
            analysis_context = f"ç”¨æˆ·æŸ¥è¯¢ï¼š{user_query}"
        else:
            # æ„å»ºåˆ†æä¸Šä¸‹æ–‡
            context_parts = []
            
            if "knowledge_base" in retrieval_results:
                kb_contexts = retrieval_results["knowledge_base"]
                context_parts.append(
                    f"çŸ¥è¯†åº“å†…å®¹ï¼ˆ{len(kb_contexts)} ä¸ªç‰‡æ®µï¼‰:\n"
                    + "\n".join([
                        f"- {ctx.get('content', '')[:200]}"
                        for ctx in kb_contexts[:3]
                    ])
                )
            
            if "web_search" in retrieval_results:
                search_data = retrieval_results["web_search"]
                context_parts.append(
                    f"æœç´¢ç»“æœï¼ˆå…³é”®è¯: {search_data.get('query', '')}):\n"
                    f"{search_data.get('results', '')[:500]}"
                )
            
            analysis_context = "\n\n".join(context_parts)
            thoughts.append(f"è·å–åˆ°æ£€ç´¢ç»“æœï¼Œå‡†å¤‡åˆ†æ")
        
        # 2. ä½¿ç”¨ LLM è¿›è¡Œæ·±åº¦åˆ†æ
        logger.info("ğŸ¤” ä½¿ç”¨ LLM è¿›è¡Œå†…å®¹åˆ†æ...")
        
        # è·å–å½“å‰å­ä»»åŠ¡çš„æè¿°ï¼Œä»¥ä¾¿é’ˆå¯¹æ€§åˆ†æ
        current_subtask = workspace.get_current_subtask()
        task_description = current_subtask.description if current_subtask else "æ·±åº¦åˆ†æå†…å®¹"
        
        # é»˜è®¤promptï¼ˆå¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æ¿€æ´»çš„æ¨¡æ¿ï¼Œä½¿ç”¨è¿™ä¸ªï¼‰
        default_analysis_prompt = f"""ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„æŠ€æœ¯åˆ†æä¸“å®¶å’Œç ”ç©¶é¡¾é—®ã€‚è¯·å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œæ·±åº¦ã€ç³»ç»ŸåŒ–çš„åˆ†æã€‚

ã€ä»»åŠ¡è¦æ±‚ã€‘ï¼š{task_description}

ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š{user_query}

ã€å¾…åˆ†æå†…å®¹ã€‘ï¼š
{analysis_context}

ã€åˆ†æç»´åº¦ã€‘è¯·ä»ä»¥ä¸‹å¤šä¸ªç»´åº¦è¿›è¡Œæ·±å…¥åˆ†æï¼š

1. **æ ¸å¿ƒæ¦‚å¿µè¯†åˆ«**ï¼š
   - è¯†åˆ«å¹¶è§£é‡Šæ ¸å¿ƒæŠ€æœ¯æ¦‚å¿µã€æœ¯è¯­
   - åŒºåˆ†åŸºç¡€æ¦‚å¿µä¸é«˜çº§æ¦‚å¿µ

2. **å…³é”®ä¿¡æ¯æå–**ï¼š
   - æå–é‡è¦äº‹å®ã€æ•°æ®ã€ç»Ÿè®¡ä¿¡æ¯
   - è¯†åˆ«å…³é”®è®ºç‚¹å’Œç»“è®º
   - æ ‡æ³¨ä¿¡æ¯æ¥æºï¼ˆå¦‚æœ‰ï¼‰

3. **æŠ€æœ¯åŸç†åˆ†æ**ï¼ˆå¦‚é€‚ç”¨ï¼‰ï¼š
   - è§£é‡ŠæŠ€æœ¯å®ç°åŸç†
   - åˆ†ææŠ€æœ¯æ¶æ„å’Œè®¾è®¡æ€è·¯
   - å¯¹æ¯”ä¸åŒæŠ€æœ¯æ–¹æ¡ˆçš„ä¼˜åŠ£

4. **å…³è”æ€§åˆ†æ**ï¼š
   - å‘ç°æ¦‚å¿µä¹‹é—´çš„é€»è¾‘å…³ç³»
   - è¯†åˆ«å› æœå…³ç³»ã€æ¼”è¿›å…³ç³»
   - æ„å»ºçŸ¥è¯†å›¾è°±å¼çš„å…³è”

5. **è¶‹åŠ¿ä¸æ´å¯Ÿ**ï¼š
   - è¯†åˆ«æŠ€æœ¯æ¼”è¿›è¶‹åŠ¿
   - å‘ç°æ½œåœ¨é—®é¢˜å’ŒæŒ‘æˆ˜
   - é¢„æµ‹æœªæ¥å‘å±•æ–¹å‘

6. **æ‰¹åˆ¤æ€§æ€è€ƒ**ï¼š
   - æŒ‡å‡ºä¿¡æ¯çš„å±€é™æ€§
   - è¯†åˆ«å¯èƒ½å­˜åœ¨çš„åè§æˆ–äº‰è®®
   - æå‡ºéœ€è¦è¿›ä¸€æ­¥éªŒè¯çš„ç‚¹

ä»¥ JSON æ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼š
{{
  "core_concepts": [
    {{"concept": "æ¦‚å¿µåç§°", "explanation": "è¯¦ç»†è§£é‡Š", "importance": "high|medium|low"}}
  ],
  "key_facts": [
    {{"fact": "äº‹å®æè¿°", "source": "æ¥æºï¼ˆå¦‚æœ‰ï¼‰", "confidence": "high|medium|low"}}
  ],
  "key_data": [
    {{"data_point": "æ•°æ®ç‚¹", "value": "å…·ä½“æ•°å€¼æˆ–æè¿°", "context": "èƒŒæ™¯è¯´æ˜"}}
  ],
  "technical_principles": [
    {{"principle": "åŸç†åç§°", "explanation": "åŸç†è§£é‡Š", "advantages": ["ä¼˜åŠ¿1"], "limitations": ["å±€é™1"]}}
  ],
  "relationships": [
    {{"from": "æ¦‚å¿µA", "to": "æ¦‚å¿µB", "relationship_type": "å› æœ|æ¼”è¿›|å¯¹æ¯”|è¡¥å……", "description": "å…³ç³»æè¿°"}}
  ],
  "trends_insights": [
    {{"trend": "è¶‹åŠ¿æè¿°", "evidence": "æ”¯æŒè¯æ®", "implications": "å½±å“åˆ†æ"}}
  ],
  "critical_notes": [
    {{"note_type": "å±€é™æ€§|äº‰è®®ç‚¹|å¾…éªŒè¯", "description": "è¯¦ç»†è¯´æ˜"}}
  ],
  "analysis_summary": "å…¨é¢çš„åˆ†ææ€»ç»“ï¼ˆ300-500å­—ï¼‰",
  "confidence_score": 0.0-1.0
}}

è¦æ±‚ï¼š
- åˆ†æè¦æ·±å…¥ã€ç³»ç»Ÿã€å…¨é¢
- ä¿æŒå®¢è§‚ï¼Œé¿å…ä¸»è§‚è‡†æ–­
- ä¼˜å…ˆä½¿ç”¨æä¾›çš„å†…å®¹ï¼Œæ ‡æ³¨æ¨ç†éƒ¨åˆ†
- é•¿åº¦ï¼š500-1000å­—çš„æ·±åº¦åˆ†æ

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
"""
        
        # ä»æ•°æ®åº“è·å–æ¿€æ´»çš„promptï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤prompt
        analysis_prompt = get_agent_prompt(
            agent_id="analysis_specialist",
            session=session,
            default_prompt=default_analysis_prompt,
            user_query=user_query,
            task_description=task_description,
            analysis_context=analysis_context
        )
        
        llm_response, _ = await invoke_llm(
            messages=[{"role": "user", "content": analysis_prompt}],
            settings=settings,
            temperature=0.4,  # ç¨é«˜çš„æ¸©åº¦ä»¥è·å¾—æ›´æœ‰åˆ›æ„çš„æ´å¯Ÿ
            max_tokens=2500,  # å¢åŠ tokené™åˆ¶ä»¥æ”¯æŒæ›´æ·±å…¥çš„åˆ†æ
        )
        
        # è§£æ LLM å“åº”
        analysis_result = parse_json_from_llm(llm_response)
        
        thoughts.append("å®Œæˆå†…å®¹åˆ†æï¼Œæå–äº†å…³é”®ä¿¡æ¯")
        observations.append(
            f"åˆ†æå®Œæˆï¼šè¯†åˆ« {len(analysis_result.get('core_topics', []))} ä¸ªæ ¸å¿ƒä¸»é¢˜ï¼Œ"
            f"{len(analysis_result.get('key_facts', []))} ä¸ªå…³é”®äº‹å®"
        )
        
        # 3. å­˜å‚¨ç»“æœ
        workspace.store_agent_result(agent_id, analysis_result)
        workspace.set_shared_data("analysis_result", analysis_result)
        
        # 4. å‘é€ç»“æœæ¶ˆæ¯
        workspace.send_message(
            from_agent=agent_id,
            to_agent="orchestrator",
            message_type="result",
            content={
                "status": "completed",
                "analysis_result": analysis_result,
            },
        )
        
        workspace.update_agent_status(agent_id, "completed")
        
        logger.info("âœ… [åˆ†æä¸“å®¶] åˆ†æå®Œæˆ")
        
        return {
            "agent_thoughts": {agent_id: thoughts},
            "agent_observations": {agent_id: observations},
        }
    
    except Exception as e:
        logger.error(f"âŒ [åˆ†æä¸“å®¶] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        workspace.update_agent_status(agent_id, "failed")
        workspace.send_message(
            from_agent=agent_id,
            to_agent="orchestrator",
            message_type="error",
            content={"error": str(e)},
        )
        
        return {
            "agent_thoughts": {agent_id: [f"æ‰§è¡Œå¤±è´¥: {str(e)}"]},
            "error": str(e),
        }


# ==================== æ€»ç»“ä¸“å®¶ï¼ˆSummarization Specialistï¼‰ ====================

async def summarization_specialist_node(
    state: MultiAgentState,
    settings: Settings,
    session: Session,
    tool_records: Optional[List["ToolRecord"]] = None,
) -> Dict[str, Any]:
    """
    æ€»ç»“ä¸“å®¶æ™ºèƒ½ä½“
    
    èŒè´£ï¼š
    - ä¿¡æ¯æ•´åˆ
    - æŠ¥å‘Šç”Ÿæˆ
    - ç»“æ„åŒ–è¾“å‡º
    
    èƒ½åŠ›ï¼š
    - å†…å®¹æ€»ç»“
    - æŠ¥å‘Šæ’°å†™
    - æ ¼å¼è½¬æ¢
    """
    logger.info("ğŸ“ [æ€»ç»“ä¸“å®¶] å¼€å§‹æ‰§è¡Œæ€»ç»“ä»»åŠ¡...")
    
    workspace = SharedWorkspace(state)
    agent_id = "summarization_specialist"
    
    workspace.register_agent(agent_id)
    workspace.update_agent_status(agent_id, "running")
    
    user_query = state.get("user_query", "")
    thoughts = []
    observations = []
    
    try:
        # 1. æ”¶é›†æ‰€æœ‰æ™ºèƒ½ä½“çš„ç»“æœ
        retrieval_results = workspace.get_shared_data("retrieval_results", {})
        analysis_result = workspace.get_shared_data("analysis_result", {})
        
        # 2. æ„å»ºæ€»ç»“ä¸Šä¸‹æ–‡ï¼ˆé€šç”¨å¤„ç†æ‰€æœ‰å·¥å…·ç»“æœï¼‰
        context_parts = []
        
        if retrieval_results:
            context_parts.append("## æ£€ç´¢ä¸å·¥å…·æ‰§è¡Œç»“æœ")
            
            # çŸ¥è¯†åº“æ£€ç´¢ç»“æœ
            if "knowledge_base" in retrieval_results:
                kb_contexts = retrieval_results["knowledge_base"]
                context_parts.append(
                    f"### çŸ¥è¯†åº“å†…å®¹ï¼ˆ{len(kb_contexts)} ä¸ªç‰‡æ®µï¼‰\n"
                    + "\n".join([
                        f"{i+1}. {ctx.get('content', '')[:300]}"
                        for i, ctx in enumerate(kb_contexts)
                    ])
                )
            
            # é€šç”¨å·¥å…·ç»“æœå¤„ç† - è‡ªåŠ¨å¤„ç†æ‰€æœ‰å·¥å…·ï¼ˆå¤©æ°”ã€æœç´¢ã€ç¬”è®°ç­‰ï¼‰
            tool_result_keys = [k for k in retrieval_results.keys() if k != "knowledge_base"]
            
            for tool_key in tool_result_keys:
                tool_data = retrieval_results[tool_key]
                
                # è·å–å·¥å…·åç§°
                from .tool_service import BUILTIN_TOOLS
                tool_name = BUILTIN_TOOLS.get(tool_key, type('obj', (object,), {'name': tool_key.replace('_', ' ').title()})).name
                
                # æå–ç»“æœå†…å®¹
                if isinstance(tool_data, dict):
                    result_content = tool_data.get("result", "") or tool_data.get("data", "") or str(tool_data)
                else:
                    result_content = str(tool_data)
                
                # é™åˆ¶é•¿åº¦
                result_preview = result_content[:1000] if len(result_content) > 1000 else result_content
                
                context_parts.append(
                    f"### {tool_name} æ‰§è¡Œç»“æœ\n{result_preview}"
                )
        
        if analysis_result:
            context_parts.append("## åˆ†æç»“æœ")
            context_parts.append(f"æ ¸å¿ƒä¸»é¢˜: {', '.join(analysis_result.get('core_topics', []))}")
            context_parts.append(f"å…³é”®å‘ç°: " + "; ".join(analysis_result.get('key_findings', [])[:3]))
            context_parts.append(f"åˆ†ææ€»ç»“: {analysis_result.get('analysis_summary', '')}")
        
        full_context = "\n\n".join(context_parts)
        
        thoughts.append("æ”¶é›†äº†æ‰€æœ‰æ™ºèƒ½ä½“çš„ç»“æœï¼Œå‡†å¤‡ç”Ÿæˆæ€»ç»“")
        
        # 3. ä½¿ç”¨ LLM ç”Ÿæˆç»¼åˆæ€»ç»“
        logger.info("âœï¸ ä½¿ç”¨ LLM ç”Ÿæˆç»¼åˆæ€»ç»“...")
        
        # è·å–å½“å‰å­ä»»åŠ¡æè¿°
        current_subtask = workspace.get_current_subtask()
        task_description = current_subtask.description if current_subtask else "ç”Ÿæˆå…¨é¢çš„æ€»ç»“æŠ¥å‘Š"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ·±åº¦åˆ†æç»“æœ
        has_deep_analysis = analysis_result and "core_concepts" in analysis_result
        
        # åˆ¤æ–­ä¿¡æ¯è´¨é‡ï¼ˆé€šç”¨æ–¹æ¡ˆï¼‰
        has_kb_info = "knowledge_base" in retrieval_results and len(retrieval_results.get("knowledge_base", [])) > 0
        
        # æ„å»ºä¿¡æ¯æºè¯´æ˜ - è‡ªåŠ¨è¯†åˆ«æ‰€æœ‰å·²æ‰§è¡Œçš„å·¥å…·
        info_sources = []
        if has_kb_info:
            info_sources.append("çŸ¥è¯†åº“å†…å®¹")
        
        # é€šç”¨å¤„ç†ï¼šåˆ—å‡ºæ‰€æœ‰å·²æ‰§è¡Œçš„å·¥å…·
        tool_result_keys = [k for k in retrieval_results.keys() if k != "knowledge_base"]
        if tool_result_keys:
            from .tool_service import BUILTIN_TOOLS
            for tool_key in tool_result_keys:
                tool_name = BUILTIN_TOOLS.get(tool_key, type('obj', (object,), {'name': tool_key.replace('_', ' ').title()})).name
                info_sources.append(tool_name)
        
        if info_sources:
            info_quality_note = f"âœ… å·²è·å–ï¼š{' + '.join(info_sources)}"
        else:
            info_quality_note = "âš ï¸ æ£€ç´¢ä¿¡æ¯æœ‰é™ï¼Œè¯·åŸºäºè‡ªèº«çŸ¥è¯†åˆç†å›ç­”ï¼Œå¹¶è¯´æ˜ä¿¡æ¯æ¥æºçš„å±€é™æ€§"
        
        # é»˜è®¤promptï¼ˆå¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æ¿€æ´»çš„æ¨¡æ¿ï¼Œä½¿ç”¨è¿™ä¸ªï¼‰
        default_summarization_prompt = f"""ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·ç”Ÿæˆæ¸…æ™°ã€å‡†ç¡®çš„å›ç­”ã€‚

ã€ä»»åŠ¡è¦æ±‚ã€‘ï¼š{task_description}

ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š{user_query}

ã€æ”¶é›†åˆ°çš„ä¿¡æ¯ã€‘ï¼š
{full_context if full_context else "ï¼ˆæœªæ£€ç´¢åˆ°ç‰¹å®šä¿¡æ¯ï¼‰"}

ã€ä¿¡æ¯æ¥æºè¯´æ˜ã€‘ï¼š{info_quality_note}

ã€å›ç­”è¦æ±‚ã€‘ï¼š

1. **æ™ºèƒ½é€‰æ‹©ä¿¡æ¯æº**ï¼š
   - å¦‚æœæœ‰å¤šä¸ªä¿¡æ¯æºï¼ˆçŸ¥è¯†åº“ã€ç½‘ç»œæœç´¢ï¼‰ï¼Œä¼˜å…ˆä½¿ç”¨æœ€ç›¸å…³çš„
   - ä¸è¦å¼ºåˆ¶ä½¿ç”¨ä¸ç›¸å…³çš„çŸ¥è¯†åº“å†…å®¹
   - å¦‚æœç½‘ç»œæœç´¢æ›´å‡†ç¡®ï¼Œä¼˜å…ˆä½¿ç”¨æœç´¢ç»“æœ
   - å¦‚æœä¿¡æ¯ä¸è¶³æˆ–ä¸ç›¸å…³ï¼Œè¯·è¯šå®è¯´æ˜

2. **å›ç­”æ–¹å¼**ï¼š
   - **ç®€å•å¯¹è¯é—®é¢˜**ï¼šç›´æ¥ã€ç®€æ´åœ°å›ç­”ï¼ˆ200-400å­—ï¼‰
   - **ä¿¡æ¯æŸ¥è¯¢**ï¼šæä¾›å‡†ç¡®ä¿¡æ¯ï¼Œåˆ—å‡ºå…³é”®ç‚¹ï¼ˆ400-800å­—ï¼‰
   - **ç ”ç©¶æŠ¥å‘Š**ï¼šä½¿ç”¨ Markdown æ ¼å¼ï¼Œç»“æ„åŒ–ç»„ç»‡ï¼ˆ800-1500å­—ï¼‰

3. **å†…å®¹è´¨é‡**ï¼š
   - å‡†ç¡®æ€§ç¬¬ä¸€ï¼šä¸ç¼–é€ ä¸ç¡®å®šçš„ä¿¡æ¯
   - ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸è¦è¿‡åº¦é“ºé™ˆ
   - ä½¿ç”¨æ¸…æ™°çš„ Markdown æ ¼å¼ï¼ˆæ ‡é¢˜ã€åˆ—è¡¨ã€å¼•ç”¨ï¼‰
   - è¯­è¨€è‡ªç„¶æµç•…ï¼Œé¿å…ç”Ÿç¡¬çš„æŠ¥å‘Šä½“

4. **ç‰¹æ®Šæƒ…å†µå¤„ç†**ï¼š
   - å¦‚æœçŸ¥è¯†åº“å†…å®¹ä¸é—®é¢˜æ— å…³ â†’ å¿½ç•¥çŸ¥è¯†åº“ï¼Œä½¿ç”¨å…¶ä»–ä¿¡æ¯æºæˆ–è‡ªèº«çŸ¥è¯†
   - å¦‚æœç½‘ç»œæœç´¢ç»“æœæ›´å‡†ç¡® â†’ ä¼˜å…ˆä½¿ç”¨æœç´¢ç»“æœ
   - å¦‚æœä¿¡æ¯ä¸è¶³ â†’ å¦è¯šè¯´æ˜ï¼Œç»™å‡ºå»ºè®®

5. **ç¦æ­¢è¡Œä¸º**ï¼š
   - âŒ ä¸è¦å¼ºåˆ¶å‡‘å­—æ•°æˆä¸ºå†—é•¿çš„æŠ¥å‘Š
   - âŒ ä¸è¦ä½¿ç”¨æ— å…³çš„çŸ¥è¯†åº“å†…å®¹
   - âŒ ä¸è¦ç¼–é€ æ•°æ®æˆ–å¼•ç”¨
   - âŒ ä¸è¦ä½¿ç”¨è¿‡äºæ­£å¼çš„æŠ¥å‘Šæ¨¡æ¿ï¼ˆé™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚æŠ¥å‘Šï¼‰

{"ã€è¡¥å……ã€‘ï¼šå·²æœ‰æ·±åº¦åˆ†æç»“æœï¼Œè¯·å……åˆ†åˆ©ç”¨åˆ†æä¸“å®¶æä¾›çš„æ´å¯Ÿ" if has_deep_analysis else ""}

ç°åœ¨è¯·ç›´æ¥ã€å‡†ç¡®åœ°å›ç­”ç”¨æˆ·é—®é¢˜ï¼š
"""
        
        # ä»æ•°æ®åº“è·å–æ¿€æ´»çš„promptï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤prompt
        summarization_prompt = get_agent_prompt(
            agent_id="summarization_specialist",
            session=session,
            default_prompt=default_summarization_prompt,
            user_query=user_query,
            task_description=task_description,
            full_context=full_context if full_context else "ï¼ˆæœªæ£€ç´¢åˆ°ç‰¹å®šä¿¡æ¯ï¼‰",
            info_quality_note=info_quality_note,
            has_deep_analysis="å·²æœ‰æ·±åº¦åˆ†æç»“æœï¼Œè¯·å……åˆ†åˆ©ç”¨åˆ†æä¸“å®¶æä¾›çš„æ´å¯Ÿ" if has_deep_analysis else ""
        )
        
        final_answer, _ = await invoke_llm(
            messages=[{"role": "user", "content": summarization_prompt}],
            settings=settings,
            temperature=0.6,  # å¹³è¡¡åˆ›é€ æ€§å’Œå‡†ç¡®æ€§
            max_tokens=3000,  # å¢åŠ tokenä»¥æ”¯æŒæ›´é•¿çš„æŠ¥å‘Š
        )
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯æ¶ˆæ¯
        if final_answer.startswith("LLM è°ƒç”¨") or len(final_answer) < 50:
            logger.warning(f"âš ï¸ [æ€»ç»“ä¸“å®¶] LLM å“åº”å¼‚å¸¸: {final_answer}")
            # é™çº§ç­–ç•¥ï¼šç”Ÿæˆç®€å•æ€»ç»“
            fallback_answer = f"""# {user_query}

## æ‰§è¡Œæ‘˜è¦
æœ¬æ¬¡å¤šæ™ºèƒ½ä½“åä½œå®Œæˆäº†ä»¥ä¸‹å·¥ä½œï¼š

### æ£€ç´¢ç»“æœ
{"âœ… å·²å®ŒæˆçŸ¥è¯†åº“æ£€ç´¢å’Œç½‘ç»œæœç´¢" if retrieval_results else "âš ï¸ æ£€ç´¢ä¿¡æ¯æœ‰é™"}

### åˆ†æç»“æœ
{"âœ… å·²å®Œæˆæ·±åº¦åˆ†æ" if analysis_result else "âš ï¸ åˆ†æä¿¡æ¯æœ‰é™"}

## è¯´æ˜
ç”±äºLLMå“åº”è¶…æ—¶æˆ–å¼‚å¸¸ï¼Œç³»ç»Ÿç”Ÿæˆäº†ç®€åŒ–ç‰ˆæŠ¥å‘Šã€‚å»ºè®®ï¼š
1. é‡æ–°æäº¤é—®é¢˜
2. ç®€åŒ–é—®é¢˜æè¿°
3. æ£€æŸ¥ç½‘ç»œè¿æ¥

åŸå§‹é”™è¯¯ä¿¡æ¯ï¼š{final_answer}
"""
            final_answer = fallback_answer
            thoughts.append("LLMå“åº”å¼‚å¸¸ï¼Œä½¿ç”¨é™çº§ç­–ç•¥ç”Ÿæˆç®€åŒ–æŠ¥å‘Š")
        else:
            thoughts.append("ç”Ÿæˆäº†ç»¼åˆæ€»ç»“å›ç­”")
        
        observations.append(f"æ€»ç»“å®Œæˆï¼Œç”Ÿæˆå›ç­”é•¿åº¦ï¼š{len(final_answer)} å­—ç¬¦")
        
        # 4. æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜æ–‡ä»¶ï¼ˆwrite_noteã€draw_diagramï¼‰
        if tool_records:
            # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦è¦æ±‚ä¿å­˜æ–‡ä»¶
            save_keywords = ["å†™å…¥", "ä¿å­˜", "å†™", "ç”Ÿæˆæ–‡ä»¶", "åˆ›å»ºæ–‡ä»¶", "ä¿å­˜åˆ°", "å†™å…¥ç¬”è®°", "ç»˜åˆ¶", "ç”»", "æ€ç»´å¯¼å›¾"]
            user_wants_save = any(keyword in user_query for keyword in save_keywords)
            
            if user_wants_save:
                logger.info("ğŸ“ æ£€æµ‹åˆ°ç”¨æˆ·è¦æ±‚ä¿å­˜æ–‡ä»¶ï¼Œå‡†å¤‡è°ƒç”¨æ–‡ä»¶ä¿å­˜å·¥å…·...")
                
                # æ„å»ºå¯ç”¨å·¥å…·æ˜ å°„
                from .tool_service import BUILTIN_TOOLS
                available_tools_map = {}
                for tool in tool_records:
                    try:
                        config = json.loads(tool.config or "{}")
                        builtin_key = config.get("builtin_key")
                        if builtin_key and builtin_key in BUILTIN_TOOLS:
                            available_tools_map[builtin_key] = tool
                    except:
                        continue
                
                # ä»ç”¨æˆ·æŸ¥è¯¢ä¸­æå–æ–‡ä»¶åï¼ˆå¦‚æœæŒ‡å®šäº†ï¼‰
                specified_filename = None
                if "åå­—ä¸º" in user_query or "åä¸º" in user_query:
                    import re
                    match = re.search(r'(?:åå­—ä¸º|åä¸º)\s*([^\sï¼Œã€‚]+\.md)', user_query)
                    if match:
                        specified_filename = match.group(1)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæ€ç»´å¯¼å›¾
                needs_diagram = "draw_diagram" in available_tools_map and any(kw in user_query for kw in ["æ€ç»´å¯¼å›¾", "ç»˜åˆ¶", "ç”»å›¾", "å›¾è¡¨"])
                diagram_code = None
                
                if needs_diagram:
                    try:
                        # ç”Ÿæˆæ€ç»´å¯¼å›¾ä»£ç 
                        logger.info(f"ğŸ¨ ä¸ºæ€ç»´å¯¼å›¾ç”ŸæˆMermaidä»£ç ...")
                        diagram_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹æŠ¥å‘Šå†…å®¹ç”ŸæˆMermaidæ€ç»´å¯¼å›¾ä»£ç ï¼š

ã€æŠ¥å‘Šå†…å®¹ã€‘ï¼š
{final_answer[:3000]}

ã€ä»»åŠ¡ã€‘ï¼šæå–æŠ¥å‘Šçš„æ ¸å¿ƒä¸»é¢˜å’Œå…³é”®æ¦‚å¿µï¼Œç”Ÿæˆä¸€ä¸ªç»“æ„æ¸…æ™°çš„Mermaidæ€ç»´å¯¼å›¾ï¼ˆgraph TDæ ¼å¼ï¼‰ã€‚
- æå–æ ¸å¿ƒä¸»é¢˜å’Œå…³é”®æ¦‚å¿µ
- æ„å»ºåˆç†çš„å±‚çº§ç»“æ„
- çªå‡ºé‡è¦çš„å…³è”å…³ç³»

ã€è¯­æ³•è¦æ±‚ã€‘ï¼š
- ä½¿ç”¨æ ‡å‡†çš„ graph TD æ ¼å¼
- èŠ‚ç‚¹æ ¼å¼ï¼šA[æ–‡æœ¬å†…å®¹]
- è¿æ¥æ ¼å¼ï¼šA --> B
- ä¸è¦ä½¿ç”¨ ::iconã€:::classã€classDef ç­‰é«˜çº§è¯­æ³•
- ä¸è¦ä½¿ç”¨ subgraphï¼ˆå­å›¾ï¼‰
- ä¿æŒè¯­æ³•ç®€æ´æ ‡å‡†

è¯·ç›´æ¥è¾“å‡ºå®Œæ•´çš„Mermaidä»£ç ï¼Œä¸è¦æœ‰markdownä»£ç å—æ ‡è®°ï¼ˆä¸è¦```mermaidï¼‰ï¼Œä¸è¦æœ‰ä»»ä½•å‰ç¼€è¯´æ˜ã€‚
ç¤ºä¾‹æ ¼å¼ï¼š
graph TD
    A[ä¸»é¢˜] --> B[å­æ¦‚å¿µ1]
    A --> C[å­æ¦‚å¿µ2]
    B --> B1[ç»†èŠ‚]
"""
                        
                        diagram_code, _ = await invoke_llm(
                            messages=[{"role": "user", "content": diagram_prompt}],
                            settings=settings,
                            temperature=0.7,
                            max_tokens=2000,
                        )
                        # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                        diagram_code = diagram_code.strip()
                        if diagram_code.startswith("```"):
                            diagram_code = diagram_code.split("```", 2)[1]
                            if diagram_code.startswith("mermaid"):
                                diagram_code = diagram_code[7:]
                            diagram_code = diagram_code.strip()
                        if diagram_code.endswith("```"):
                            diagram_code = diagram_code[:-3].strip()
                        
                        # æ¸…ç†ä¸æ”¯æŒçš„Mermaidè¯­æ³•
                        import re
                        # ç§»é™¤ ::icon(...) è¯­æ³•
                        diagram_code = re.sub(r'\s*::icon\([^)]*\)', '', diagram_code)
                        # ç§»é™¤ :::className è¯­æ³•
                        diagram_code = re.sub(r'\s*:::[^\s\n]+', '', diagram_code)
                        # ç§»é™¤ classDef å®šä¹‰
                        diagram_code = re.sub(r'classDef\s+\w+\s+[^\n]+\n?', '', diagram_code)
                        # ç§»é™¤ class èµ‹å€¼
                        diagram_code = re.sub(r'class\s+[\w,]+\s+\w+\s*\n?', '', diagram_code)
                        
                        logger.info(f"âœ… å·²ç”Ÿæˆæ€ç»´å¯¼å›¾ä»£ç ï¼Œé•¿åº¦ï¼š{len(diagram_code)} å­—ç¬¦")
                    except Exception as e:
                        logger.error(f"âŒ ç”Ÿæˆæ€ç»´å¯¼å›¾ä»£ç å¤±è´¥: {e}")
                        thoughts.append(f"âš ï¸ æ€ç»´å¯¼å›¾ç”Ÿæˆå¤±è´¥: {str(e)}")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦write_note
                if "write_note" in available_tools_map and any(kw in user_query for kw in ["ç¬”è®°", "æŠ¥å‘Š", "æ€»ç»“", "å†™å…¥", "ä¿å­˜"]):
                    try:
                        # æ„å»ºæ–‡ä»¶å†…å®¹ï¼šæŠ¥å‘Š + æ€ç»´å¯¼å›¾ï¼ˆå¦‚æœç”Ÿæˆï¼‰
                        file_content = final_answer
                        
                        if diagram_code:
                            # å¦‚æœç”¨æˆ·æŒ‡å®šäº†åŒä¸€ä¸ªæ–‡ä»¶åï¼Œå°†æ€ç»´å¯¼å›¾è¿½åŠ åˆ°æŠ¥å‘Šä¸­
                            if specified_filename:
                                file_content += f"\n\n---\n\n# æ€ç»´å¯¼å›¾\n\n```mermaid\n{diagram_code}\n```\n"
                                thoughts.append("âœ… å·²å°†æŠ¥å‘Šå’Œæ€ç»´å¯¼å›¾åˆå¹¶å†™å…¥åŒä¸€æ–‡ä»¶")
                            else:
                                # å¦‚æœæ²¡æŒ‡å®šæ–‡ä»¶åï¼Œæ€ç»´å¯¼å›¾ä¼šå•ç‹¬ä¿å­˜
                                pass
                        
                        # ç¡®å®šæ–‡ä»¶å
                        filename = specified_filename if specified_filename else f"note_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
                        
                        logger.info(f"ğŸ“ è°ƒç”¨write_noteå·¥å…·ï¼Œæ–‡ä»¶åï¼š{filename}")
                        result = execute_tool(
                            tool=available_tools_map["write_note"],
                            arguments={"filename": filename, "content": file_content},
                            settings=settings,
                            session=session,
                        )
                        thoughts.append(f"âœ… å·²ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶ï¼š{filename}")
                        observations.append(f"æŠ¥å‘Šå·²ä¿å­˜åˆ° {filename}")
                        logger.info(f"âœ… æ–‡ä»¶ä¿å­˜æˆåŠŸï¼š{filename}")
                    except Exception as e:
                        logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
                        thoughts.append(f"âš ï¸ æ–‡ä»¶ä¿å­˜å¤±è´¥: {str(e)}")
                
                # å¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šæ–‡ä»¶åï¼Œä¸”éœ€è¦å•ç‹¬ä¿å­˜æ€ç»´å¯¼å›¾
                if needs_diagram and diagram_code and not specified_filename:
                    try:
                        filename = f"diagram_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
                        logger.info(f"ğŸ¨ è°ƒç”¨draw_diagramå·¥å…·ï¼Œæ–‡ä»¶åï¼š{filename}")
                        result = execute_tool(
                            tool=available_tools_map["draw_diagram"],
                            arguments={"filename": filename, "diagram_code": diagram_code},
                            settings=settings,
                            session=session,
                        )
                        thoughts.append(f"âœ… å·²ä¿å­˜æ€ç»´å¯¼å›¾åˆ°æ–‡ä»¶ï¼š{filename}")
                        observations.append(f"æ€ç»´å¯¼å›¾å·²ä¿å­˜åˆ° {filename}")
                        logger.info(f"âœ… æ€ç»´å¯¼å›¾ä¿å­˜æˆåŠŸï¼š{filename}")
                    except Exception as e:
                        logger.error(f"âŒ ä¿å­˜æ€ç»´å¯¼å›¾å¤±è´¥: {e}")
                        thoughts.append(f"âš ï¸ æ€ç»´å¯¼å›¾ä¿å­˜å¤±è´¥: {str(e)}")
        
        # 5. å­˜å‚¨ç»“æœ
        workspace.store_agent_result(agent_id, {"final_answer": final_answer})
        workspace.set_shared_data("final_answer", final_answer)
        
        # 6. å‘é€ç»“æœæ¶ˆæ¯
        workspace.send_message(
            from_agent=agent_id,
            to_agent="orchestrator",
            message_type="result",
            content={
                "status": "completed",
                "final_answer": final_answer,
            },
        )
        
        workspace.update_agent_status(agent_id, "completed")
        
        logger.info("âœ… [æ€»ç»“ä¸“å®¶] æ€»ç»“å®Œæˆ")
        
        return {
            "agent_thoughts": {agent_id: thoughts},
            "agent_observations": {agent_id: observations},
            "final_answer": final_answer,
        }
    
    except Exception as e:
        logger.error(f"âŒ [æ€»ç»“ä¸“å®¶] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        workspace.update_agent_status(agent_id, "failed")
        workspace.send_message(
            from_agent=agent_id,
            to_agent="orchestrator",
            message_type="error",
            content={"error": str(e)},
        )
        
        return {
            "agent_thoughts": {agent_id: [f"æ‰§è¡Œå¤±è´¥: {str(e)}"]},
            "error": str(e),
        }


# ==================== éªŒè¯ä¸“å®¶ï¼ˆVerification Specialistï¼‰ ====================

async def verification_specialist_node(
    state: MultiAgentState,
    settings: Settings,
    session: Session,
) -> Dict[str, Any]:
    """
    éªŒè¯ä¸“å®¶æ™ºèƒ½ä½“ï¼ˆå¯é€‰ï¼‰
    
    èŒè´£ï¼š
    - è´¨é‡æ£€æŸ¥
    - äº‹å®æ ¸æŸ¥
    - ä¸€è‡´æ€§éªŒè¯
    
    èƒ½åŠ›ï¼š
    - ä¿¡æ¯éªŒè¯
    - è´¨é‡è¯„ä¼°
    """
    logger.info("âœ”ï¸ [éªŒè¯ä¸“å®¶] å¼€å§‹æ‰§è¡ŒéªŒè¯ä»»åŠ¡...")
    
    workspace = SharedWorkspace(state)
    agent_id = "verification_specialist"
    
    workspace.register_agent(agent_id)
    workspace.update_agent_status(agent_id, "running")
    
    thoughts = []
    observations = []
    
    try:
        # 1. è·å–æœ€ç»ˆç­”æ¡ˆ
        final_answer = workspace.get_shared_data("final_answer", "")
        
        if not final_answer:
            thoughts.append("æœªæ‰¾åˆ°æœ€ç»ˆç­”æ¡ˆï¼Œè·³è¿‡éªŒè¯")
            workspace.update_agent_status(agent_id, "skipped")
            return {
                "agent_thoughts": {agent_id: thoughts},
            }
        
        # 2. ä½¿ç”¨ LLM è¿›è¡Œè´¨é‡è¯„ä¼°
        logger.info("ğŸ” ä½¿ç”¨ LLM è¿›è¡Œè´¨é‡éªŒè¯...")
        
        # é»˜è®¤promptï¼ˆå¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æ¿€æ´»çš„æ¨¡æ¿ï¼Œä½¿ç”¨è¿™ä¸ªï¼‰
        default_verification_prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹å›ç­”çš„è´¨é‡ï¼š

å›ç­”å†…å®¹ï¼š
{final_answer}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°ï¼ˆ0-10åˆ†ï¼‰ï¼š
1. å‡†ç¡®æ€§ï¼šä¿¡æ¯æ˜¯å¦å‡†ç¡®å¯é 
2. å®Œæ•´æ€§ï¼šæ˜¯å¦å…¨é¢å›ç­”äº†é—®é¢˜
3. æ¸…æ™°åº¦ï¼šè¡¨è¾¾æ˜¯å¦æ¸…æ™°æ˜“æ‡‚
4. ç›¸å…³æ€§ï¼šæ˜¯å¦ä¸é—®é¢˜ç›¸å…³

ä»¥ JSON æ ¼å¼è¾“å‡ºè¯„ä¼°ç»“æœï¼š
{{
  "accuracy_score": 0-10,
  "completeness_score": 0-10,
  "clarity_score": 0-10,
  "relevance_score": 0-10,
  "overall_score": 0-10,
  "issues": ["é—®é¢˜1", "é—®é¢˜2", ...],
  "suggestions": ["å»ºè®®1", "å»ºè®®2", ...],
  "verdict": "é€šè¿‡" æˆ– "éœ€è¦æ”¹è¿›"
}}

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
"""
        
        # ä»æ•°æ®åº“è·å–æ¿€æ´»çš„promptï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤prompt
        verification_prompt = get_agent_prompt(
            agent_id="verification_specialist",
            session=session,
            default_prompt=default_verification_prompt,
            final_answer=final_answer
        )
        
        llm_response, _ = await invoke_llm(
            messages=[{"role": "user", "content": verification_prompt}],
            settings=settings,
            temperature=0.2,
            max_tokens=800,
        )
        
        verification_result = parse_json_from_llm(llm_response)
        
        overall_score = verification_result.get("overall_score", 7)
        verdict = verification_result.get("verdict", "é€šè¿‡")
        
        thoughts.append(f"å®Œæˆè´¨é‡éªŒè¯ï¼Œæ€»åˆ†ï¼š{overall_score}/10")
        observations.append(f"éªŒè¯ç»“æœï¼š{verdict}ï¼Œæ€»åˆ† {overall_score}/10")
        
        # 3. å­˜å‚¨ç»“æœ
        workspace.store_agent_result(agent_id, verification_result)
        workspace.set_shared_data("verification_result", verification_result)
        
        # 4. å‘é€ç»“æœæ¶ˆæ¯
        workspace.send_message(
            from_agent=agent_id,
            to_agent="orchestrator",
            message_type="result",
            content={
                "status": "completed",
                "verification_result": verification_result,
            },
        )
        
        workspace.update_agent_status(agent_id, "completed")
        
        logger.info(f"âœ… [éªŒè¯ä¸“å®¶] éªŒè¯å®Œæˆï¼Œç»“æœï¼š{verdict}")
        
        return {
            "agent_thoughts": {agent_id: thoughts},
            "agent_observations": {agent_id: observations},
            "quality_score": overall_score / 10.0,
        }
    
    except Exception as e:
        logger.error(f"âŒ [éªŒè¯ä¸“å®¶] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        workspace.update_agent_status(agent_id, "failed")
        workspace.send_message(
            from_agent=agent_id,
            to_agent="orchestrator",
            message_type="error",
            content={"error": str(e)},
        )
        
        return {
            "agent_thoughts": {agent_id: [f"æ‰§è¡Œå¤±è´¥: {str(e)}"]},
            "error": str(e),
        }


# ==================== æ™ºèƒ½ä½“æ³¨å†Œè¡¨ ====================

AGENT_REGISTRY = {
    "retrieval_specialist": {
        "name": "æ£€ç´¢ä¸“å®¶",
        "description": "è´Ÿè´£çŸ¥è¯†åº“æ£€ç´¢å’Œç½‘ç»œæœç´¢",
        "node_function": retrieval_specialist_node,
        "capabilities": ["knowledge_base_retrieval", "web_search", "document_analysis"],
    },
    "analysis_specialist": {
        "name": "åˆ†æä¸“å®¶",
        "description": "è´Ÿè´£æ•°æ®åˆ†æå’Œå†…å®¹ç†è§£",
        "node_function": analysis_specialist_node,
        "capabilities": ["text_analysis", "data_extraction", "pattern_recognition"],
    },
    "summarization_specialist": {
        "name": "æ€»ç»“ä¸“å®¶",
        "description": "è´Ÿè´£ä¿¡æ¯æ•´åˆå’ŒæŠ¥å‘Šç”Ÿæˆ",
        "node_function": summarization_specialist_node,
        "capabilities": ["content_summarization", "report_generation", "format_conversion"],
    },
    "verification_specialist": {
        "name": "éªŒè¯ä¸“å®¶",
        "description": "è´Ÿè´£è´¨é‡æ£€æŸ¥å’Œäº‹å®æ ¸æŸ¥ï¼ˆå¯é€‰ï¼‰",
        "node_function": verification_specialist_node,
        "capabilities": ["quality_check", "fact_verification", "consistency_validation"],
    },
}


def get_agent_by_id(agent_id: str) -> Optional[Dict[str, Any]]:
    """æ ¹æ®IDè·å–æ™ºèƒ½ä½“ä¿¡æ¯"""
    return AGENT_REGISTRY.get(agent_id)


def list_available_agents() -> List[Dict[str, Any]]:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ™ºèƒ½ä½“"""
    return [
        {
            "id": agent_id,
            "name": info["name"],
            "description": info["description"],
            "capabilities": info["capabilities"],
        }
        for agent_id, info in AGENT_REGISTRY.items()
    ]


def get_default_prompts() -> List[Dict[str, Any]]:
    """è·å–æ‰€æœ‰é»˜è®¤çš„promptæ¨¡æ¿ï¼ˆç¡¬ç¼–ç çš„åŸå§‹promptï¼‰"""
    return [
        {
            "agent_id": "retrieval_specialist",
            "name": "æ£€ç´¢ä¸“å®¶-é»˜è®¤æ¨¡æ¿",
            "description": "ç³»ç»Ÿé»˜è®¤çš„æ£€ç´¢ä¸“å®¶è¯´æ˜æ¨¡æ¿ï¼Œä½œä¸ºç¤ºä¾‹å‚è€ƒï¼ˆæ£€ç´¢ä¸“å®¶ä¸»è¦æ‰§è¡Œæ£€ç´¢æ“ä½œï¼Œä¸ç›´æ¥ä½¿ç”¨LLMï¼‰",
            "content": """æ£€ç´¢ä¸“å®¶æ™ºèƒ½ä½“èŒè´£è¯´æ˜ï¼š

ã€æ™ºèƒ½ä½“è§’è‰²ã€‘ï¼šæ£€ç´¢ä¸“å®¶ï¼ˆRetrieval Specialistï¼‰

ã€ä¸»è¦èŒè´£ã€‘ï¼š
1. çŸ¥è¯†åº“æ£€ç´¢ï¼ˆRAGï¼‰
   - ä½¿ç”¨å‘é‡æ£€ç´¢ä»çŸ¥è¯†åº“ä¸­æŸ¥æ‰¾ç›¸å…³å†…å®¹
   - æ”¯æŒè¯­ä¹‰æœç´¢å’Œå…³é”®è¯åŒ¹é…
   - è¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µ

2. ç½‘ç»œæœç´¢
   - å½“ç”¨æˆ·æŸ¥è¯¢åŒ…å«æœç´¢å…³é”®è¯æ—¶ï¼Œæ‰§è¡Œç½‘ç»œæœç´¢
   - è·å–æœ€æ–°çš„ç½‘ç»œä¿¡æ¯
   - æ•´åˆæœç´¢ç»“æœ

3. æ–‡æ¡£æŸ¥æ‰¾
   - åˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œç¡®å®šéœ€è¦æ£€ç´¢çš„æ–‡æ¡£ç±»å‹
   - æ‰§è¡Œç›¸åº”çš„æ£€ç´¢ç­–ç•¥

ã€å·¥ä½œæµç¨‹ã€‘ï¼š
1. æ¥æ”¶ç”¨æˆ·æŸ¥è¯¢ï¼š{user_query}
2. åˆ¤æ–­æ˜¯å¦éœ€è¦çŸ¥è¯†åº“æ£€ç´¢ï¼ˆæ ¹æ®use_knowledge_baseæ ‡å¿—ï¼‰
3. åˆ¤æ–­æ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢ï¼ˆæ ¹æ®æŸ¥è¯¢å…³é”®è¯ï¼‰
4. æ‰§è¡Œç›¸åº”çš„æ£€ç´¢æ“ä½œ
5. æ•´ç†æ£€ç´¢ç»“æœå¹¶è¿”å›ç»™åè°ƒå™¨

ã€è¾“å‡ºæ ¼å¼ã€‘ï¼š
æ£€ç´¢ç»“æœä»¥ç»“æ„åŒ–æ ¼å¼è¿”å›ï¼ŒåŒ…æ‹¬ï¼š
- knowledge_base: çŸ¥è¯†åº“æ£€ç´¢ç»“æœåˆ—è¡¨
- web_search: ç½‘ç»œæœç´¢ç»“æœï¼ˆå¦‚æœæ‰§è¡Œäº†æœç´¢ï¼‰

ã€æ³¨æ„äº‹é¡¹ã€‘ï¼š
- æ£€ç´¢ä¸“å®¶ä¸»è¦è´Ÿè´£ä¿¡æ¯æ£€ç´¢ï¼Œä¸è¿›è¡Œå†…å®¹åˆ†æ
- æ£€ç´¢ç»“æœä¼šä¼ é€’ç»™åˆ†æä¸“å®¶è¿›è¡Œè¿›ä¸€æ­¥å¤„ç†
- ç¡®ä¿æ£€ç´¢ç»“æœçš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§"""
        },
        {
            "agent_id": "analysis_specialist",
            "name": "åˆ†æä¸“å®¶-é»˜è®¤æ¨¡æ¿",
            "description": "ç³»ç»Ÿé»˜è®¤çš„åˆ†æä¸“å®¶promptï¼Œä½œä¸ºç¤ºä¾‹å‚è€ƒ",
            "content": """ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„æŠ€æœ¯åˆ†æä¸“å®¶å’Œç ”ç©¶é¡¾é—®ã€‚è¯·å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œæ·±åº¦ã€ç³»ç»ŸåŒ–çš„åˆ†æã€‚

ã€ä»»åŠ¡è¦æ±‚ã€‘ï¼š{task_description}

ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š{user_query}

ã€å¾…åˆ†æå†…å®¹ã€‘ï¼š
{analysis_context}

ã€åˆ†æç»´åº¦ã€‘è¯·ä»ä»¥ä¸‹å¤šä¸ªç»´åº¦è¿›è¡Œæ·±å…¥åˆ†æï¼š

1. **æ ¸å¿ƒæ¦‚å¿µè¯†åˆ«**ï¼š
   - è¯†åˆ«å¹¶è§£é‡Šæ ¸å¿ƒæŠ€æœ¯æ¦‚å¿µã€æœ¯è¯­
   - åŒºåˆ†åŸºç¡€æ¦‚å¿µä¸é«˜çº§æ¦‚å¿µ

2. **å…³é”®ä¿¡æ¯æå–**ï¼š
   - æå–é‡è¦äº‹å®ã€æ•°æ®ã€ç»Ÿè®¡ä¿¡æ¯
   - è¯†åˆ«å…³é”®è®ºç‚¹å’Œç»“è®º
   - æ ‡æ³¨ä¿¡æ¯æ¥æºï¼ˆå¦‚æœ‰ï¼‰

3. **æŠ€æœ¯åŸç†åˆ†æ**ï¼ˆå¦‚é€‚ç”¨ï¼‰ï¼š
   - è§£é‡ŠæŠ€æœ¯å®ç°åŸç†
   - åˆ†ææŠ€æœ¯æ¶æ„å’Œè®¾è®¡æ€è·¯
   - å¯¹æ¯”ä¸åŒæŠ€æœ¯æ–¹æ¡ˆçš„ä¼˜åŠ£

4. **å…³è”æ€§åˆ†æ**ï¼š
   - å‘ç°æ¦‚å¿µä¹‹é—´çš„é€»è¾‘å…³ç³»
   - è¯†åˆ«å› æœå…³ç³»ã€æ¼”è¿›å…³ç³»
   - æ„å»ºçŸ¥è¯†å›¾è°±å¼çš„å…³è”

5. **è¶‹åŠ¿ä¸æ´å¯Ÿ**ï¼š
   - è¯†åˆ«æŠ€æœ¯æ¼”è¿›è¶‹åŠ¿
   - å‘ç°æ½œåœ¨é—®é¢˜å’ŒæŒ‘æˆ˜
   - é¢„æµ‹æœªæ¥å‘å±•æ–¹å‘

6. **æ‰¹åˆ¤æ€§æ€è€ƒ**ï¼š
   - æŒ‡å‡ºä¿¡æ¯çš„å±€é™æ€§
   - è¯†åˆ«å¯èƒ½å­˜åœ¨çš„åè§æˆ–äº‰è®®
   - æå‡ºéœ€è¦è¿›ä¸€æ­¥éªŒè¯çš„ç‚¹

ä»¥ JSON æ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼š
{{
  "core_concepts": [
    {{"concept": "æ¦‚å¿µåç§°", "explanation": "è¯¦ç»†è§£é‡Š", "importance": "high|medium|low"}}
  ],
  "key_facts": [
    {{"fact": "äº‹å®æè¿°", "source": "æ¥æºï¼ˆå¦‚æœ‰ï¼‰", "confidence": "high|medium|low"}}
  ],
  "key_data": [
    {{"data_point": "æ•°æ®ç‚¹", "value": "å…·ä½“æ•°å€¼æˆ–æè¿°", "context": "èƒŒæ™¯è¯´æ˜"}}
  ],
  "technical_principles": [
    {{"principle": "åŸç†åç§°", "explanation": "åŸç†è§£é‡Š", "advantages": ["ä¼˜åŠ¿1"], "limitations": ["å±€é™1"]}}
  ],
  "relationships": [
    {{"from": "æ¦‚å¿µA", "to": "æ¦‚å¿µB", "relationship_type": "å› æœ|æ¼”è¿›|å¯¹æ¯”|è¡¥å……", "description": "å…³ç³»æè¿°"}}
  ],
  "trends_insights": [
    {{"trend": "è¶‹åŠ¿æè¿°", "evidence": "æ”¯æŒè¯æ®", "implications": "å½±å“åˆ†æ"}}
  ],
  "critical_notes": [
    {{"note_type": "å±€é™æ€§|äº‰è®®ç‚¹|å¾…éªŒè¯", "description": "è¯¦ç»†è¯´æ˜"}}
  ],
  "analysis_summary": "å…¨é¢çš„åˆ†ææ€»ç»“ï¼ˆ300-500å­—ï¼‰",
  "confidence_score": 0.0-1.0
}}

è¦æ±‚ï¼š
- åˆ†æè¦æ·±å…¥ã€ç³»ç»Ÿã€å…¨é¢
- ä¿æŒå®¢è§‚ï¼Œé¿å…ä¸»è§‚è‡†æ–­
- ä¼˜å…ˆä½¿ç”¨æä¾›çš„å†…å®¹ï¼Œæ ‡æ³¨æ¨ç†éƒ¨åˆ†
- é•¿åº¦ï¼š500-1000å­—çš„æ·±åº¦åˆ†æ

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""
        },
        {
            "agent_id": "summarization_specialist",
            "name": "æ€»ç»“ä¸“å®¶-é»˜è®¤æ¨¡æ¿",
            "description": "ç³»ç»Ÿé»˜è®¤çš„æ€»ç»“ä¸“å®¶promptï¼Œä½œä¸ºç¤ºä¾‹å‚è€ƒ",
            "content": """ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ç ”ç©¶æŠ¥å‘Šæ’°å†™ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·ç”Ÿæˆä¸€ä»½é«˜è´¨é‡ã€ç»“æ„åŒ–çš„ç ”ç©¶æŠ¥å‘Šæˆ–ç­”æ¡ˆã€‚

ã€ä»»åŠ¡è¦æ±‚ã€‘ï¼š{task_description}

ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š{user_query}

ã€æ”¶é›†åˆ°çš„ä¿¡æ¯ã€‘ï¼š
{full_context}

ã€æŠ¥å‘Šæ’°å†™è¦æ±‚ã€‘ï¼š

1. **ç»“æ„åŒ–ç»„ç»‡**ï¼š
   - ä½¿ç”¨æ¸…æ™°çš„ Markdown æ ¼å¼
   - åˆç†çš„æ ‡é¢˜å±‚çº§ï¼ˆ# ## ### ï¼‰
   - å¦‚æœæ˜¯ç ”ç©¶æŠ¥å‘Šï¼ŒåŒ…å«ï¼šå¼•è¨€ã€ä¸»è¦å†…å®¹ã€ç»“è®º
   - å¦‚æœæ˜¯æŠ€æœ¯åˆ†æï¼ŒåŒ…å«ï¼šæ¦‚è¿°ã€æŠ€æœ¯åŸç†ã€åº”ç”¨æ¡ˆä¾‹ã€è¶‹åŠ¿åˆ†æ

2. **å†…å®¹æ·±åº¦**ï¼š
   - ä¸è¦åªæ˜¯ç½—åˆ—ä¿¡æ¯ï¼Œè¦è¿›è¡Œæ·±åº¦æ•´åˆå’Œæç‚¼
   - å»ºç«‹ä¸åŒä¿¡æ¯ç‚¹ä¹‹é—´çš„é€»è¾‘è”ç³»
   - æä¾›æ¸…æ™°çš„è®ºè¯å’Œæ¨ç†è¿‡ç¨‹
   - çªå‡ºå…³é”®å‘ç°å’Œæ ¸å¿ƒæ´å¯Ÿ

3. **è¡¨è¾¾è´¨é‡**ï¼š
   - è¯­è¨€æµç•…ã€ä¸“ä¸šã€å‡†ç¡®
   - é¿å…é‡å¤å’Œå†—ä½™
   - ä½¿ç”¨å…·ä½“çš„æ•°æ®å’Œæ¡ˆä¾‹æ”¯æ’‘è®ºç‚¹
   - é€‚å½“ä½¿ç”¨åˆ—è¡¨ã€è¡¨æ ¼ç­‰å½¢å¼

4. **ä¿¡æ¯æ¥æº**ï¼š
   - ä¼˜å…ˆä½¿ç”¨æä¾›çš„æ£€ç´¢ç»“æœå’Œåˆ†æç»“æœ
   - å¦‚æœå¼•ç”¨å…·ä½“æ•°æ®æˆ–è§‚ç‚¹ï¼Œå¯æ³¨æ˜æ¥æº
   - åŒºåˆ†äº‹å®é™ˆè¿°å’Œæ¨ç†ç»“è®º

5. **å®Œæ•´æ€§**ï¼š
   - å…¨é¢å›ç­”ç”¨æˆ·æå‡ºçš„æ‰€æœ‰é—®é¢˜ç‚¹
   - ä¸é—æ¼å…³é”®ä¿¡æ¯
   - å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®æŒ‡å‡º

6. **é•¿åº¦è¦æ±‚**ï¼š
   - ç®€å•é—®é¢˜ï¼š300-600å­—
   - ä¸­ç­‰å¤æ‚åº¦ï¼š600-1200å­—
   - å¤æ‚ç ”ç©¶æŠ¥å‘Šï¼š1200-2000å­—

ã€ç‰¹åˆ«æ³¨æ„ã€‘ï¼š
- è¿™æ˜¯å¤šæ™ºèƒ½ä½“åä½œçš„æœ€ç»ˆè¾“å‡ºï¼Œè¦ä½“ç°é«˜è´¨é‡
- æ•´åˆæ‰€æœ‰å‰åºæ™ºèƒ½ä½“çš„å·¥ä½œæˆæœ
- ç¡®ä¿æŠ¥å‘Šçš„ä¸“ä¸šæ€§å’Œå¯è¯»æ€§
{has_deep_analysis}

ç°åœ¨è¯·ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼š"""
        },
        {
            "agent_id": "verification_specialist",
            "name": "éªŒè¯ä¸“å®¶-é»˜è®¤æ¨¡æ¿",
            "description": "ç³»ç»Ÿé»˜è®¤çš„éªŒè¯ä¸“å®¶promptï¼Œä½œä¸ºç¤ºä¾‹å‚è€ƒ",
            "content": """è¯·è¯„ä¼°ä»¥ä¸‹å›ç­”çš„è´¨é‡ï¼š

å›ç­”å†…å®¹ï¼š
{final_answer}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°ï¼ˆ0-10åˆ†ï¼‰ï¼š
1. å‡†ç¡®æ€§ï¼šä¿¡æ¯æ˜¯å¦å‡†ç¡®å¯é 
2. å®Œæ•´æ€§ï¼šæ˜¯å¦å…¨é¢å›ç­”äº†é—®é¢˜
3. æ¸…æ™°åº¦ï¼šè¡¨è¾¾æ˜¯å¦æ¸…æ™°æ˜“æ‡‚
4. ç›¸å…³æ€§ï¼šæ˜¯å¦ä¸é—®é¢˜ç›¸å…³

ä»¥ JSON æ ¼å¼è¾“å‡ºè¯„ä¼°ç»“æœï¼š
{{
  "accuracy_score": 0-10,
  "completeness_score": 0-10,
  "clarity_score": 0-10,
  "relevance_score": 0-10,
  "overall_score": 0-10,
  "issues": ["é—®é¢˜1", "é—®é¢˜2", ...],
  "suggestions": ["å»ºè®®1", "å»ºè®®2", ...],
  "verdict": "é€šè¿‡" æˆ– "éœ€è¦æ”¹è¿›"
}}

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""
        },
    ]

