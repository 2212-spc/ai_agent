"""
æ™ºèƒ½ä½“è§’è‰²å®šä¹‰
å®šä¹‰å„ä¸ªä¸“å®¶æ™ºèƒ½ä½“çš„èŠ‚ç‚¹å‡½æ•°å’Œèƒ½åŠ›
"""
from __future__ import annotations

import logging
from typing import Any, Dict, List, Optional

from sqlalchemy.orm import Session

from .config import Settings
from .database import ToolRecord
from .graph_agent import invoke_llm, parse_json_from_llm
from .rag_service import retrieve_context
from .shared_workspace import MultiAgentState, SharedWorkspace
from .tool_service import execute_tool

logger = logging.getLogger(__name__)


# ==================== æ£€ç´¢ä¸“å®¶ï¼ˆRetrieval Specialistï¼‰ ====================

async def retrieval_specialist_node(
    state: MultiAgentState,
    settings: Settings,
    session: Session,
    tool_records: List[ToolRecord],
) -> Dict[str, Any]:
    """
    æ£€ç´¢ä¸“å®¶æ™ºèƒ½ä½“
    
    èŒè´£ï¼š
    - çŸ¥è¯†åº“æ£€ç´¢ï¼ˆRAGï¼‰
    - ç½‘ç»œæœç´¢
    - æ–‡æ¡£æŸ¥æ‰¾
    
    èƒ½åŠ›ï¼š
    - å‘é‡æ£€ç´¢
    - å…³é”®è¯æœç´¢
    - ç½‘é¡µæœç´¢å·¥å…·
    """
    logger.info("ğŸ” [æ£€ç´¢ä¸“å®¶] å¼€å§‹æ‰§è¡Œæ£€ç´¢ä»»åŠ¡...")
    
    workspace = SharedWorkspace(state)
    agent_id = "retrieval_specialist"
    
    # æ³¨å†Œæ™ºèƒ½ä½“
    workspace.register_agent(agent_id)
    workspace.update_agent_status(agent_id, "running")
    
    user_query = state.get("user_query", "")
    use_knowledge_base = state.get("use_knowledge_base", False)
    
    retrieval_results = {}
    thoughts = []
    observations = []
    
    try:
        # 1. çŸ¥è¯†åº“æ£€ç´¢
        if use_knowledge_base:
            try:
                logger.info("ğŸ“š æ‰§è¡ŒçŸ¥è¯†åº“æ£€ç´¢...")
                contexts = retrieve_context(
                    query=user_query,
                    settings=settings,
                    top_k=5,
                )
                
                if contexts:
                    retrieval_results["knowledge_base"] = [
                        {
                            "document_id": ctx.document_id,
                            "original_name": ctx.original_name,
                            "content": ctx.content[:500],
                        }
                        for ctx in contexts
                    ]
                    thoughts.append(f"ä»çŸ¥è¯†åº“æ£€ç´¢åˆ° {len(contexts)} ä¸ªç›¸å…³ç‰‡æ®µ")
                    observations.append(
                        f"çŸ¥è¯†åº“æ£€ç´¢å®Œæˆï¼šæ‰¾åˆ° {len(contexts)} ä¸ªæ–‡æ¡£ç‰‡æ®µ"
                    )
                else:
                    thoughts.append("çŸ¥è¯†åº“æ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
                    observations.append("çŸ¥è¯†åº“ä¸ºç©ºæˆ–æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
            
            except Exception as e:
                logger.error(f"çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {e}")
                thoughts.append(f"çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {str(e)}")
        
        # 2. ç½‘ç»œæœç´¢ï¼ˆå¦‚æœæœ‰æœç´¢å·¥å…·ï¼‰
        search_tool = None
        for tool in tool_records:
            try:
                import json
                config = json.loads(tool.config or "{}")
                if config.get("builtin_key") == "web_search":
                    search_tool = tool
                    break
            except:
                continue
        
        if search_tool and any(
            keyword in user_query.lower()
            for keyword in ["æœç´¢", "æŸ¥æ‰¾", "æœ€æ–°", "search", "find"]
        ):
            try:
                logger.info("ğŸŒ æ‰§è¡Œç½‘ç»œæœç´¢...")
                
                # æå–æœç´¢å…³é”®è¯
                from .graph_agent import extract_search_query
                search_query = extract_search_query(user_query)
                
                # æ‰§è¡Œæœç´¢
                search_result = execute_tool(
                    tool=search_tool,
                    arguments={"query": search_query, "num_results": 5},
                    settings=settings,
                    session=session,
                )
                
                retrieval_results["web_search"] = {
                    "query": search_query,
                    "results": search_result,
                }
                
                thoughts.append(f"æ‰§è¡Œäº†ç½‘ç»œæœç´¢ï¼š{search_query}")
                observations.append(f"ç½‘ç»œæœç´¢å®Œæˆï¼Œå…³é”®è¯ï¼š{search_query}")
                
            except Exception as e:
                logger.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {e}")
                thoughts.append(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
        
        # 3. å­˜å‚¨ç»“æœåˆ°å…±äº«å·¥ä½œç©ºé—´
        workspace.store_agent_result(agent_id, retrieval_results)
        workspace.set_shared_data("retrieval_results", retrieval_results)
        
        # 4. å‘é€ç»“æœæ¶ˆæ¯ç»™åè°ƒå™¨
        workspace.send_message(
            from_agent=agent_id,
            to_agent="orchestrator",
            message_type="result",
            content={
                "status": "completed",
                "retrieval_results": retrieval_results,
                "summary": f"æ£€ç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(retrieval_results)} ç±»ç»“æœ",
            },
        )
        
        workspace.update_agent_status(agent_id, "completed")
        
        logger.info(f"âœ… [æ£€ç´¢ä¸“å®¶] æ‰§è¡Œå®Œæˆï¼Œæ‰¾åˆ° {len(retrieval_results)} ç±»ç»“æœ")
        
        return {
            "agent_thoughts": {agent_id: thoughts},
            "agent_observations": {agent_id: observations},
            "retrieved_contexts": retrieval_results.get("knowledge_base", []),
        }
    
    except Exception as e:
        logger.error(f"âŒ [æ£€ç´¢ä¸“å®¶] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        workspace.update_agent_status(agent_id, "failed")
        workspace.send_message(
            from_agent=agent_id,
            to_agent="orchestrator",
            message_type="error",
            content={"error": str(e)},
        )
        
        return {
            "agent_thoughts": {agent_id: [f"æ‰§è¡Œå¤±è´¥: {str(e)}"]},
            "error": str(e),
        }


# ==================== åˆ†æä¸“å®¶ï¼ˆAnalysis Specialistï¼‰ ====================

async def analysis_specialist_node(
    state: MultiAgentState,
    settings: Settings,
) -> Dict[str, Any]:
    """
    åˆ†æä¸“å®¶æ™ºèƒ½ä½“
    
    èŒè´£ï¼š
    - æ•°æ®åˆ†æ
    - å†…å®¹ç†è§£
    - å…³é”®ä¿¡æ¯æå–
    
    èƒ½åŠ›ï¼š
    - æ–‡æœ¬åˆ†æï¼ˆä½¿ç”¨ LLMï¼‰
    - æ•°æ®æå–
    - æ¨¡å¼è¯†åˆ«
    """
    logger.info("ğŸ“Š [åˆ†æä¸“å®¶] å¼€å§‹æ‰§è¡Œåˆ†æä»»åŠ¡...")
    
    workspace = SharedWorkspace(state)
    agent_id = "analysis_specialist"
    
    workspace.register_agent(agent_id)
    workspace.update_agent_status(agent_id, "running")
    
    user_query = state.get("user_query", "")
    thoughts = []
    observations = []
    
    try:
        # 1. è·å–æ£€ç´¢ä¸“å®¶çš„ç»“æœ
        retrieval_results = workspace.get_shared_data("retrieval_results", {})
        
        if not retrieval_results:
            thoughts.append("æœªæ‰¾åˆ°æ£€ç´¢ç»“æœï¼Œä½¿ç”¨ç”¨æˆ·æŸ¥è¯¢è¿›è¡Œåˆ†æ")
            analysis_context = f"ç”¨æˆ·æŸ¥è¯¢ï¼š{user_query}"
        else:
            # æ„å»ºåˆ†æä¸Šä¸‹æ–‡
            context_parts = []
            
            if "knowledge_base" in retrieval_results:
                kb_contexts = retrieval_results["knowledge_base"]
                context_parts.append(
                    f"çŸ¥è¯†åº“å†…å®¹ï¼ˆ{len(kb_contexts)} ä¸ªç‰‡æ®µï¼‰:\n"
                    + "\n".join([
                        f"- {ctx.get('content', '')[:200]}"
                        for ctx in kb_contexts[:3]
                    ])
                )
            
            if "web_search" in retrieval_results:
                search_data = retrieval_results["web_search"]
                context_parts.append(
                    f"æœç´¢ç»“æœï¼ˆå…³é”®è¯: {search_data.get('query', '')}):\n"
                    f"{search_data.get('results', '')[:500]}"
                )
            
            analysis_context = "\n\n".join(context_parts)
            thoughts.append(f"è·å–åˆ°æ£€ç´¢ç»“æœï¼Œå‡†å¤‡åˆ†æ")
        
        # 2. ä½¿ç”¨ LLM è¿›è¡Œæ·±åº¦åˆ†æ
        logger.info("ğŸ¤” ä½¿ç”¨ LLM è¿›è¡Œå†…å®¹åˆ†æ...")
        
        # è·å–å½“å‰å­ä»»åŠ¡çš„æè¿°ï¼Œä»¥ä¾¿é’ˆå¯¹æ€§åˆ†æ
        current_subtask = workspace.get_current_subtask()
        task_description = current_subtask.description if current_subtask else "æ·±åº¦åˆ†æå†…å®¹"
        
        analysis_prompt = f"""ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„æŠ€æœ¯åˆ†æä¸“å®¶å’Œç ”ç©¶é¡¾é—®ã€‚è¯·å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œæ·±åº¦ã€ç³»ç»ŸåŒ–çš„åˆ†æã€‚

ã€ä»»åŠ¡è¦æ±‚ã€‘ï¼š{task_description}

ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š{user_query}

ã€å¾…åˆ†æå†…å®¹ã€‘ï¼š
{analysis_context}

ã€åˆ†æç»´åº¦ã€‘è¯·ä»ä»¥ä¸‹å¤šä¸ªç»´åº¦è¿›è¡Œæ·±å…¥åˆ†æï¼š

1. **æ ¸å¿ƒæ¦‚å¿µè¯†åˆ«**ï¼š
   - è¯†åˆ«å¹¶è§£é‡Šæ ¸å¿ƒæŠ€æœ¯æ¦‚å¿µã€æœ¯è¯­
   - åŒºåˆ†åŸºç¡€æ¦‚å¿µä¸é«˜çº§æ¦‚å¿µ

2. **å…³é”®ä¿¡æ¯æå–**ï¼š
   - æå–é‡è¦äº‹å®ã€æ•°æ®ã€ç»Ÿè®¡ä¿¡æ¯
   - è¯†åˆ«å…³é”®è®ºç‚¹å’Œç»“è®º
   - æ ‡æ³¨ä¿¡æ¯æ¥æºï¼ˆå¦‚æœ‰ï¼‰

3. **æŠ€æœ¯åŸç†åˆ†æ**ï¼ˆå¦‚é€‚ç”¨ï¼‰ï¼š
   - è§£é‡ŠæŠ€æœ¯å®ç°åŸç†
   - åˆ†ææŠ€æœ¯æ¶æ„å’Œè®¾è®¡æ€è·¯
   - å¯¹æ¯”ä¸åŒæŠ€æœ¯æ–¹æ¡ˆçš„ä¼˜åŠ£

4. **å…³è”æ€§åˆ†æ**ï¼š
   - å‘ç°æ¦‚å¿µä¹‹é—´çš„é€»è¾‘å…³ç³»
   - è¯†åˆ«å› æœå…³ç³»ã€æ¼”è¿›å…³ç³»
   - æ„å»ºçŸ¥è¯†å›¾è°±å¼çš„å…³è”

5. **è¶‹åŠ¿ä¸æ´å¯Ÿ**ï¼š
   - è¯†åˆ«æŠ€æœ¯æ¼”è¿›è¶‹åŠ¿
   - å‘ç°æ½œåœ¨é—®é¢˜å’ŒæŒ‘æˆ˜
   - é¢„æµ‹æœªæ¥å‘å±•æ–¹å‘

6. **æ‰¹åˆ¤æ€§æ€è€ƒ**ï¼š
   - æŒ‡å‡ºä¿¡æ¯çš„å±€é™æ€§
   - è¯†åˆ«å¯èƒ½å­˜åœ¨çš„åè§æˆ–äº‰è®®
   - æå‡ºéœ€è¦è¿›ä¸€æ­¥éªŒè¯çš„ç‚¹

ä»¥ JSON æ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼š
{{
  "core_concepts": [
    {{"concept": "æ¦‚å¿µåç§°", "explanation": "è¯¦ç»†è§£é‡Š", "importance": "high|medium|low"}}
  ],
  "key_facts": [
    {{"fact": "äº‹å®æè¿°", "source": "æ¥æºï¼ˆå¦‚æœ‰ï¼‰", "confidence": "high|medium|low"}}
  ],
  "key_data": [
    {{"data_point": "æ•°æ®ç‚¹", "value": "å…·ä½“æ•°å€¼æˆ–æè¿°", "context": "èƒŒæ™¯è¯´æ˜"}}
  ],
  "technical_principles": [
    {{"principle": "åŸç†åç§°", "explanation": "åŸç†è§£é‡Š", "advantages": ["ä¼˜åŠ¿1"], "limitations": ["å±€é™1"]}}
  ],
  "relationships": [
    {{"from": "æ¦‚å¿µA", "to": "æ¦‚å¿µB", "relationship_type": "å› æœ|æ¼”è¿›|å¯¹æ¯”|è¡¥å……", "description": "å…³ç³»æè¿°"}}
  ],
  "trends_insights": [
    {{"trend": "è¶‹åŠ¿æè¿°", "evidence": "æ”¯æŒè¯æ®", "implications": "å½±å“åˆ†æ"}}
  ],
  "critical_notes": [
    {{"note_type": "å±€é™æ€§|äº‰è®®ç‚¹|å¾…éªŒè¯", "description": "è¯¦ç»†è¯´æ˜"}}
  ],
  "analysis_summary": "å…¨é¢çš„åˆ†ææ€»ç»“ï¼ˆ300-500å­—ï¼‰",
  "confidence_score": 0.0-1.0
}}

è¦æ±‚ï¼š
- åˆ†æè¦æ·±å…¥ã€ç³»ç»Ÿã€å…¨é¢
- ä¿æŒå®¢è§‚ï¼Œé¿å…ä¸»è§‚è‡†æ–­
- ä¼˜å…ˆä½¿ç”¨æä¾›çš„å†…å®¹ï¼Œæ ‡æ³¨æ¨ç†éƒ¨åˆ†
- é•¿åº¦ï¼š500-1000å­—çš„æ·±åº¦åˆ†æ

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
"""
        
        llm_response, _ = await invoke_llm(
            messages=[{"role": "user", "content": analysis_prompt}],
            settings=settings,
            temperature=0.4,  # ç¨é«˜çš„æ¸©åº¦ä»¥è·å¾—æ›´æœ‰åˆ›æ„çš„æ´å¯Ÿ
            max_tokens=2500,  # å¢åŠ tokené™åˆ¶ä»¥æ”¯æŒæ›´æ·±å…¥çš„åˆ†æ
        )
        
        # è§£æ LLM å“åº”
        analysis_result = parse_json_from_llm(llm_response)
        
        thoughts.append("å®Œæˆå†…å®¹åˆ†æï¼Œæå–äº†å…³é”®ä¿¡æ¯")
        observations.append(
            f"åˆ†æå®Œæˆï¼šè¯†åˆ« {len(analysis_result.get('core_topics', []))} ä¸ªæ ¸å¿ƒä¸»é¢˜ï¼Œ"
            f"{len(analysis_result.get('key_facts', []))} ä¸ªå…³é”®äº‹å®"
        )
        
        # 3. å­˜å‚¨ç»“æœ
        workspace.store_agent_result(agent_id, analysis_result)
        workspace.set_shared_data("analysis_result", analysis_result)
        
        # 4. å‘é€ç»“æœæ¶ˆæ¯
        workspace.send_message(
            from_agent=agent_id,
            to_agent="orchestrator",
            message_type="result",
            content={
                "status": "completed",
                "analysis_result": analysis_result,
            },
        )
        
        workspace.update_agent_status(agent_id, "completed")
        
        logger.info("âœ… [åˆ†æä¸“å®¶] åˆ†æå®Œæˆ")
        
        return {
            "agent_thoughts": {agent_id: thoughts},
            "agent_observations": {agent_id: observations},
        }
    
    except Exception as e:
        logger.error(f"âŒ [åˆ†æä¸“å®¶] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        workspace.update_agent_status(agent_id, "failed")
        workspace.send_message(
            from_agent=agent_id,
            to_agent="orchestrator",
            message_type="error",
            content={"error": str(e)},
        )
        
        return {
            "agent_thoughts": {agent_id: [f"æ‰§è¡Œå¤±è´¥: {str(e)}"]},
            "error": str(e),
        }


# ==================== æ€»ç»“ä¸“å®¶ï¼ˆSummarization Specialistï¼‰ ====================

async def summarization_specialist_node(
    state: MultiAgentState,
    settings: Settings,
) -> Dict[str, Any]:
    """
    æ€»ç»“ä¸“å®¶æ™ºèƒ½ä½“
    
    èŒè´£ï¼š
    - ä¿¡æ¯æ•´åˆ
    - æŠ¥å‘Šç”Ÿæˆ
    - ç»“æ„åŒ–è¾“å‡º
    
    èƒ½åŠ›ï¼š
    - å†…å®¹æ€»ç»“
    - æŠ¥å‘Šæ’°å†™
    - æ ¼å¼è½¬æ¢
    """
    logger.info("ğŸ“ [æ€»ç»“ä¸“å®¶] å¼€å§‹æ‰§è¡Œæ€»ç»“ä»»åŠ¡...")
    
    workspace = SharedWorkspace(state)
    agent_id = "summarization_specialist"
    
    workspace.register_agent(agent_id)
    workspace.update_agent_status(agent_id, "running")
    
    user_query = state.get("user_query", "")
    thoughts = []
    observations = []
    
    try:
        # 1. æ”¶é›†æ‰€æœ‰æ™ºèƒ½ä½“çš„ç»“æœ
        retrieval_results = workspace.get_shared_data("retrieval_results", {})
        analysis_result = workspace.get_shared_data("analysis_result", {})
        
        # 2. æ„å»ºæ€»ç»“ä¸Šä¸‹æ–‡
        context_parts = []
        
        if retrieval_results:
            context_parts.append("## æ£€ç´¢ç»“æœ")
            
            if "knowledge_base" in retrieval_results:
                kb_contexts = retrieval_results["knowledge_base"]
                context_parts.append(
                    f"çŸ¥è¯†åº“å†…å®¹ï¼ˆ{len(kb_contexts)} ä¸ªç‰‡æ®µï¼‰:\n"
                    + "\n".join([
                        f"{i+1}. {ctx.get('content', '')[:300]}"
                        for i, ctx in enumerate(kb_contexts)
                    ])
                )
            
            if "web_search" in retrieval_results:
                search_data = retrieval_results["web_search"]
                context_parts.append(
                    f"ç½‘ç»œæœç´¢ç»“æœ:\n{search_data.get('results', '')[:800]}"
                )
        
        if analysis_result:
            context_parts.append("## åˆ†æç»“æœ")
            context_parts.append(f"æ ¸å¿ƒä¸»é¢˜: {', '.join(analysis_result.get('core_topics', []))}")
            context_parts.append(f"å…³é”®å‘ç°: " + "; ".join(analysis_result.get('key_findings', [])[:3]))
            context_parts.append(f"åˆ†ææ€»ç»“: {analysis_result.get('analysis_summary', '')}")
        
        full_context = "\n\n".join(context_parts)
        
        thoughts.append("æ”¶é›†äº†æ‰€æœ‰æ™ºèƒ½ä½“çš„ç»“æœï¼Œå‡†å¤‡ç”Ÿæˆæ€»ç»“")
        
        # 3. ä½¿ç”¨ LLM ç”Ÿæˆç»¼åˆæ€»ç»“
        logger.info("âœï¸ ä½¿ç”¨ LLM ç”Ÿæˆç»¼åˆæ€»ç»“...")
        
        # è·å–å½“å‰å­ä»»åŠ¡æè¿°
        current_subtask = workspace.get_current_subtask()
        task_description = current_subtask.description if current_subtask else "ç”Ÿæˆå…¨é¢çš„æ€»ç»“æŠ¥å‘Š"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ·±åº¦åˆ†æç»“æœ
        has_deep_analysis = analysis_result and "core_concepts" in analysis_result
        
        summarization_prompt = f"""ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ç ”ç©¶æŠ¥å‘Šæ’°å†™ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·ç”Ÿæˆä¸€ä»½é«˜è´¨é‡ã€ç»“æ„åŒ–çš„ç ”ç©¶æŠ¥å‘Šæˆ–ç­”æ¡ˆã€‚

ã€ä»»åŠ¡è¦æ±‚ã€‘ï¼š{task_description}

ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š{user_query}

ã€æ”¶é›†åˆ°çš„ä¿¡æ¯ã€‘ï¼š
{full_context}

ã€æŠ¥å‘Šæ’°å†™è¦æ±‚ã€‘ï¼š

1. **ç»“æ„åŒ–ç»„ç»‡**ï¼š
   - ä½¿ç”¨æ¸…æ™°çš„ Markdown æ ¼å¼
   - åˆç†çš„æ ‡é¢˜å±‚çº§ï¼ˆ# ## ### ï¼‰
   - å¦‚æœæ˜¯ç ”ç©¶æŠ¥å‘Šï¼ŒåŒ…å«ï¼šå¼•è¨€ã€ä¸»è¦å†…å®¹ã€ç»“è®º
   - å¦‚æœæ˜¯æŠ€æœ¯åˆ†æï¼ŒåŒ…å«ï¼šæ¦‚è¿°ã€æŠ€æœ¯åŸç†ã€åº”ç”¨æ¡ˆä¾‹ã€è¶‹åŠ¿åˆ†æ

2. **å†…å®¹æ·±åº¦**ï¼š
   - ä¸è¦åªæ˜¯ç½—åˆ—ä¿¡æ¯ï¼Œè¦è¿›è¡Œæ·±åº¦æ•´åˆå’Œæç‚¼
   - å»ºç«‹ä¸åŒä¿¡æ¯ç‚¹ä¹‹é—´çš„é€»è¾‘è”ç³»
   - æä¾›æ¸…æ™°çš„è®ºè¯å’Œæ¨ç†è¿‡ç¨‹
   - çªå‡ºå…³é”®å‘ç°å’Œæ ¸å¿ƒæ´å¯Ÿ

3. **è¡¨è¾¾è´¨é‡**ï¼š
   - è¯­è¨€æµç•…ã€ä¸“ä¸šã€å‡†ç¡®
   - é¿å…é‡å¤å’Œå†—ä½™
   - ä½¿ç”¨å…·ä½“çš„æ•°æ®å’Œæ¡ˆä¾‹æ”¯æ’‘è®ºç‚¹
   - é€‚å½“ä½¿ç”¨åˆ—è¡¨ã€è¡¨æ ¼ç­‰å½¢å¼

4. **ä¿¡æ¯æ¥æº**ï¼š
   - ä¼˜å…ˆä½¿ç”¨æä¾›çš„æ£€ç´¢ç»“æœå’Œåˆ†æç»“æœ
   - å¦‚æœå¼•ç”¨å…·ä½“æ•°æ®æˆ–è§‚ç‚¹ï¼Œå¯æ³¨æ˜æ¥æº
   - åŒºåˆ†äº‹å®é™ˆè¿°å’Œæ¨ç†ç»“è®º

5. **å®Œæ•´æ€§**ï¼š
   - å…¨é¢å›ç­”ç”¨æˆ·æå‡ºçš„æ‰€æœ‰é—®é¢˜ç‚¹
   - ä¸é—æ¼å…³é”®ä¿¡æ¯
   - å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®æŒ‡å‡º

6. **é•¿åº¦è¦æ±‚**ï¼š
   - ç®€å•é—®é¢˜ï¼š300-600å­—
   - ä¸­ç­‰å¤æ‚åº¦ï¼š600-1200å­—
   - å¤æ‚ç ”ç©¶æŠ¥å‘Šï¼š1200-2000å­—

ã€ç‰¹åˆ«æ³¨æ„ã€‘ï¼š
- è¿™æ˜¯å¤šæ™ºèƒ½ä½“åä½œçš„æœ€ç»ˆè¾“å‡ºï¼Œè¦ä½“ç°é«˜è´¨é‡
- æ•´åˆæ‰€æœ‰å‰åºæ™ºèƒ½ä½“çš„å·¥ä½œæˆæœ
- ç¡®ä¿æŠ¥å‘Šçš„ä¸“ä¸šæ€§å’Œå¯è¯»æ€§
{"- å·²æœ‰æ·±åº¦åˆ†æç»“æœï¼Œè¯·å……åˆ†åˆ©ç”¨åˆ†æä¸“å®¶æä¾›çš„æ´å¯Ÿ" if has_deep_analysis else ""}

ç°åœ¨è¯·ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼š
"""
        
        final_answer, _ = await invoke_llm(
            messages=[{"role": "user", "content": summarization_prompt}],
            settings=settings,
            temperature=0.6,  # å¹³è¡¡åˆ›é€ æ€§å’Œå‡†ç¡®æ€§
            max_tokens=3000,  # å¢åŠ tokenä»¥æ”¯æŒæ›´é•¿çš„æŠ¥å‘Š
        )
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯æ¶ˆæ¯
        if final_answer.startswith("LLM è°ƒç”¨") or len(final_answer) < 50:
            logger.warning(f"âš ï¸ [æ€»ç»“ä¸“å®¶] LLM å“åº”å¼‚å¸¸: {final_answer}")
            # é™çº§ç­–ç•¥ï¼šç”Ÿæˆç®€å•æ€»ç»“
            fallback_answer = f"""# {user_query}

## æ‰§è¡Œæ‘˜è¦
æœ¬æ¬¡å¤šæ™ºèƒ½ä½“åä½œå®Œæˆäº†ä»¥ä¸‹å·¥ä½œï¼š

### æ£€ç´¢ç»“æœ
{"âœ… å·²å®ŒæˆçŸ¥è¯†åº“æ£€ç´¢å’Œç½‘ç»œæœç´¢" if retrieval_results else "âš ï¸ æ£€ç´¢ä¿¡æ¯æœ‰é™"}

### åˆ†æç»“æœ
{"âœ… å·²å®Œæˆæ·±åº¦åˆ†æ" if analysis_result else "âš ï¸ åˆ†æä¿¡æ¯æœ‰é™"}

## è¯´æ˜
ç”±äºLLMå“åº”è¶…æ—¶æˆ–å¼‚å¸¸ï¼Œç³»ç»Ÿç”Ÿæˆäº†ç®€åŒ–ç‰ˆæŠ¥å‘Šã€‚å»ºè®®ï¼š
1. é‡æ–°æäº¤é—®é¢˜
2. ç®€åŒ–é—®é¢˜æè¿°
3. æ£€æŸ¥ç½‘ç»œè¿æ¥

åŸå§‹é”™è¯¯ä¿¡æ¯ï¼š{final_answer}
"""
            final_answer = fallback_answer
            thoughts.append("LLMå“åº”å¼‚å¸¸ï¼Œä½¿ç”¨é™çº§ç­–ç•¥ç”Ÿæˆç®€åŒ–æŠ¥å‘Š")
        else:
            thoughts.append("ç”Ÿæˆäº†ç»¼åˆæ€»ç»“å›ç­”")
        
        observations.append(f"æ€»ç»“å®Œæˆï¼Œç”Ÿæˆå›ç­”é•¿åº¦ï¼š{len(final_answer)} å­—ç¬¦")
        
        # 4. å­˜å‚¨ç»“æœ
        workspace.store_agent_result(agent_id, {"final_answer": final_answer})
        workspace.set_shared_data("final_answer", final_answer)
        
        # 5. å‘é€ç»“æœæ¶ˆæ¯
        workspace.send_message(
            from_agent=agent_id,
            to_agent="orchestrator",
            message_type="result",
            content={
                "status": "completed",
                "final_answer": final_answer,
            },
        )
        
        workspace.update_agent_status(agent_id, "completed")
        
        logger.info("âœ… [æ€»ç»“ä¸“å®¶] æ€»ç»“å®Œæˆ")
        
        return {
            "agent_thoughts": {agent_id: thoughts},
            "agent_observations": {agent_id: observations},
            "final_answer": final_answer,
        }
    
    except Exception as e:
        logger.error(f"âŒ [æ€»ç»“ä¸“å®¶] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        workspace.update_agent_status(agent_id, "failed")
        workspace.send_message(
            from_agent=agent_id,
            to_agent="orchestrator",
            message_type="error",
            content={"error": str(e)},
        )
        
        return {
            "agent_thoughts": {agent_id: [f"æ‰§è¡Œå¤±è´¥: {str(e)}"]},
            "error": str(e),
        }


# ==================== éªŒè¯ä¸“å®¶ï¼ˆVerification Specialistï¼‰ ====================

async def verification_specialist_node(
    state: MultiAgentState,
    settings: Settings,
) -> Dict[str, Any]:
    """
    éªŒè¯ä¸“å®¶æ™ºèƒ½ä½“ï¼ˆå¯é€‰ï¼‰
    
    èŒè´£ï¼š
    - è´¨é‡æ£€æŸ¥
    - äº‹å®æ ¸æŸ¥
    - ä¸€è‡´æ€§éªŒè¯
    
    èƒ½åŠ›ï¼š
    - ä¿¡æ¯éªŒè¯
    - è´¨é‡è¯„ä¼°
    """
    logger.info("âœ”ï¸ [éªŒè¯ä¸“å®¶] å¼€å§‹æ‰§è¡ŒéªŒè¯ä»»åŠ¡...")
    
    workspace = SharedWorkspace(state)
    agent_id = "verification_specialist"
    
    workspace.register_agent(agent_id)
    workspace.update_agent_status(agent_id, "running")
    
    thoughts = []
    observations = []
    
    try:
        # 1. è·å–æœ€ç»ˆç­”æ¡ˆ
        final_answer = workspace.get_shared_data("final_answer", "")
        
        if not final_answer:
            thoughts.append("æœªæ‰¾åˆ°æœ€ç»ˆç­”æ¡ˆï¼Œè·³è¿‡éªŒè¯")
            workspace.update_agent_status(agent_id, "skipped")
            return {
                "agent_thoughts": {agent_id: thoughts},
            }
        
        # 2. ä½¿ç”¨ LLM è¿›è¡Œè´¨é‡è¯„ä¼°
        logger.info("ğŸ” ä½¿ç”¨ LLM è¿›è¡Œè´¨é‡éªŒè¯...")
        
        verification_prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹å›ç­”çš„è´¨é‡ï¼š

å›ç­”å†…å®¹ï¼š
{final_answer}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°ï¼ˆ0-10åˆ†ï¼‰ï¼š
1. å‡†ç¡®æ€§ï¼šä¿¡æ¯æ˜¯å¦å‡†ç¡®å¯é 
2. å®Œæ•´æ€§ï¼šæ˜¯å¦å…¨é¢å›ç­”äº†é—®é¢˜
3. æ¸…æ™°åº¦ï¼šè¡¨è¾¾æ˜¯å¦æ¸…æ™°æ˜“æ‡‚
4. ç›¸å…³æ€§ï¼šæ˜¯å¦ä¸é—®é¢˜ç›¸å…³

ä»¥ JSON æ ¼å¼è¾“å‡ºè¯„ä¼°ç»“æœï¼š
{{
  "accuracy_score": 0-10,
  "completeness_score": 0-10,
  "clarity_score": 0-10,
  "relevance_score": 0-10,
  "overall_score": 0-10,
  "issues": ["é—®é¢˜1", "é—®é¢˜2", ...],
  "suggestions": ["å»ºè®®1", "å»ºè®®2", ...],
  "verdict": "é€šè¿‡" æˆ– "éœ€è¦æ”¹è¿›"
}}

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
"""
        
        llm_response, _ = await invoke_llm(
            messages=[{"role": "user", "content": verification_prompt}],
            settings=settings,
            temperature=0.2,
            max_tokens=800,
        )
        
        verification_result = parse_json_from_llm(llm_response)
        
        overall_score = verification_result.get("overall_score", 7)
        verdict = verification_result.get("verdict", "é€šè¿‡")
        
        thoughts.append(f"å®Œæˆè´¨é‡éªŒè¯ï¼Œæ€»åˆ†ï¼š{overall_score}/10")
        observations.append(f"éªŒè¯ç»“æœï¼š{verdict}ï¼Œæ€»åˆ† {overall_score}/10")
        
        # 3. å­˜å‚¨ç»“æœ
        workspace.store_agent_result(agent_id, verification_result)
        workspace.set_shared_data("verification_result", verification_result)
        
        # 4. å‘é€ç»“æœæ¶ˆæ¯
        workspace.send_message(
            from_agent=agent_id,
            to_agent="orchestrator",
            message_type="result",
            content={
                "status": "completed",
                "verification_result": verification_result,
            },
        )
        
        workspace.update_agent_status(agent_id, "completed")
        
        logger.info(f"âœ… [éªŒè¯ä¸“å®¶] éªŒè¯å®Œæˆï¼Œç»“æœï¼š{verdict}")
        
        return {
            "agent_thoughts": {agent_id: thoughts},
            "agent_observations": {agent_id: observations},
            "quality_score": overall_score / 10.0,
        }
    
    except Exception as e:
        logger.error(f"âŒ [éªŒè¯ä¸“å®¶] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        workspace.update_agent_status(agent_id, "failed")
        workspace.send_message(
            from_agent=agent_id,
            to_agent="orchestrator",
            message_type="error",
            content={"error": str(e)},
        )
        
        return {
            "agent_thoughts": {agent_id: [f"æ‰§è¡Œå¤±è´¥: {str(e)}"]},
            "error": str(e),
        }


# ==================== æ™ºèƒ½ä½“æ³¨å†Œè¡¨ ====================

AGENT_REGISTRY = {
    "retrieval_specialist": {
        "name": "æ£€ç´¢ä¸“å®¶",
        "description": "è´Ÿè´£çŸ¥è¯†åº“æ£€ç´¢å’Œç½‘ç»œæœç´¢",
        "node_function": retrieval_specialist_node,
        "capabilities": ["knowledge_base_retrieval", "web_search", "document_analysis"],
    },
    "analysis_specialist": {
        "name": "åˆ†æä¸“å®¶",
        "description": "è´Ÿè´£æ•°æ®åˆ†æå’Œå†…å®¹ç†è§£",
        "node_function": analysis_specialist_node,
        "capabilities": ["text_analysis", "data_extraction", "pattern_recognition"],
    },
    "summarization_specialist": {
        "name": "æ€»ç»“ä¸“å®¶",
        "description": "è´Ÿè´£ä¿¡æ¯æ•´åˆå’ŒæŠ¥å‘Šç”Ÿæˆ",
        "node_function": summarization_specialist_node,
        "capabilities": ["content_summarization", "report_generation", "format_conversion"],
    },
    "verification_specialist": {
        "name": "éªŒè¯ä¸“å®¶",
        "description": "è´Ÿè´£è´¨é‡æ£€æŸ¥å’Œäº‹å®æ ¸æŸ¥ï¼ˆå¯é€‰ï¼‰",
        "node_function": verification_specialist_node,
        "capabilities": ["quality_check", "fact_verification", "consistency_validation"],
    },
}


def get_agent_by_id(agent_id: str) -> Optional[Dict[str, Any]]:
    """æ ¹æ®IDè·å–æ™ºèƒ½ä½“ä¿¡æ¯"""
    return AGENT_REGISTRY.get(agent_id)


def list_available_agents() -> List[Dict[str, Any]]:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ™ºèƒ½ä½“"""
    return [
        {
            "id": agent_id,
            "name": info["name"],
            "description": info["description"],
            "capabilities": info["capabilities"],
        }
        for agent_id, info in AGENT_REGISTRY.items()
    ]

