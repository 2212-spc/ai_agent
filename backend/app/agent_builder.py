"""
Agent Builder - åŠ¨æ€æ„å»ºå’Œæ‰§è¡Œè‡ªå®šä¹‰Agentå·¥ä½œæµ
å‚è€ƒ n8n å’Œ Coze çš„æ¨¡å¼ï¼šæ”¯æŒèŠ‚ç‚¹å¼å·¥ä½œæµï¼ŒåŠ¨æ€ç»„åˆå·¥å…·
"""
from __future__ import annotations

import json
import logging
import uuid
from datetime import datetime
from typing import Any, Dict, List, Optional

from langgraph.graph import END, StateGraph
from langgraph.checkpoint.memory import MemorySaver
from sqlalchemy.orm import Session

from .config import Settings
from .database import AgentConfig, ToolRecord
from .graph_agent import AgentState, invoke_llm, knowledge_search_node
from .tool_service import execute_tool

logger = logging.getLogger(__name__)


# ==================== èŠ‚ç‚¹ç±»å‹å®šä¹‰ ====================

NODE_TYPES = {
    "planner": "è§„åˆ’å™¨",
    "router": "è·¯ç”±å™¨",
    "knowledge_search": "çŸ¥è¯†åº“æ£€ç´¢",
    "tool_executor": "å·¥å…·æ‰§è¡Œå™¨",
    "condition": "æ¡ä»¶åˆ¤æ–­",
    "synthesizer": "åˆæˆå™¨",
    "llm_call": "LLMè°ƒç”¨",
}


def create_dynamic_node(
    node_config: Dict[str, Any],
    settings: Settings,
    session: Session,
    tool_records: List[ToolRecord],
) -> callable:
    """
    æ ¹æ®èŠ‚ç‚¹é…ç½®åŠ¨æ€åˆ›å»ºèŠ‚ç‚¹å‡½æ•°
    
    Args:
        node_config: èŠ‚ç‚¹é…ç½® {type, data, ...}
        settings: é…ç½®å¯¹è±¡
        session: æ•°æ®åº“ä¼šè¯
        tool_records: å¯ç”¨å·¥å…·åˆ—è¡¨
    
    Returns:
        èŠ‚ç‚¹å‡½æ•°
    """
    node_type = node_config.get("type", "")
    node_data = node_config.get("data", {})
    
    async def dynamic_node(state: AgentState) -> Dict[str, Any]:
        """åŠ¨æ€èŠ‚ç‚¹æ‰§è¡Œå‡½æ•°"""
        logger.info(f"ğŸ”§ [åŠ¨æ€èŠ‚ç‚¹] {node_type}: {node_data.get('label', '')}")
        
        if node_type == "planner":
            return await handle_planner_node(state, settings, tool_records, node_data)
        elif node_type == "knowledge_search":
            # ä½¿ç”¨ graph_agent ä¸­çš„çŸ¥è¯†åº“æ£€ç´¢èŠ‚ç‚¹
            from .graph_agent import knowledge_search_node as rag_search_node
            result = rag_search_node(state, settings)
            # ç¡®ä¿è¿”å›çš„æ•°æ®æ ¼å¼æ­£ç¡®
            if "retrieved_contexts" in result:
                # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼ˆRetrievedContext -> dictï¼‰
                retrieved = result["retrieved_contexts"]
                if retrieved:
                    result["retrieved_contexts"] = [
                        {
                            "document_id": ctx.document_id if hasattr(ctx, 'document_id') else ctx.get('document_id'),
                            "original_name": ctx.original_name if hasattr(ctx, 'original_name') else ctx.get('original_name'),
                            "content": ctx.content[:500] if hasattr(ctx, 'content') else ctx.get('content', '')[:500]
                        }
                        for ctx in retrieved
                    ]
            return result
        elif node_type == "tool_executor":
            return await handle_tool_executor_node(state, settings, session, tool_records, node_data)
        elif node_type == "condition":
            return handle_condition_node(state, node_data)
        elif node_type == "llm_call":
            return await handle_llm_call_node(state, settings, node_data)
        elif node_type == "synthesizer":
            return await handle_synthesizer_node(state, settings, node_data)
        elif node_type == "delay":
            return handle_delay_node(state, node_data)
        elif node_type == "variable":
            return handle_variable_node(state, node_data)
        elif node_type == "loop":
            return handle_loop_node(state, node_data)
        else:
            logger.warning(f"æœªçŸ¥èŠ‚ç‚¹ç±»å‹: {node_type}")
            return {"thoughts": [f"è·³è¿‡æœªçŸ¥èŠ‚ç‚¹: {node_type}"]}
    
    return dynamic_node


async def handle_planner_node(
    state: AgentState,
    settings: Settings,
    tool_records: List[ToolRecord],
    node_data: Dict[str, Any],
) -> Dict[str, Any]:
    """å¤„ç†è§„åˆ’å™¨èŠ‚ç‚¹"""
    user_query = state.get("user_query", "")
    custom_prompt = node_data.get("prompt", "")
    
    planning_prompt = custom_prompt or f"åˆ†æç”¨æˆ·é—®é¢˜ï¼š{user_query}ï¼Œåˆ¶å®šæ‰§è¡Œè®¡åˆ’ã€‚"
    
    try:
        reply, _ = await invoke_llm(
            messages=[{"role": "user", "content": planning_prompt}],
            settings=settings,
            temperature=0.3,
            max_tokens=500,
        )
        
        return {
            "plan": reply,
            "thoughts": [f"è§„åˆ’å®Œæˆ: {reply[:100]}..."],
        }
    except Exception as e:
        logger.error(f"è§„åˆ’å™¨å¤±è´¥: {e}")
        return {"plan": "ä½¿ç”¨é»˜è®¤è§„åˆ’", "thoughts": [f"è§„åˆ’å¼‚å¸¸: {str(e)}"]}


async def handle_tool_executor_node(
    state: AgentState,
    settings: Settings,
    session: Session,
    tool_records: List[ToolRecord],
    node_data: Dict[str, Any],
) -> Dict[str, Any]:
    """å¤„ç†å·¥å…·æ‰§è¡Œå™¨èŠ‚ç‚¹"""
    tool_id = node_data.get("toolId")
    tool_args = node_data.get("arguments", {})
    user_query = state.get("user_query", "")
    
    if not tool_id:
        return {"observations": ["å·¥å…·IDæœªæŒ‡å®š"], "error": "Missing tool_id"}
    
    # æŸ¥æ‰¾å·¥å…·
    selected_tool = next((t for t in tool_records if t.id == tool_id), None)
    if not selected_tool:
        return {"observations": [f"å·¥å…· {tool_id} ä¸å­˜åœ¨"], "error": "Tool not found"}
    
    # å¦‚æœå‚æ•°ä¸ºç©ºï¼Œå°è¯•ä»ç”¨æˆ·æŸ¥è¯¢ä¸­æ™ºèƒ½æå–
    if not tool_args:
        tool_args = extract_tool_args_from_query(selected_tool, user_query, state)
    
    try:
        result = execute_tool(
            tool=selected_tool,
            arguments=tool_args,
            settings=settings,
            session=session,
        )
        
        tool_result = {
            "tool_name": selected_tool.name,
            "output": result,
            "arguments": tool_args,
        }
        
        return {
            "tool_results": [tool_result],
            "observations": [f"å·¥å…· {selected_tool.name} æ‰§è¡ŒæˆåŠŸ"],
            "thoughts": [f"è°ƒç”¨å·¥å…·: {selected_tool.name}"],
        }
    except Exception as e:
        logger.error(f"å·¥å…·æ‰§è¡Œå¤±è´¥: {e}")
        return {
            "observations": [f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"],
            "error": str(e),
        }


def extract_tool_args_from_query(
    tool: ToolRecord,
    user_query: str,
    state: AgentState,
) -> Dict[str, Any]:
    """
    ä»ç”¨æˆ·æŸ¥è¯¢ä¸­æ™ºèƒ½æå–å·¥å…·å‚æ•°
    
    å‚è€ƒ graph_agent.py ä¸­çš„å‚æ•°æå–é€»è¾‘
    """
    import json
    import re
    
    try:
        config = json.loads(tool.config or "{}")
        builtin_key = config.get("builtin_key", "")
    except:
        builtin_key = ""
    
    args = {}
    
    # å¤©æ°”å·¥å…·
    if builtin_key == "get_weather":
        # æå–åŸå¸‚å
        from .graph_agent import extract_city_from_query
        city = extract_city_from_query(user_query)
        args = {"city": city}
    
    # æœç´¢å·¥å…·
    elif builtin_key == "web_search":
        from .graph_agent import extract_search_query
        query = extract_search_query(user_query)
        args = {"query": query, "num_results": 5}
    
    # çŸ¥è¯†åº“æ£€ç´¢å·¥å…·
    elif builtin_key == "search_knowledge":
        # ä½¿ç”¨ç”¨æˆ·æŸ¥è¯¢ä½œä¸ºæ£€ç´¢å…³é”®è¯
        args = {"query": user_query, "top_k": 3}
    
    # ç¬”è®°å·¥å…·
    elif builtin_key == "write_note":
        # ä»çŠ¶æ€ä¸­æå–ä¿¡æ¯ï¼Œç”Ÿæˆç¬”è®°å†…å®¹
        tool_results = state.get("tool_results", [])
        
        # ä¼˜å…ˆæŸ¥æ‰¾æœç´¢ç»“æœ
        search_result = None
        weather_result = None
        llm_result = None
        
        for tr in tool_results:
            tool_name = str(tr.get("tool_name", "")).lower()
            if "æœç´¢" in tool_name or "search" in tool_name or "web_search" in tool_name:
                search_result = tr
            elif "å¤©æ°”" in tool_name or "weather" in tool_name:
                weather_result = tr
            elif "llm" in tool_name or "æ€»ç»“" in tool_name or "llm_call" in tool_name:
                llm_result = tr
        
        if search_result:
            # ä»æœç´¢ç»“æœä¸­æå–å†…å®¹
            search_output = search_result.get("output", "")
            search_query = search_result.get("arguments", {}).get("query", "æœç´¢ç»“æœ")
            # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
            safe_query = search_query.replace(" ", "_").replace("/", "_").replace("\\", "_")[:30]
            filename = f"{safe_query}_ç¬”è®°_{datetime.now().strftime('%Y%m%d')}.txt"
            # ä½¿ç”¨æœç´¢ç»“æœä½œä¸ºç¬”è®°å†…å®¹ï¼ˆå¦‚æœå¤ªé•¿åˆ™æˆªå–ï¼‰
            content = f"æŸ¥è¯¢ä¸»é¢˜ï¼š{search_query}\n\næœç´¢ç»“æœï¼š\n{search_output[:1500]}"
            args = {"filename": filename, "content": content}
        elif llm_result:
            # ä½¿ç”¨LLMæ€»ç»“ç»“æœ
            llm_output = llm_result.get("output", "")
            filename = f"æ€»ç»“ç¬”è®°_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            content = f"ç”¨æˆ·æŸ¥è¯¢ï¼š{user_query}\n\næ€»ç»“å†…å®¹ï¼š\n{llm_output[:1500]}"
            args = {"filename": filename, "content": content}
        elif weather_result:
            # åŸæœ‰å¤©æ°”é€»è¾‘
            city = weather_result.get("arguments", {}).get("city", "æœªçŸ¥åŸå¸‚")
            filename = f"{city}_æé†’_{datetime.now().strftime('%Y%m%d')}.txt"
            content = f"æ ¹æ®å¤©æ°”æŸ¥è¯¢ç»“æœï¼Œæé†’ï¼š{weather_result.get('output', '')[:200]}"
            args = {"filename": filename, "content": content}
        else:
            # é»˜è®¤ç¬”è®°ï¼šä½¿ç”¨æ‰€æœ‰å·¥å…·ç»“æœ
            all_outputs = []
            for tr in tool_results:
                tool_name = tr.get("tool_name", "å·¥å…·")
                output = tr.get("output", "")
                if output:
                    all_outputs.append(f"ã€{tool_name}ã€‘\n{output[:500]}\n")
            
            if all_outputs:
                filename = f"ç¬”è®°_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
                content = f"ç”¨æˆ·æŸ¥è¯¢ï¼š{user_query}\n\nå¤„ç†ç»“æœï¼š\n" + "\n".join(all_outputs)
            else:
                filename = f"ç¬”è®°_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
                content = f"ç”¨æˆ·æŸ¥è¯¢ï¼š{user_query}\n\nå¤„ç†ç»“æœï¼šå·²å®Œæˆ"
            args = {"filename": filename, "content": content}
    
    # ç»˜å›¾å·¥å…·
    elif builtin_key == "draw_diagram":
        # æå–ä¸»é¢˜
        topic = user_query[:30].replace("æœç´¢", "").replace("æŸ¥è¯¢", "").strip()
        args = {
            "filename": f"{topic}_mindmap.md",
            "diagram_code": f"mindmap\n  root(({topic}))\n    ä¿¡æ¯1\n    ä¿¡æ¯2",
            "diagram_type": "mindmap"
        }
    
    # è·å–ç½‘é¡µå†…å®¹å·¥å…·
    elif builtin_key == "fetch_webpage":
        # å¦‚æœä¹‹å‰æœ‰æœç´¢ç»“æœï¼Œå°è¯•ä»ç»“æœä¸­æå–URL
        tool_results = state.get("tool_results", [])
        import re
        
        for tr in tool_results:
            if "æœç´¢" in str(tr.get("tool_name", "")) or "search" in str(tr.get("tool_name", "")).lower():
                # ä»æœç´¢ç»“æœä¸­æå–ç¬¬ä¸€ä¸ªURL
                output = tr.get("output", "")
                # æå–URLï¼ˆåŒ¹é… http:// æˆ– https:// å¼€å¤´çš„URLï¼‰
                urls = re.findall(r'https?://[^\s\n\)]+', output)
                if urls:
                    args = {"url": urls[0]}
                    break
        
        if not args:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°URLï¼Œå°è¯•ä»ç”¨æˆ·æŸ¥è¯¢ä¸­æå–
            urls = re.findall(r'https?://[^\s\n\)]+', user_query)
            if urls:
                args = {"url": urls[0]}
            else:
                # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œå°è¯•ä½¿ç”¨æœç´¢ç»“æœä¸­çš„é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
                args = {"url": ""}  # ä¼šå¯¼è‡´å·¥å…·æŠ¥é”™ï¼Œä½†æ¯”é™é»˜å¤±è´¥å¥½
    
    return args


def handle_condition_node(state: AgentState, node_data: Dict[str, Any]) -> Dict[str, Any]:
    """å¤„ç†æ¡ä»¶åˆ¤æ–­èŠ‚ç‚¹ï¼ˆç±»ä¼¼ n8n çš„ IF èŠ‚ç‚¹ï¼‰"""
    condition_type = node_data.get("conditionType", "always")  # always, contains, equals
    condition_value = node_data.get("conditionValue", "")
    
    should_continue = True
    
    if condition_type == "contains":
        # æ£€æŸ¥å·¥å…·ç»“æœæˆ–ç”¨æˆ·æŸ¥è¯¢ä¸­æ˜¯å¦åŒ…å«æŸä¸ªå€¼
        # ä¼˜å…ˆæ£€æŸ¥æœ€è¿‘çš„å·¥å…·ç»“æœ
        tool_results = state.get("tool_results", [])
        check_text = ""
        
        if tool_results:
            # æ£€æŸ¥æœ€è¿‘çš„å·¥å…·ç»“æœ
            last_result = tool_results[-1]
            check_text = str(last_result.get("output", ""))
        else:
            # å¦‚æœæ²¡æœ‰å·¥å…·ç»“æœï¼Œæ£€æŸ¥ç”¨æˆ·æŸ¥è¯¢
            check_text = state.get("user_query", "")
        
        should_continue = condition_value.lower() in check_text.lower()
        logger.info(f"ğŸ”€ æ¡ä»¶åˆ¤æ–­: æ£€æŸ¥ '{condition_value}' æ˜¯å¦åœ¨æ–‡æœ¬ä¸­ -> {should_continue}")
        
    elif condition_type == "equals":
        # æ£€æŸ¥çŠ¶æ€å€¼æ˜¯å¦ç­‰äºæŸä¸ªå€¼
        check_field = node_data.get("checkField", "user_query")
        state_value = str(state.get(check_field, ""))
        should_continue = state_value == condition_value
    
    return {
        "next_action": "continue" if should_continue else "skip",
        "thoughts": [f"æ¡ä»¶åˆ¤æ–­: {condition_type} = {should_continue}"],
        "observations": [f"æ¡ä»¶ '{condition_value}' {'æ»¡è¶³' if should_continue else 'ä¸æ»¡è¶³'}"],
    }


async def handle_llm_call_node(
    state: AgentState,
    settings: Settings,
    node_data: Dict[str, Any],
) -> Dict[str, Any]:
    """å¤„ç†è‡ªå®šä¹‰LLMè°ƒç”¨èŠ‚ç‚¹"""
    prompt_template = node_data.get("prompt", "")
    user_query = state.get("user_query", "")
    
    # æ›¿æ¢æ¨¡æ¿å˜é‡
    prompt = prompt_template.replace("{{user_query}}", user_query)
    
    # å¯ä»¥ä»çŠ¶æ€ä¸­æå–å…¶ä»–å˜é‡
    tool_results = state.get("tool_results", [])
    if tool_results:
        last_result = tool_results[-1].get("output", "") if tool_results else ""
        prompt = prompt.replace("{{last_tool_result}}", last_result[:500])
    
    try:
        reply, _ = await invoke_llm(
            messages=[{"role": "user", "content": prompt}],
            settings=settings,
            temperature=node_data.get("temperature", 0.7),
            max_tokens=node_data.get("maxTokens", 1000),
        )
        
        return {
            "thoughts": [reply[:200]],
            "observations": [f"LLMè°ƒç”¨å®Œæˆ: {reply[:100]}..."],
        }
    except Exception as e:
        logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
        return {"error": str(e)}


async def handle_synthesizer_node(
    state: AgentState,
    settings: Settings,
    node_data: Dict[str, Any],
) -> Dict[str, Any]:
    """å¤„ç†åˆæˆå™¨èŠ‚ç‚¹"""
    from .graph_agent import synthesizer_node
    
    # å¦‚æœæœ‰è‡ªå®šä¹‰æç¤ºè¯ï¼Œä¿®æ”¹çŠ¶æ€
    custom_prompt = node_data.get("prompt", "")
    
    # å¦‚æœæ²¡æœ‰è‡ªå®šä¹‰æç¤ºè¯ï¼Œä½¿ç”¨æ”¹è¿›çš„é»˜è®¤æç¤ºè¯
    if not custom_prompt:
        tool_results = state.get("tool_results", [])
        retrieved_contexts = state.get("retrieved_contexts", [])
        
        # æ„å»ºé»˜è®¤æç¤ºè¯
        default_prompt_parts = [
            "è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆè¯¦ç»†ã€å®Œæ•´çš„å›å¤ï¼š",
            "",
            f"ç”¨æˆ·æŸ¥è¯¢ï¼š{state.get('user_query', '')}",
            ""
        ]
        
        if tool_results:
            default_prompt_parts.append("å·¥å…·æ‰§è¡Œç»“æœï¼š")
            for tr in tool_results:
                tool_name = tr.get("tool_name", "å·¥å…·")
                output = tr.get("output", "")
                if output:
                    default_prompt_parts.append(f"ã€{tool_name}ã€‘")
                    default_prompt_parts.append(f"{output[:1000]}")  # é™åˆ¶é•¿åº¦
                    default_prompt_parts.append("")
        
        if retrieved_contexts:
            default_prompt_parts.append("çŸ¥è¯†åº“æ£€ç´¢ç»“æœï¼š")
            for ctx in retrieved_contexts[:3]:
                default_prompt_parts.append(f"- {ctx.get('content', '')[:300]}")
            default_prompt_parts.append("")
        
        default_prompt_parts.extend([
            "è¯·æ•´åˆä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªè¯¦ç»†ã€æœ‰ç”¨çš„å›ç­”ã€‚",
            "å¦‚æœç”¨æˆ·è¦æ±‚å†™å…¥ç¬”è®°ï¼Œè¯·ç¡®è®¤ç¬”è®°å†…å®¹å·²åŒ…å«ç›¸å…³çš„é‡è¦ä¿¡æ¯ã€‚",
            "å¦‚æœæŸ¥è¯¢æ¶‰åŠæœç´¢ï¼Œè¯·ç¡®ä¿åœ¨å›å¤ä¸­åŒ…å«æœç´¢åˆ°çš„å…³é”®ä¿¡æ¯ï¼Œè€Œä¸æ˜¯åªè¯´'å·²å†™å…¥ç¬”è®°'ã€‚"
        ])
        
        custom_prompt = "\n".join(default_prompt_parts)
    
    if custom_prompt:
        # ä¸´æ—¶ä¿®æ”¹ç”¨æˆ·æŸ¥è¯¢ä»¥åŒ…å«è‡ªå®šä¹‰æç¤º
        original_query = state.get("user_query", "")
        state["user_query"] = f"{original_query}\n\n{custom_prompt}"
    
    result = await synthesizer_node(state, settings)
    
    # æ¢å¤åŸå§‹æŸ¥è¯¢
    if custom_prompt:
        state["user_query"] = original_query
    
    return result


def handle_delay_node(state: AgentState, node_data: Dict[str, Any]) -> Dict[str, Any]:
    """å¤„ç†å»¶è¿Ÿç­‰å¾…èŠ‚ç‚¹"""
    import asyncio
    
    seconds = node_data.get("seconds", 1)
    try:
        seconds = float(seconds)
        if seconds > 0:
            # æ³¨æ„ï¼šåœ¨åŒæ­¥å‡½æ•°ä¸­ä½¿ç”¨ asyncio.sleep éœ€è¦ç‰¹æ®Šå¤„ç†
            # è¿™é‡Œåªæ˜¯æ ‡è®°å»¶è¿Ÿï¼Œå®é™…å»¶è¿Ÿç”±æ‰§è¡Œå™¨æ§åˆ¶
            logger.info(f"â±ï¸ å»¶è¿Ÿç­‰å¾… {seconds} ç§’")
        return {
            "thoughts": [f"å»¶è¿Ÿç­‰å¾… {seconds} ç§’"],
            "observations": [f"å·²ç­‰å¾… {seconds} ç§’"],
        }
    except:
        return {"thoughts": ["å»¶è¿Ÿå‚æ•°æ— æ•ˆ"]}


def handle_variable_node(state: AgentState, node_data: Dict[str, Any]) -> Dict[str, Any]:
    """å¤„ç†å˜é‡è®¾ç½®èŠ‚ç‚¹"""
    var_name = node_data.get("variableName", "")
    var_value = node_data.get("variableValue", "")
    
    if not var_name:
        return {"observations": ["å˜é‡åæœªè®¾ç½®"]}
    
    # æ›¿æ¢æ¨¡æ¿å˜é‡
    if "{{last_tool_result}}" in var_value:
        tool_results = state.get("tool_results", [])
        if tool_results:
            last_result = tool_results[-1].get("output", "")
            var_value = var_value.replace("{{last_tool_result}}", str(last_result)[:500])
    
    if "{{user_query}}" in var_value:
        var_value = var_value.replace("{{user_query}}", state.get("user_query", ""))
    
    # å­˜å‚¨åˆ°çŠ¶æ€ï¼ˆå¯ä»¥æ‰©å±•AgentStateæ·»åŠ variableså­—æ®µï¼‰
    return {
        "thoughts": [f"è®¾ç½®å˜é‡ {var_name} = {var_value[:50]}..."],
        "observations": [f"å˜é‡ {var_name} å·²è®¾ç½®"],
    }


def handle_loop_node(state: AgentState, node_data: Dict[str, Any]) -> Dict[str, Any]:
    """å¤„ç†å¾ªç¯æ‰§è¡ŒèŠ‚ç‚¹ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    max_iterations = node_data.get("maxIterations", 5)
    condition = node_data.get("condition", "")
    
    current_step = state.get("current_step", 0)
    
    if current_step >= max_iterations:
        return {
            "next_action": "break",
            "thoughts": [f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}ï¼Œé€€å‡ºå¾ªç¯"],
        }
    
    # æ£€æŸ¥å¾ªç¯æ¡ä»¶
    if condition:
        # ç®€åŒ–ç‰ˆï¼šæ£€æŸ¥æ¡ä»¶æ˜¯å¦æ»¡è¶³
        tool_results = state.get("tool_results", [])
        if tool_results:
            last_output = str(tool_results[-1].get("output", ""))
            if condition.lower() not in last_output.lower():
                return {
                    "next_action": "break",
                    "thoughts": [f"å¾ªç¯æ¡ä»¶ä¸æ»¡è¶³ï¼Œé€€å‡ºå¾ªç¯"],
                }
    
    return {
        "next_action": "continue",
        "thoughts": [f"å¾ªç¯ç»§ç»­ï¼Œå½“å‰è¿­ä»£ {current_step + 1}/{max_iterations}"],
    }


def build_dynamic_graph(
    nodes: List[Dict[str, Any]],
    edges: List[Dict[str, Any]],
    settings: Settings,
    session: Session,
    tool_records: List[ToolRecord],
) -> StateGraph:
    """
    æ ¹æ®èŠ‚ç‚¹å’Œè¾¹é…ç½®åŠ¨æ€æ„å»ºLangGraphå·¥ä½œæµ
    
    Args:
        nodes: èŠ‚ç‚¹åˆ—è¡¨ [{id, type, data, ...}]
        edges: è¾¹åˆ—è¡¨ [{source, target, ...}]
        settings: é…ç½®å¯¹è±¡
        session: æ•°æ®åº“ä¼šè¯
        tool_records: å¯ç”¨å·¥å…·åˆ—è¡¨
    
    Returns:
        ç¼–è¯‘åçš„LangGraphå›¾
    """
    logger.info(f"ğŸ—ï¸ æ„å»ºåŠ¨æ€å·¥ä½œæµ: {len(nodes)} ä¸ªèŠ‚ç‚¹, {len(edges)} æ¡è¾¹")
    
    workflow = StateGraph(AgentState)
    node_map = {}  # node_id -> node_function
    
    # 1. æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹
    for node_config in nodes:
        node_id = node_config.get("id")
        node_type = node_config.get("type")
        
        if not node_id or not node_type:
            logger.warning(f"è·³è¿‡æ— æ•ˆèŠ‚ç‚¹: {node_config}")
            continue
        
        # åˆ›å»ºèŠ‚ç‚¹å‡½æ•°
        node_func = create_dynamic_node(node_config, settings, session, tool_records)
        workflow.add_node(node_id, node_func)
        node_map[node_id] = node_config
    
    # 2. è®¾ç½®å…¥å£ç‚¹ï¼ˆæ‰¾åˆ°æ²¡æœ‰å…¥è¾¹çš„èŠ‚ç‚¹ä½œä¸ºèµ·å§‹ï¼‰
    source_nodes = {edge["source"] for edge in edges}
    target_nodes = {edge["target"] for edge in edges}
    entry_nodes = [n for n in node_map.keys() if n not in target_nodes]
    
    if entry_nodes:
        workflow.set_entry_point(entry_nodes[0])
    else:
        # å¦‚æœæ²¡æœ‰è¾¹ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹
        if nodes:
            workflow.set_entry_point(nodes[0]["id"])
    
    # 3. æ·»åŠ è¾¹ï¼ˆæ¡ä»¶è¾¹æˆ–æ™®é€šè¾¹ï¼‰
    for edge in edges:
        source = edge.get("source")
        target = edge.get("target")
        condition = edge.get("condition")  # æ¡ä»¶è¾¹çš„åˆ¤æ–­å‡½æ•°
        
        if source not in node_map or target not in node_map:
            logger.warning(f"è·³è¿‡æ— æ•ˆè¾¹: {source} -> {target}")
            continue
        
        if condition:
            # æ¡ä»¶è¾¹ï¼ˆç±»ä¼¼ n8n çš„ IF èŠ‚ç‚¹åˆ†æ”¯ï¼‰
            def make_router(source_id: str, target_id: str, cond: str):
                def route_func(state: AgentState) -> str:
                    # æ£€æŸ¥æ¡ä»¶
                    if cond == "always":
                        return target_id
                    elif cond == "condition_true":
                        # æ£€æŸ¥ä¸Šä¸€ä¸ªèŠ‚ç‚¹çš„ next_action
                        next_action = state.get("next_action", "")
                        return target_id if next_action == "continue" else END
                    else:
                        return target_id
                
                return route_func
            
            workflow.add_conditional_edges(
                source,
                make_router(source, target, condition),
                {target: target, END: END},
            )
        else:
            # æ™®é€šè¾¹
            workflow.add_edge(source, target)
    
    # 4. æ·»åŠ ç»“æŸè¾¹ï¼ˆæ‰¾åˆ°æ²¡æœ‰å‡ºè¾¹çš„èŠ‚ç‚¹ï¼‰
    outgoing_edges = {edge["source"] for edge in edges}
    exit_nodes = [n for n in node_map.keys() if n not in outgoing_edges]
    
    for exit_node in exit_nodes:
        workflow.add_edge(exit_node, END)
    
    logger.info("âœ… åŠ¨æ€å·¥ä½œæµæ„å»ºå®Œæˆ")
    return workflow


async def execute_custom_agent(
    agent_config: AgentConfig,
    user_query: str,
    settings: Settings,
    session: Session,
    tool_records: List[ToolRecord],
    conversation_history: List[Dict[str, str]] = None,
) -> Dict[str, Any]:
    """
    æ‰§è¡Œè‡ªå®šä¹‰Agenté…ç½®
    
    Args:
        agent_config: Agenté…ç½®å¯¹è±¡
        user_query: ç”¨æˆ·æŸ¥è¯¢
        settings: é…ç½®å¯¹è±¡
        session: æ•°æ®åº“ä¼šè¯
        tool_records: å¯ç”¨å·¥å…·åˆ—è¡¨
        conversation_history: å¯¹è¯å†å²
    
    Returns:
        æ‰§è¡Œç»“æœ
    """
    try:
        # è§£æé…ç½®
        config_data = json.loads(agent_config.config)
        nodes = config_data.get("nodes", [])
        edges = config_data.get("edges", [])
        
        # æ„å»ºåŠ¨æ€å›¾
        workflow = build_dynamic_graph(nodes, edges, settings, session, tool_records)
        
        # ç¼–è¯‘å›¾
        checkpointer = MemorySaver()
        app = workflow.compile(checkpointer=checkpointer)
        
        # åˆå§‹åŒ–çŠ¶æ€
        initial_state: AgentState = {
            "user_query": user_query,
            "conversation_history": conversation_history or [],
            "plan": None,
            "current_step": 0,
            "max_iterations": 20,
            "available_tools": [tool.id for tool in tool_records],
            "tool_calls_made": [],
            "tool_results": [],
            "skipped_tasks": [],
            "use_knowledge_base": any(n.get("type") == "knowledge_search" for n in nodes),
            "retrieved_contexts": [],
            "thoughts": [],
            "observations": [],
            "next_action": None,
            "needs_human_input": False,
            "human_feedback": None,
            "reflection": None,
            "quality_score": 0.0,
            "final_answer": None,
            "is_complete": False,
            "error": None,
        }
        
        thread_id = str(uuid.uuid4())
        config = {"configurable": {"thread_id": thread_id}}
        
        # æ‰§è¡Œå·¥ä½œæµ
        final_state = await app.ainvoke(initial_state, config=config)
        
        return {
            "success": True,
            "final_answer": final_state.get("final_answer", "æ‰§è¡Œå®Œæˆ"),
            "thoughts": final_state.get("thoughts", []),
            "observations": final_state.get("observations", []),
            "tool_results": final_state.get("tool_results", []),
            "retrieved_contexts": final_state.get("retrieved_contexts", []),
            "thread_id": thread_id,
        }
    
    except Exception as e:
        logger.error(f"æ‰§è¡Œè‡ªå®šä¹‰Agentå¤±è´¥: {e}", exc_info=True)
        return {
            "success": False,
            "error": str(e),
            "final_answer": f"æ‰§è¡Œå¤±è´¥: {str(e)}",
        }


async def stream_custom_agent(
    agent_config: AgentConfig,
    user_query: str,
    settings: Settings,
    session: Session,
    tool_records: List[ToolRecord],
    conversation_history: List[Dict[str, str]] = None,
):
    """
    æµå¼æ‰§è¡Œè‡ªå®šä¹‰Agentï¼ˆç”¨äºå‰ç«¯å®æ—¶å±•ç¤ºï¼‰
    """
    try:
        config_data = json.loads(agent_config.config)
        nodes = config_data.get("nodes", [])
        edges = config_data.get("edges", [])
        
        workflow = build_dynamic_graph(nodes, edges, settings, session, tool_records)
        checkpointer = MemorySaver()
        app = workflow.compile(checkpointer=checkpointer)
        
        initial_state: AgentState = {
            "user_query": user_query,
            "conversation_history": conversation_history or [],
            "plan": None,
            "current_step": 0,
            "max_iterations": 20,
            "available_tools": [tool.id for tool in tool_records],
            "tool_calls_made": [],
            "tool_results": [],
            "skipped_tasks": [],
            "use_knowledge_base": any(n.get("type") == "knowledge_search" for n in nodes),
            "retrieved_contexts": [],
            "thoughts": [],
            "observations": [],
            "next_action": None,
            "needs_human_input": False,
            "human_feedback": None,
            "reflection": None,
            "quality_score": 0.0,
            "final_answer": None,
            "is_complete": False,
            "error": None,
        }
        
        thread_id = str(uuid.uuid4())
        config = {"configurable": {"thread_id": thread_id}}
        
        # æµå¼æ‰§è¡Œ
        final_answer = None
        async for event in app.astream(initial_state, config=config):
            for node_name, node_output in event.items():
                if node_name != "__end__":
                    # æ£€æŸ¥æ˜¯å¦æœ‰æœ€ç»ˆç­”æ¡ˆ
                    if "final_answer" in node_output and node_output["final_answer"]:
                        final_answer = node_output["final_answer"]
                    
                    yield {
                        "event": "node_output",
                        "node": node_name,
                        "data": node_output,
                        "timestamp": datetime.now().isoformat(),
                    }
        
        # å¦‚æœè¿˜æ²¡æœ‰å‘é€æœ€ç»ˆç­”æ¡ˆï¼Œç°åœ¨å‘é€
        if final_answer:
            yield {
                "event": "final_answer",
                "content": final_answer,
                "timestamp": datetime.now().isoformat(),
            }
        
        yield {
            "event": "completed",
            "thread_id": thread_id,
        }
    
    except Exception as e:
        logger.error(f"æµå¼æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        yield {
            "event": "error",
            "message": str(e),
        }

